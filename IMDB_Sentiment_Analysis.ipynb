{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yW-OMw3wSYB"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwt90Bj1PB_"
      },
      "source": [
        "Tuto pour démarrer :\n",
        "https://bhadreshpsavani.medium.com/tutorial-on-sentimental-analysis-using-pytorch-b1431306a2d7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smH29zmcyd43",
        "outputId": "c2ac7735-37e6-4932-fb7c-1e01fa6ed756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJlIuoU3Wun9"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_25-QAQzKQg"
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/MyDrive/Sentiment Analyis'\n",
        "#root = \"/Users/lucienavez/Library/CloudStorage/GoogleDrive-lucienavez@gmail.com/Mon Drive/Sentiment Analyis\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUDD5XUlwPw2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrokRsDpzV99",
        "outputId": "2d09d23d-fff3-413e-f4e9-25f0e8e05b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bcolz-zipline\n",
            "  Downloading bcolz_zipline-1.2.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 15.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23,>=1.16 in /usr/local/lib/python3.7/dist-packages (from bcolz-zipline) (1.21.6)\n",
            "Installing collected packages: bcolz-zipline\n",
            "Successfully installed bcolz-zipline-1.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 14.5 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 73.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "# Missing libraries\n",
        "! pip install bcolz-zipline\n",
        "! pip install gensim\n",
        "! pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
        "! pip install contractions\n",
        "! pip install bs4\n",
        "\n",
        "# Basics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "import bcolz\n",
        "import matplotlib.pyplot as plt\n",
        "import contractions\n",
        "import string\n",
        "from collections import Counter, OrderedDict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('wordnet') # for lemmatization\n",
        "nltk.download('stopwords') # for stopwords\n",
        "nltk.download('omw-1.4') # Don't know why but code doesn't work (lemmization part)\n",
        "nltk.download('words')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mdMjOQ5TOku",
        "outputId": "755056d3-1767-42ca-9a8d-90bfb9f8b70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Hardware\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnPsBKPUpZuy"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th-M7v_OTY7i"
      },
      "source": [
        "# Dataset\n",
        "The dataset is downloaded from the following link : \n",
        "https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/data\n",
        "\n",
        "This dataset is made of movie review available on amazon\n",
        "\n",
        "The dataset follows the following format :\n",
        "\n",
        "\n",
        "2 fields:\n",
        "\n",
        "- review: text of the review\n",
        "- sentiment: 2 possible values (positive or negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHYgzvwIYcIn",
        "outputId": "f9332ae9-73d5-4f1a-a577-c3acd4d7e9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  (50000, 2)\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ],
      "source": [
        "#Load the data\n",
        "data_path = root + '/data/IMDB Dataset.csv'\n",
        "\n",
        "df = pd.read_csv(data_path, encoding='latin-1')\n",
        "print(\"Size: \", df.shape)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q--LPdQqmFZ_",
        "outputId": "e8417339-d9b7-479f-df46-43505b1056f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yUNhSdQGiI1",
        "outputId": "0e364888-7bd4-4902-b337-f7e796378d92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  review sentiment\n",
            "40877  I recently started watching this show, and I h...  positive\n",
            "18057  \"Return of the Jedi\" is often remembered for w...  positive\n",
            "19066  I remember I loved this movie when it came out...  negative\n",
            "20525  I don't know what the last reviewer is talking...  positive\n",
            "5847   From the very beginning I was so excited to se...  positive\n",
            "                                                  review sentiment\n",
            "33553  I really liked this Summerslam due to the look...  positive\n",
            "9427   Not many television shows appeal to quite as m...  positive\n",
            "199    The film quickly gets to a major chase scene w...  negative\n",
            "12447  Jane Austen would definitely approve of this o...  positive\n",
            "39489  Expectations were somewhat high for me when I ...  negative\n"
          ]
        }
      ],
      "source": [
        "#Split the data between a training set and a test set\n",
        "df_train, df_test = train_test_split(df, random_state = 42, shuffle = True, test_size = 0.1)\n",
        "print(df_train.head())\n",
        "print(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDDigm6nGliP"
      },
      "outputs": [],
      "source": [
        "#Create 2 separate csv file in order to load them later\n",
        "df_train.to_csv(root + '/data/train_imdb.csv', index=True)\n",
        "df_test.to_csv(root + '/data/test_imdb.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcxi4X5CWuoA"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "See following links : \n",
        "https://medium.com/analytics-vidhya/pre-processing-tweets-for-sentiment-analysis-a74deda9993e\n",
        "https://towardsdatascience.com/how-to-preprocess-social-media-data-and-text-messages-b011efacf74\n",
        "\n",
        "Data must be pre-processed in order to be used for training.\n",
        "For the time being, the only kind of pre-processing that has been applied is removing the emojis.\n",
        "Howerver, some parts of the tweets may be noisy for the model and would require extra attention : \n",
        "- usernames and @ (handles) : @LATimesautos, @kirstiealley, ...\n",
        "- hashtags : #lebron, #kindle2\n",
        "- abbreviations : LMAO, pls, DM\n",
        "- people names + brands : Obama, Kindle, Nike, Susan Boyle\n",
        "- html characters : &amp, &lt, &gt\n",
        "- urls and links : http://bit.ly/1e1xQ6, http://t.co/3wEwWZi, http://t.co/3wEwWZi, ...\n",
        "- weird punctuation : --, -, \n",
        "- unexpected lowercases, uppercases\n",
        "- emojis with punctuation : :), :-), =D\n",
        "- ...\n",
        "\n",
        "Analyse data to find out what kind of pre-processing is needed.\n",
        "\n",
        "1. Put everything to lowercase (in the dataset, it does not seem that lowercase and uppercase play a significant enough role in the sentiment of the tweet)\n",
        "\n",
        "2. URLs : remove them, they won't be useful for the model\n",
        "3. HTML characters : remove them, they won't be useful for the model\n",
        "4. Emojis : I don't know if we should remove them or not, they may be useful for the model because they can be used to express emotions. For instance, we could imagine to replace them by their meaning : :) could be \"smile\", :D could be \"laugh\", :/ could be \"sad\", etc. However, it is not clear that the model will be able to understand the meaning of the emojis. For the time being, I have removed them.\n",
        "5. Punctuation : remove it, it won't be useful for the model\n",
        "6. Numbers : remove them, they won't be useful for the model\n",
        "7. Abbreviations : replace them with their full form, they may be useful for the model. We can maybe use a dictionary to replace them (custom) --> Maybe this is overkill compared to the actual gain of doing that. Or we can use a library like TextBlob to replace them (not custom).\n",
        "8. Hashtags : remove them, they won't be useful for the model\n",
        "9. @ : remove them, they won't be useful for the model\n",
        "10. People names : remove them, they won't be useful for the model\n",
        "11. Removing stopwords : remove them, they won't be useful for the model\n",
        "\n",
        "Additionnaly we can use techniques as :\n",
        "- Lemmatization : converting a word to its base form. For instance : \"running\" -> \"run\", \"better\" -> \"good\", \"am\" -> \"be\", etc.\n",
        "- Stemming : removing the suffixes of a word. For instance : \"running\" -> \"run\", \"better\" -> \"better\", \"tarts\" -> \"tart\", etc. Only removes suffixes, not prefixes. Stemming is faster than lemmatization but less accurate.\n",
        "\n",
        "Example where lemmatization and stemming are applied :\n",
        "- Lemmatization : \"Caring\" -> \"Care\"\n",
        "- Stemming : \"Caring\" -> \"Car\"\n",
        "\n",
        "Obviously, \"Care\" is not the base form of \"Caring\" and \"Car\" is not the base form of \"Caring\" either.\n",
        "There are many ways to implement them but the most common one is to use the NLTK package.\n",
        "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqU0E4AypktY"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zkfaRdJWuoB"
      },
      "source": [
        "#### Lemmization, Stemming and Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ1lRVQ98ioU"
      },
      "outputs": [],
      "source": [
        "def lemmatize(df):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  df.text = df.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "  return df\n",
        "\n",
        "def stem(df):\n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  df.text = df.text.apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "  return df\n",
        "\n",
        "def removeStopwords(df):\n",
        "  english_stopwords = stopwords.words('english')\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item not in english_stopwords])\n",
        "\n",
        "  return df\n",
        "\n",
        "def expandContractions(df):\n",
        "  df.text = df.text.apply(lambda x: [contractions.fix(word) for word in x])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBhCnWUvrNBI"
      },
      "outputs": [],
      "source": [
        "def preProcess(df, lemmatization = True, stemming = False, remove_stopwords = True):\n",
        "  \n",
        "  # Lowercase the text\n",
        "  df.text = df.text.str.lower()\n",
        "  # Remove urls \n",
        "  df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "  df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "  # Remove html reference characters\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "  # Remove hashtags (must be done before removing punctuation)\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'#', '', x))\n",
        "  # Remove mentions (must be done before removing punctuation)\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'@\\w+', '', x))\n",
        "  # Remove numbers\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "  # Remove special characters\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'\\[[^]]*\\]', '', x))\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'[^a-zA-z0-9\\s]', '', x))\n",
        "\n",
        "  print(\"Special characters removed\")\n",
        "\n",
        "  # Tokenize\n",
        "  tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
        "  df['text'] = df.apply(lambda row: tokenizer(row['text']), axis=1)\n",
        "\n",
        "  print(\"Text tokenized\")\n",
        "\n",
        "  # Expand contractions\n",
        "  df = expandContractions(df)\n",
        "\n",
        "  if remove_stopwords:\n",
        "    # Remove stopwords\n",
        "    df = removeStopwords(df)\n",
        "\n",
        "  print(\"Stop words removed\")\n",
        "    \n",
        "  if lemmatization:\n",
        "    # Lemmatization\n",
        "    df = lemmatize(df)\n",
        "\n",
        "  if stemming:\n",
        "    # Stemming\n",
        "    df = stem(df)\n",
        "\n",
        "  print(\"Lemmatized and stemmatized\")\n",
        "\n",
        "    \n",
        "  # Remove punctuation tokens in each row of the text column\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item not in string.punctuation])\n",
        "  # Remove any weird characters tokens\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item.isalpha()])\n",
        "  # Remove any tokens with length less than 2\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if len(item) > 2])\n",
        "  # Remove any repeated tokens\n",
        "  df.text = df.text.apply(lambda x: list(OrderedDict.fromkeys(x)))\n",
        "\n",
        "  print(\"Punctuation removed\")\n",
        "\n",
        "  # Remove any non-english words\n",
        "  english_words = set(nltk.corpus.words.words())\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item in english_words])\n",
        "\n",
        "  print(\"Non-english words removed\")\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTHoUz8Cpnun"
      },
      "outputs": [],
      "source": [
        "def loadData(data_path):\n",
        "  # Load CSV file\n",
        "  df = pd.read_csv(data_path, encoding='latin-1')\n",
        "\n",
        "  # Add sentiment and text column header\n",
        "  column_names = ['text', 'sentiment']\n",
        "  df = pd.DataFrame(zip(df.iloc[:, 1], df.iloc[:, 2]), columns=column_names)\n",
        "\n",
        "  # Pre-process\n",
        "  df = preProcess(df)\n",
        "\n",
        "  # Vocabulary size \n",
        "  # Build the vocabulary\n",
        "  voc = set()\n",
        "  for text in df.text:\n",
        "    for word in text:\n",
        "      voc.add(word)\n",
        "\n",
        "  return df, len(voc), voc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZmZfLA4TZ7f",
        "outputId": "e9bf9b2f-5c0b-4b69-dea6-9ca4b4070c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train data...\n",
            "Special characters removed\n",
            "Text tokenized\n",
            "Stop words removed\n",
            "Lemmatized and stemmatized\n",
            "Punctuation removed\n",
            "Non-english words removed\n",
            "\n",
            "Loading test data...\n",
            "Special characters removed\n",
            "Text tokenized\n",
            "Stop words removed\n",
            "Lemmatized and stemmatized\n",
            "Punctuation removed\n",
            "Non-english words removed\n"
          ]
        }
      ],
      "source": [
        "# Read the data at the path\n",
        "data_path = root + '/data/'\n",
        "print(\"Loading train data...\")\n",
        "df_train, vocab_size_train, voc_train = loadData(data_path + 'train_imdb.csv')\n",
        "print(\"\\nLoading test data...\")\n",
        "df_test, vocab_size_test, voc_test = loadData(data_path + 'test_imdb.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5MMAWJABX1Z",
        "outputId": "6e193fe7-dcee-4b55-8594-830e10f9582d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size:  45000\n",
            "negative    22519\n",
            "positive    22481\n",
            "Name: sentiment, dtype: int64\n",
            "                                                text sentiment\n",
            "0  [recently, watching, show, say, really, made, ...  positive\n",
            "1  [return, often, wrong, rather, right, shame, l...  positive\n",
            "2  [remember, movie, came, year, old, commodore, ...  negative\n",
            "3  [know, last, reviewer, talking, show, pure, en...  positive\n",
            "4  [beginning, excited, see, movie, poster, possi...  positive\n"
          ]
        }
      ],
      "source": [
        "print(\"Train size: \", df_train.text.size)\n",
        "print(df_train.sentiment.value_counts())\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGWDZr4uBpP6",
        "outputId": "2368a4db-b43a-41e0-812f-29e493c2a835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test size:  5000\n",
            "positive    2519\n",
            "negative    2481\n",
            "Name: sentiment, dtype: int64\n",
            "                                                text sentiment\n",
            "0  [really, due, look, arena, curtain, overall, i...  positive\n",
            "1  [many, television, show, appeal, quite, differ...  positive\n",
            "2  [film, quickly, get, major, chase, scene, ever...  negative\n",
            "3  [jane, would, definitely, approve, awesome, jo...  positive\n",
            "4  [expectation, somewhat, high, went, see, movie...  negative\n"
          ]
        }
      ],
      "source": [
        "print(\"Test size: \", df_test.text.size)\n",
        "print(df_test.sentiment.value_counts())\n",
        "print(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZgkwmLqYJrE",
        "outputId": "1e51a6bb-59c2-4ba6-df7a-435279dd5f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (45000, 2)\n",
            "Test data shape:  (5000, 2)\n",
            "Vocabulary size:  30615\n",
            "Vocabulary size:  17650\n",
            "Vocabulary train:  ['sickeningly', 'wary', 'sagittarius', 'outwit', 'progovernment', 'indignation', 'employed', 'unyieldingly', 'insufficiency', 'asbestos']\n",
            "Vocabulary test:  ['restrictive', 'sickeningly', 'variously', 'wary', 'veer', 'snuggle', 'diversity', 'nodule', 'escapist', 'outwit']\n",
            "Train data sample: \n",
            "                                                 text sentiment\n",
            "0  [recently, watching, show, say, really, made, ...  positive\n",
            "1  [return, often, wrong, rather, right, shame, l...  positive\n",
            "2  [remember, movie, came, year, old, commodore, ...  negative\n",
            "3  [know, last, reviewer, talking, show, pure, en...  positive\n",
            "4  [beginning, excited, see, movie, poster, possi...  positive\n",
            "Test data sample: \n",
            "                                                 text sentiment\n",
            "0  [really, due, look, arena, curtain, overall, i...  positive\n",
            "1  [many, television, show, appeal, quite, differ...  positive\n",
            "2  [film, quickly, get, major, chase, scene, ever...  negative\n",
            "3  [jane, would, definitely, approve, awesome, jo...  positive\n",
            "4  [expectation, somewhat, high, went, see, movie...  negative\n"
          ]
        }
      ],
      "source": [
        "print('Train data shape: ', df_train.shape)\n",
        "print('Test data shape: ', df_test.shape)\n",
        "print('Vocabulary size: ', vocab_size_train)\n",
        "print('Vocabulary size: ', vocab_size_test)\n",
        "print('Vocabulary train: ', list(voc_train)[:10])\n",
        "print('Vocabulary test: ', list(voc_test)[:10])\n",
        "print('Train data sample: \\n', df_train.head())\n",
        "print('Test data sample: \\n', df_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FsWOb4-sYF4"
      },
      "source": [
        "## Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATKXjICmsYF4"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary for the training data\n",
        "vocabulary = set()\n",
        "\n",
        "# Add special tokens to the vocabulary\n",
        "pad = \"<PAD>\" # Used to pad short sentences to the MAX_TOKENS length\n",
        "sos = \"<SOS>\" # Start-of-sentence\n",
        "eos = \"<EOS>\" # End-of-sentence\n",
        "ukn = \"<UKN>\" # Unknown word\n",
        "\n",
        "vocabulary.add(pad)\n",
        "vocabulary.add(sos)\n",
        "vocabulary.add(eos)\n",
        "vocabulary.add(ukn)\n",
        "\n",
        "\n",
        "MAX_TOKENS = 0 # Maximum number of tokens in a sentence\n",
        "\n",
        "# Iterate over the training data\n",
        "for row in range(len(df_train)):\n",
        "      # Each row[\"text\"] is a list of tokens\n",
        "      # Each token is a word that has to be added to the vocabulary\n",
        "      MAX_TOKENS = max(MAX_TOKENS, len(df_train.loc[row, \"text\"]))\n",
        "      # Save the row that has the maximum number of tokens\n",
        "      if MAX_TOKENS == len(df_train.loc[row, \"text\"]):\n",
        "        max_row = df_train.loc[row, \"text\"]\n",
        "        row_index = row\n",
        "      tokens = df_train.loc[row, \"text\"]\n",
        "      for word in tokens:\n",
        "        vocabulary.add(word)\n",
        "            \n",
        "vocab_size = len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvvErr8d2Qg7",
        "outputId": "7baa70d4-3e37-4e06-8ebd-e57c99d8edf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "692\n",
            "30619\n",
            "['spoiler', 'although', 'many', 'commentator', 'film', 'term', 'fit', 'poorly', 'quote', 'encyclopedia', 'fantastic', 'incongruous', 'imagery', 'one', 'explain', 'unimaginative', 'way', 'plucky', 'boy', 'large', 'seeking', 'fortune', 'driver', 'seat', 'red', 'mustang', 'could', 'curious', 'might', 'read', 'said', 'lad', 'behind', 'wheel', 'sport', 'car', 'surely', 'protest', 'fantasy', 'incongruity', 'offer', 'mostly', 'appear', 'within', 'first', 'fifteen', 'minute', 'thereafter', 'get', 'iteration', 'squalid', 'progression', 'far', 'soon', 'prof', 'predictable', 'hand', 'literally', 'believable', 'unfair', 'tax', 'particular', 'flaw', 'plausible', 'suspension', 'disbelief', 'fallen', 'precipitously', 'typical', 'viewer', 'scale', 'value', 'ever', 'since', 'raider', 'lost', 'ark', 'hallucinatory', 'know', 'hallucination', 'part', 'knowing', 'rate', 'people', 'enjoy', 'drug', 'result', 'loss', 'anyone', 'would', 'take', 'course', 'occasional', 'bad', 'trip', 'movie', 'must', 'pun', 'juxtaposition', 'word', 'startling', 'time', 'startle', 'god', 'like', 'something', 'damn', 'two', 'interjection', 'par', 'script', 'sadly', 'sense', 'pass', 'dialogue', 'reveal', 'direct', 'proportion', 'naivete', 'regarding', 'speech', 'pattern', 'rising', 'world', 'completely', 'defined', 'minutely', 'make', 'rational', 'indeed', 'cartoon', 'elementary', 'school', 'newspaper', 'numerous', 'guest', 'star', 'cast', 'cameo', 'role', 'even', 'intelligent', 'little', 'hero', 'blink', 'eyelash', 'need', 'several', 'second', 'concoct', 'lie', 'character', 'despite', 'nearly', 'every', 'scene', 'significant', 'development', 'scant', 'reward', 'enough', 'wish', 'better', 'understand', 'coming', 'vaguely', 'story', 'protagonist', 'struggling', 'fully', 'recognizable', 'realistically', 'painted', 'image', 'removed', 'normal', 'context', 'ambiguous', 'paradoxical', 'shocking', 'framework', 'see', 'succession', 'stereotypical', 'dilapidated', 'billboard', 'filling', 'station', 'eatery', 'cheap', 'hotel', 'habitue', 'along', 'country', 'highway', 'exactly', 'largely', 'responsible', 'traditional', 'emphasis', 'content', 'sum', 'picture', 'millionaire', 'dressed', 'clown', 'pirate', 'posh', 'costume', 'party', 'sitting', 'serene', 'mute', 'cautious', 'chauffeur', 'inch', 'fragile', 'skiff', 'sea', 'desperate', 'humanity', 'implore', 'window', 'smear', 'glass', 'blood', 'imagine', 'stadium', 'full', 'abandoned', 'antique', 'limousine', 'white', 'piano', 'ghost', 'detritus', 'wander', 'exhausted', 'ailing', 'woman', 'cling', 'becoming', 'fall', 'asleep', 'side', 'grass', 'feast', 'transfiguration', 'day', 'brilliant', 'flash', 'horizon', 'sun', 'finding', 'consort', 'become', 'corpse', 'belief', 'soul', 'going', 'heaven', 'later', 'innocently', 'learned', 'new', 'today', 'taking', 'photograph', 'sample', 'cinematic', 'surrealism', 'whose', 'irony', 'ripple', 'invade', 'title', 'empire', 'seek', 'please', 'miss', 'ala', 'however', 'hard', 'tread', 'accelerator', 'race', 'chariot', 'beyond', 'desert', 'exquisitely', 'strange', 'rich', 'subtle', 'gorgeous', 'await', 'poor', 'none', 'necessarily', 'though', 'somewhat', 'disappointed', 'cannot', 'dismiss', 'view', 'respectability', 'another', 'genre', 'exemplify', 'sure', 'also', 'expressionism', 'existentialism', 'pessimism', 'amidst', 'omnipotent', 'power', 'structure', 'try', 'size', 'theater', 'turning', 'article', 'style', 'amazed', 'extent', 'absurd', 'valid', 'artistic', 'objection', 'vanish', 'puff', 'smoke', 'entire', 'text', 'support', 'attempt', 'show', 'human', 'situation', 'essentially', 'devoid', 'purpose', 'humankind', 'left', 'feeling', 'hopeless', 'bewildered', 'anxious', 'instantaneously', 'getting', 'away', 'depressing', 'home', 'life', 'among', 'parent', 'find', 'purposeless', 'drive', 'past', 'glittering', 'reading', 'win', 'lottery', 'promise', 'already', 'revealed', 'ambition', 'illusory', 'game', 'never', 'corporation', 'intention', 'trick', 'confuse', 'leave', 'crestfallen', 'aspirant', 'ultimately', 'playwright', 'therefore', 'logical', 'dramatic', 'action', 'conventionally', 'understood', 'frantically', 'perform', 'busyness', 'serf', 'underscore', 'fact', 'nothing', 'change', 'existence', 'timeless', 'circular', 'quality', 'language', 'play', 'repetition', 'obvious', 'sound', 'nonsense', 'underneath', 'sometimes', 'comic', 'surface', 'underlying', 'message', 'metaphysical', 'distress', 'obsession', 'silly', 'inane', 'plot', 'device', 'wherein', 'divine', 'bleak', 'future', 'return', 'moment', 'different', 'still', 'turn', 'much', 'admirer', 'anyway', 'really', 'instead', 'addition', 'work', 'quite', 'stop', 'disillusionment', 'love', 'attendant', 'met', 'person', 'decently', 'service', 'advertising', 'tutelage', 'waiting', 'note', 'famous', 'preoccupation', 'furthermore', 'indirect', 'previous', 'encounter', 'badly', 'maimed', 'arm', 'straight', 'horizontally', 'last', 'protege', 'say', 'want', 'hear', 'music', 'finger', 'end', 'beckoning', 'closer', 'finally', 'author', 'happen', 'currently', 'theologian', 'acknowledged', 'architect', 'undergirder', 'liberation', 'theology', 'catholic', 'movement', 'perhaps', 'police', 'brutality', 'corporate', 'greed', 'cliche', 'cinema', 'literature', 'sentiment', 'impressive', 'warrant', 'scripture', 'tradition', 'expose', 'earthly', 'activity', 'angel', 'principality', 'wrote', 'popular', 'three', 'institution', 'ideology', 'commend', 'worship', 'making', 'false', 'deeply', 'involved', 'becomes', 'slave', 'promising', 'control', 'immortality', 'inexorably', 'deliver', 'helplessness', 'chaos', 'death', 'yet', 'beguile', 'dominion', 'earth', 'book', 'genesis', 'bent', 'inevitably', 'hegemony', 'mistranslation', 'accurate', 'rendering', 'stewardship', 'quibble', 'beside', 'fundamental', 'problem', 'neglect', 'notice', 'reason', 'assume', 'descendent', 'exercise', 'contrary', 'demonic', 'force', 'stolen', 'add', 'observation', 'lewis', 'man', 'conquest', 'nature', 'mere', 'illusion', 'ruse', 'cover', 'talking', 'men', 'instrument', 'secondly', 'satan', 'kind', 'may', 'dangle', 'pleasure', 'niggardly', 'withdraw', 'firmly', 'thrall', 'leaving', 'prey', 'front', 'fire', 'miserably', 'sorry', 'seething', 'insight', 'seem', 'mirrored', 'remarkably', 'experience', 'nice', 'least', 'pretty', 'prior', 'falling', 'victim', 'sign', 'glisten', 'glamorously', 'longer', 'journey', 'towards', 'headquarters', 'shabby', 'lonely', 'meeting', 'else', 'giving', 'card', 'either', 'ruin', 'staffed', 'zombie', 'meet', 'ugly', 'deceitful', 'hostile', 'answer', 'common', 'dictator', 'mean', 'abide', 'totally', 'oblivious', 'partially', 'blinded', 'prematurely', 'aged', 'infantile', 'literal', 'linguistically', 'eventually', 'precious', 'taken', 'crash', 'continue', 'dead', 'wreck', 'long', 'done', 'everything', 'thought', 'present', 'tower', 'receive', 'prize', 'built', 'monument', 'vanity', 'agent', 'evade', 'disappoint', 'insult', 'throw', 'top', 'floor', 'landing', 'body', 'water', 'classic', 'symbolism', 'inevitable', 'put', 'faith', 'fate', 'warning', 'look', 'mutable', 'upon', 'seeing', 'generous', 'selfless', 'act', 'seen', 'almost', 'hour', 'half', 'handicapped', 'hardly', 'able', 'insert', 'hose', 'gas', 'tank', 'help', 'apply', 'job', 'explaining', 'motorist', 'hell', 'place', 'interpretation', 'conjectural', 'surprise', 'outrage', 'cult', 'point', 'cup', 'tea', 'convinced', 'worst', 'made']\n"
          ]
        }
      ],
      "source": [
        "print(MAX_TOKENS)\n",
        "print(vocab_size)\n",
        "print(max_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCi-HitJ1GeC"
      },
      "outputs": [],
      "source": [
        "# For each word in the vocabulary, assign a word id\n",
        "word2index = {}\n",
        "for index, word in enumerate(vocabulary):\n",
        "    word2index[word] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0yOhek36Lew"
      },
      "outputs": [],
      "source": [
        "# For each word id, assign a word\n",
        "index2word = {id: token for token, id in enumerate(word2index)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0zIy4zW9IXy",
        "outputId": "ba6052fd-c7d7-4fa3-c295-1a9a12a8084f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17738\n"
          ]
        }
      ],
      "source": [
        "print(word2index[\"<SOS>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMAGDKBDlRJW",
        "outputId": "de14cf48-00c2-4572-cab6-574ab29ed0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean length of sentences :  77.49162222222222\n"
          ]
        }
      ],
      "source": [
        "# To have an idea, compute statistics on the number of tokens in each sentence\n",
        "mean = 0\n",
        "for row in range(len(df_train)):\n",
        "    mean += len(df_train.loc[row, \"text\"])\n",
        "mean = mean / len(df_train)\n",
        "print(\"Mean length of sentences : \", mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdtW1GzNkTU_"
      },
      "source": [
        "## Build Embedding\n",
        "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "Embedding converts words to integers, and there is a vector corresponding to eachinteger.\n",
        "If there x words in a dictionary, each word will be assigned a value between 1 and x.\n",
        "\n",
        "Embedding types : \n",
        "- GloVe : Global Vectors for Word Representation (pre-trained)\n",
        "- Word2Vec : \n",
        "    - Pre-trained : https://github.com/AI-Trends/NLP-Tutorial/blob/master/Word2vec_pretrained_embeddings.ipynb\n",
        "This first version of word2vec embedding will load the vectors of an already trained model.\n",
        "    - Trained on the fly : https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis\n",
        "Unlike the previous version, we will build a vocabulary and train it with the train dataset from our data.\n",
        "- FastText : Facebook AI Research\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4NFBDMDuOjC"
      },
      "source": [
        "### Embedding matrix\n",
        "When we will have a dataset, its vocabulary, and a dictionary of embedder words and their corresponding vectors.\n",
        "There will still be no correlation between our vocabulary, and the embedder vocabulary.\n",
        "To connect them, we need to create an embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3C6w3I6uOT8"
      },
      "outputs": [],
      "source": [
        "def build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding, glove, model):\n",
        "    print(\"Building embedding matrix...\")\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    # This matrix needs to be populated : \n",
        "    #   For each word in the training vocabulary, the corresponding vector is \n",
        "    #   retrieved from the GloVe dictionary\n",
        "\n",
        "    ukn_cnt = 0     # unknown words counter\n",
        "    ukn_set = set() # set of unknown words\n",
        "    for index, word in enumerate(vocabulary):\n",
        "        if word in embedding:\n",
        "          if glove:\n",
        "            embedding_matrix[index] = embedding[word]\n",
        "          else:\n",
        "            embedding_vector = model[word]\n",
        "            embedding_vector = np.array(embedding_vector)\n",
        "            if embedding_vector is not None:\n",
        "              embedding_matrix[index] = embedding_vector\n",
        "        else:\n",
        "            ukn_cnt += 1\n",
        "            ukn_set.add(word)\n",
        "            embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "    \n",
        "    print(\"Unknown words: \", ukn_cnt)\n",
        "    print(\"Percentage of unknown words: \", ukn_cnt / vocab_size)\n",
        "    \n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi3Ub9VWYJrG",
        "outputId": "a0f39b9e-9156-49e4-d641-83e3ced5a7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained FastText embeddings...\n",
            "Error: embedding dimension is not  300\n",
            "Building embedding matrix...\n",
            "Unknown words:  1552\n",
            "Percentage of unknown words:  0.05068748162905386\n"
          ]
        }
      ],
      "source": [
        "# Choose the embedding parameters\n",
        "EMBEDDING_TYPE = \"fasttext\" # \"glove\" (pre-trained), \"word2vec\" or \"fasttext\"\n",
        "PRE_TRAINED = True # True if using pre-trained embeddings, False otherwise\n",
        "RETRAIN_EMBEDDINGS = True # True if training the embeddings, False otherwise\n",
        "\n",
        "#-----------------------------------------\n",
        "#                  GloVe \n",
        "#-----------------------------------------\n",
        "if EMBEDDING_TYPE == \"glove\":\n",
        "    if PRE_TRAINED:\n",
        "        print(\"Loading pre-trained GloVe embeddings...\")\n",
        "        embedding_path = root + '/Glove/glove.42B.300d.txt'\n",
        "        embedding_dim = 300 # The dimension of the embedding\n",
        "        \n",
        "        embedding_dict = {}\n",
        "        with open(embedding_path, 'r') as f:\n",
        "            for line in f:\n",
        "                tokens = line.split()\n",
        "                word = tokens[0]\n",
        "                vector = np.array(tokens[1:], dtype=np.float32)\n",
        "                if vector.shape[0] == embedding_dim:\n",
        "                    embedding_dict[word] = vector\n",
        "                else:\n",
        "                    print(\"Error: embedding dimension is not \", embedding_dim)\n",
        "        \n",
        "        # Create the embedding matrix   \n",
        "        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, True, embedding_dict)\n",
        "                \n",
        "#-----------------------------------------\n",
        "#                  Word2Vec \n",
        "#-----------------------------------------             \n",
        "elif EMBEDDING_TYPE == \"word2vec\":\n",
        "    if PRE_TRAINED:\n",
        "        print(\"Loading pre-trained Word2Vec embeddings...\")\n",
        "        \n",
        "        embedding_path = root + '/Word2Vec/GoogleNews-vectors-negative300.bin'\n",
        "        embedding_dim = 300 # The dimension of the embedding\n",
        "        \n",
        "        # Load the vocabulary and the corresponding vectors\n",
        "        model_w2v = KeyedVectors.load_word2vec_format(root + '/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "        embedding_dict = model_w2v.vocab\n",
        "        \n",
        "        # Create the embedding matrix\n",
        "        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n",
        "    \n",
        "    else:\n",
        "        if RETRAIN_EMBEDDINGS:\n",
        "            print(\"Training Word2Vec embeddings...\")\n",
        "            #-----------------------------------------\n",
        "            #                 Parameters\n",
        "            #-----------------------------------------\n",
        "            \n",
        "            embedding_dim = 300\n",
        "            w2v_window = 5\n",
        "            w2v_min_count = 10\n",
        "            w2v_workers = 8\n",
        "            train_embeddings_epochs = 30\n",
        "            \n",
        "            # Train the word2vec model\n",
        "            model_w2v = gensim.models.word2vec.Word2Vec(size=embedding_dim, window=w2v_window, min_count=w2v_min_count, workers=w2v_workers)\n",
        "            model_w2v.build_vocab(df_train.text)\n",
        "            words = model_w2v.wv.vocab.keys()\n",
        "            model_w2v.train(df_train.text, total_examples=len(df_train), epochs=train_embeddings_epochs)\n",
        "            model_w2v.save(root + \"/Word2Vec/w2v.model\")\n",
        "            embedding_dict = model_w2v.wv\n",
        "            \n",
        "            # Create the embedding matrix\n",
        "            embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n",
        "            \n",
        "        else:\n",
        "            print(\"Loading Word2Vec embeddings...\")\n",
        "            # Can avoid the training by loading the saved model\n",
        "            model_w2v = gensim.models.word2vec.Word2Vec.load(root + '/Word2Vec/w2v.model')\n",
        "            embedding_dim = 300\n",
        "            embedding_dict = model_w2v.wv\n",
        "        \n",
        "            # Create the embedding matrix\n",
        "            embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n",
        "            \n",
        "elif EMBEDDING_TYPE == \"fasttext\":\n",
        "    if PRE_TRAINED:\n",
        "        print(\"Loading pre-trained FastText embeddings...\")\n",
        "        \n",
        "        embedding_path = root + '/FastText/wiki-news-300d-1M.vec'\n",
        "        embedding_dim = 300\n",
        "        \n",
        "        # Load the vocabulary and the corresponding vectors\n",
        "        embedding_dict = {}\n",
        "        with open(embedding_path, 'r') as f:\n",
        "            for line in f:\n",
        "                tokens = line.split()\n",
        "                word = tokens[0]\n",
        "                vector = np.array(tokens[1:], dtype=np.float32)\n",
        "                if vector.shape[0] == embedding_dim:\n",
        "                    embedding_dict[word] = vector\n",
        "                else:\n",
        "                    print(\"Error: embedding dimension is not \", embedding_dim)\n",
        "                    \n",
        "        # Create the embedding matrix\n",
        "        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, True, embedding_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGEzA_3LYJrG",
        "outputId": "753dae10-7e40-4638-cafa-b2d3def6604f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary Size :  999994\n"
          ]
        }
      ],
      "source": [
        "# Check dictionary size : \n",
        "print(\"Dictionary Size : \", len(embedding_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Rp2m2F6-lX"
      },
      "source": [
        "## Padding sequences\n",
        "Some of the tweets are longer than others. \n",
        "In order to train the model on a fixed-length input, we must define an input length that will be the same for all tokens (MAX_TOKENS), and pad all tweets that are smaller than MAX_TOKENS with \\<PAD\\> tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFS-iTNa7phK"
      },
      "outputs": [],
      "source": [
        "# Decide on the maximum length of the sequences (based on the mean computed earlier)\n",
        "seq_length = 100 # mean was around 77"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEJFmLoRq6Xb"
      },
      "outputs": [],
      "source": [
        "#Returns the index if the word is in the vocabulary, otherwise, it returns the index of unkwon words\n",
        "def get_word(w):\n",
        "  if w in word2index:\n",
        "    return word2index[w]\n",
        "  else:\n",
        "    return word2index[\"<UKN>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmMbMzL57UNv"
      },
      "outputs": [],
      "source": [
        "def padding_and_encode(tweet, max_length):\n",
        "  sos_id = [word2index[\"<SOS>\"]]\n",
        "  eos_id = [word2index[\"<EOS>\"]]\n",
        "  pad_id = [word2index[\"<PAD>\"]]\n",
        "  \n",
        "  # Truncate the tweet if it is too long (more than seq_length)\n",
        "  if len(tweet) > max_length:\n",
        "    tweet = tweet[:max_length]\n",
        "    n_pads = 0\n",
        "  else:\n",
        "    n_pads = max_length - len(tweet)\n",
        "    \n",
        "  encoded = [get_word(w) for w in tweet] # encode without embedding here!!!\n",
        "  return sos_id + encoded + eos_id + pad_id*n_pads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxmG78TY9jqx"
      },
      "outputs": [],
      "source": [
        "# Transfrom dataframes into lists\n",
        "X_train = df_train[\"text\"].tolist()\n",
        "X_test = df_test[\"text\"].tolist()\n",
        "y_train = df_train[\"sentiment\"].tolist()\n",
        "y_test = df_test[\"sentiment\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVh-wWVhf_Ef",
        "outputId": "2ba06fb7-49b3-41ac-819f-8853a99894f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['recently', 'watching', 'show', 'say', 'really', 'made', 'laugh', 'appreciate', 'unrealistic', 'aspect', 'along', 'everything', 'else', 'people', 'said', 'realistic', 'reaction', 'dead', 'among', 'thing', 'going', 'accept', 'bring', 'back', 'life', 'completely', 'crazy', 'bit', 'help', 'smiling', 'every', 'episode', 'watched', 'think', 'great', 'take', 'strange', 'subject', 'make', 'watch', 'absolutely', 'love', 'narration', 'add', 'extra', 'wonder', 'whole', 'cannot', 'always', 'compare', 'old', 'writer', 'new', 'one', 'entity', 'definitely', 'give', 'chance', 'enjoy', 'ridiculous', 'part'] positive\n",
            "['return', 'often', 'wrong', 'rather', 'right', 'shame', 'last', 'chronological', 'installment', 'star', 'war', 'saga', 'shining', 'example', 'epic', 'storytelling', 'wrap', 'story', 'line', 'previous', 'movie', 'one', 'grand', 'finale', 'yes', 'cute', 'cuddly', 'bear', 'broaden', 'demographic', 'middle', 'slow', 'bit', 'final', 'hour', 'best', 'piece', 'entire', 'luke', 'finally', 'come', 'face', 'recognizable', 'villain', 'many', 'thing', 'people', 'tend', 'overlook', 'incredible', 'conclusion', 'went', 'slightly', 'implausible', 'empire', 'strike', 'back', 'convincing', 'exciting', 'opening', 'palace', 'masterful', 'performance', 'emperor', 'coming', 'resolution', 'solo', 'romance', 'extremely', 'powerful', 'moment', 'slight', 'annoyance', 'generation', 'time', 'every', 'single', 'scene', 'still', 'magical', 'moving', 'cinema', 'also', 'serf', 'great', 'chapter', 'good', 'fantastic'] positive\n",
            "['remember', 'movie', 'came', 'year', 'old', 'commodore', 'play', 'therefore', 'really', 'got', 'buy', 'cheap', 'put', 'man', 'bad', 'sylvester', 'say', 'like', 'word', 'entire', 'except', 'awful', 'sentimental', 'speech', 'end', 'expression', 'face', 'way', 'stupid', 'love', 'thing', 'middle', 'amazingly', 'predictable', 'ended', 'fast', 'forwarding', 'went', 'exchange', 'something', 'else'] negative\n",
            "['know', 'last', 'reviewer', 'talking', 'show', 'pure', 'entertainment', 'basically', 'dude', 'put', 'competition', 'club', 'pick', 'girl', 'different', 'scenario', 'mix', 'every', 'time', 'panel', 'judge', 'afraid', 'call', 'people', 'admit', 'recognize', 'game', 'break', 'guy', 'wrong', 'right', 'contestant', 'weak', 'strong', 'always', 'entertaining', 'relate', 'seen', 'real', 'doubt'] positive\n",
            "['beginning', 'excited', 'see', 'movie', 'poster', 'possibly', 'ever', 'seen', 'immediately', 'bought', 'one', 'dorm', 'every', 'element', 'came', 'together', 'beautifully', 'often', 'many', 'penis', 'gay', 'racial', 'joke', 'critic', 'rest', 'cast', 'deliver', 'sensationally', 'remains', 'sweet', 'throughout', 'entire', 'end', 'succeed', 'relationship', 'get', 'laid', 'supporting', 'brutal', 'problem', 'lady', 'thing', 'abundance', 'memorable', 'scene', 'given', 'make', 'easy', 'remember', 'fondly', 'brought', 'word', 'chest', 'waxing', 'watching', 'theater', 'older', 'people', 'watch', 'saw', 'group', 'four', 'mids', 'woman', 'come', 'despite', 'audience', 'still', 'filled', 'think', 'type', 'like', 'fan', 'office', 'lot', 'remind', 'family', 'guy', 'shallow', 'enough', 'adolescent', 'boy', 'clever', 'middle', 'aged', 'recommend', 'going', 'profanity', 'easily', 'offended', 'however', 'good', 'humor', 'funny'] positive\n",
            "['film', 'moving', 'without', 'sentimental', 'meaningful', 'pretentious', 'tell', 'simple', 'story', 'family', 'danger', 'falling', 'apart', 'encroachment', 'technology', 'advancing', 'society', 'make', 'business', 'increasingly', 'acting', 'wonderful', 'though', 'none', 'west', 'likely', 'actor', 'long', 'ago', 'play', 'character', 'honesty', 'reverence', 'flawed', 'major', 'weakness', 'utter', 'humanity', 'kindness', 'impossible', 'become', 'engaged', 'need', 'like', 'western'] positive\n",
            "['movie', 'think', 'like', 'amazing', 'intelligent', 'people', 'talented', 'even', 'fall', 'really', 'done', 'one', 'recent', 'year', 'come', 'back', 'wilderness', 'made', 'mark', 'truly', 'great', 'film', 'budget', 'cute', 'actress', 'intense', 'thought', 'would', 'excel', 'sentimental', 'emotional', 'role', 'ironically', 'care', 'sheer', 'incompetence', 'carelessness', 'awful', 'acting', 'banal', 'background', 'music', 'insensitive', 'direction', 'make', 'real', 'blunder', 'rule', 'german', 'police', 'issue', 'public', 'instruction', 'loud', 'speaker', 'communicate', 'among', 'unfortunately', 'become', 'standard', 'main', 'line', 'excellent', 'director', 'many', 'bad', 'worse', 'thing', 'majority', 'song', 'saving', 'please', 'repeat', 'find', 'good', 'low', 'villain', 'shaved', 'head', 'instead', 'wig', 'know'] negative\n",
            "['really', 'say', 'felt', 'movie', 'right', 'essence', 'mind', 'game', 'dreamy', 'reality', 'enter', 'aspect', 'future', 'faced', 'cruise', 'much', 'acting', 'prowess', 'must', 'impress', 'every', 'point', 'simply', 'due', 'engaging', 'also', 'self', 'lead', 'merge', 'speak', 'beautiful', 'however', 'come', 'average', 'flick', 'weekend', 'pick', 'random', 'watch', 'gleefully', 'carry', 'strong', 'sentiment', 'character', 'wash', 'one', 'beer', 'popcorn', 'certainly', 'need'] positive\n",
            "['one', 'word', 'young', 'demi', 'look', 'good', 'pregnant', 'point', 'movie', 'scary', 'first', 'scene', 'little', 'could', 'render', 'better', 'cloud', 'effect', 'get', 'old', 'well', 'worth', 'something', 'instead', 'like', 'exorcist', 'next', 'drama', 'part', 'beginning', 'simply'] negative\n",
            "['well', 'done', 'spooky', 'horror', 'movie', 'film', 'company', 'usually', 'put', 'really', 'cheesy', 'like', 'devil', 'bat', 'flying', 'serpent', 'german', 'expatriate', 'director', 'wonder', 'small', 'budget', 'swamp', 'set', 'gaunt', 'ghoulish', 'effective', 'strangler'] positive\n"
          ]
        }
      ],
      "source": [
        "# Print the 10 first elements of the training set\n",
        "for i in range(10):\n",
        "  print(X_train[i], y_train[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aXbt9gr-D-1"
      },
      "outputs": [],
      "source": [
        "# Encode \n",
        "X_train = [padding_and_encode(tweet, seq_length-2) for tweet in X_train] # -2 because of SOS and EOS\n",
        "X_test = [padding_and_encode(tweet, seq_length-2) for tweet in X_test] # -2 because of SOS and EOS\n",
        "# Make sure that the values in y_train are either 0 (0) or 1 (instead of 4)\n",
        "y_train = [0 if y == 'negative' else 1 for y in y_train]\n",
        "y_test = [0 if y == 'negative' else 1 for y in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMaiJunpkjTA",
        "outputId": "340181b7-f3fd-4d68-8022-da2a5e09f8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17738, 23660, 8821, 15183, 9367, 8422, 16617, 870, 8380, 21714, 27504, 9372, 12790, 25202, 28617, 9705, 24260, 30355, 8282, 24930, 579, 9612, 21675, 458, 8859, 24500, 18777, 30337, 26156, 5820, 1990, 16491, 2864, 23119, 14964, 26806, 3115, 12305, 2546, 10775, 5984, 22409, 9460, 7841, 19053, 7278, 14738, 8877, 25461, 10631, 29147, 24840, 10199, 12296, 26707, 28376, 23230, 20450, 27869, 4173, 2432, 14238, 1515, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197] 1\n",
            "[17738, 28202, 16657, 932, 28645, 630, 10458, 29467, 4424, 5999, 20325, 10436, 22175, 4807, 10640, 28296, 7456, 18371, 10688, 3694, 15913, 1433, 26707, 6897, 3435, 9023, 16971, 19980, 13139, 16596, 16658, 9630, 16589, 26156, 14765, 20753, 14263, 3043, 10164, 2280, 4413, 1298, 21722, 4311, 23097, 19921, 579, 28617, 14435, 19941, 12347, 25668, 8438, 7681, 24290, 11075, 4003, 8859, 28226, 30216, 289, 11239, 24823, 17180, 17767, 26171, 15938, 3259, 29003, 11228, 15007, 3886, 16693, 16148, 26438, 23292, 16491, 2646, 2912, 27237, 11111, 16750, 14013, 19019, 19983, 26806, 4018, 27557, 26206, 1515, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197] 1\n",
            "[17738, 26101, 1433, 26760, 18178, 24840, 6075, 1782, 23573, 8422, 7660, 6659, 19426, 724, 22241, 1361, 22034, 9367, 14563, 10767, 10164, 18721, 5101, 9062, 29355, 20733, 6528, 21722, 14306, 18921, 9460, 579, 9630, 10454, 8343, 4122, 7569, 25165, 8438, 5687, 13573, 25202, 1515, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197, 4197] 0\n"
          ]
        }
      ],
      "source": [
        "for i, j in zip(X_train[:3], y_train[:3]):\n",
        "  print(i, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2gwgoMKuYMb"
      },
      "source": [
        "Since we only have a training set and test set, we split the training set into training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkFvR1cYudfO",
        "outputId": "c22890fd-c7ea-458b-a102-2f26b39916a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples :  36000\n",
            "Number of validation examples :  9000\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
        "                                                  random_state = 42, \n",
        "                                                  shuffle = True, \n",
        "                                                  test_size = 0.2)\n",
        "\n",
        "# Now truncate the dataset because there is too much data\n",
        "keep_ratio = 1\n",
        "X_train, y_train = X_train[:int(len(X_train)*keep_ratio)], y_train[:int(len(y_train)*keep_ratio)]\n",
        "X_val, y_val = X_val[:int(len(X_val)*keep_ratio)], y_val[:int(len(y_val)*keep_ratio)]\n",
        "\n",
        "print(\"Number of training examples : \", len(X_train))\n",
        "print(\"Number of validation examples : \", len(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj-wn4z-xQLt",
        "outputId": "29a4fb08-43eb-4875-dbeb-ec96eddb8b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17738, 24344, 226, 18823, 5180, 631, 29927, 3570, 16491, 25100, 22925, 24267, 7574, 289, 5606, 904, 6554, 5078, 19145, 15187, 26157, 20710, 1433, 3964, 26999, 13117, 15121, 3528, 29058, 22587, 6275, 12877, 8326, 14067, 27018, 14164, 22293, 10624, 1485, 27069, 14554, 19138, 1636, 4844, 18192, 27573, 11308, 23888, 20601, 30553, 12296, 2379, 10607, 28627, 27985, 29948, 7332, 10688, 8412, 21059, 425, 5316, 10631, 17655, 2960, 26052, 15411, 14263, 3694, 18219, 17180, 6238, 705, 8877, 18441, 19977, 27557, 25461, 9328, 9800, 22305, 16383, 9155, 15787, 849, 25893, 15380, 1294, 9674, 1525, 20450, 8745, 7181, 15044, 10939, 15228, 27198, 19149, 13551, 1515] 1\n",
            "[17738, 19921, 9474, 13480, 6974, 27476, 12911, 15183, 26707, 14263, 162, 8422, 8380, 29346, 20534, 5314, 23292, 24488, 4826, 8362, 418, 3914, 2400, 22002, 23626, 24840, 18134, 25718, 22408, 9612, 26249, 25243, 12783, 1250, 7126, 14288, 16181, 7464, 16486, 8660, 19055, 21722, 11782, 4973, 1086, 5630, 2178, 18492, 10495, 6643, 24929, 24500, 18594, 5649, 9460, 23681, 15526, 2967, 15323, 1564, 17140, 12790, 25828, 4724, 17418, 1615, 11794, 17968, 28738, 4128, 2912, 4479, 3630, 284, 6210, 29423, 15082, 3937, 27985, 6811, 25024, 26806, 29923, 25584, 10427, 252, 29977, 2432, 27095, 16750, 16027, 22621, 20761, 15477, 26101, 23778, 2580, 21383, 27399, 1515] 1\n",
            "[17738, 13344, 13537, 29346, 16486, 10586, 15191, 5649, 16886, 27557, 29847, 13480, 26707, 4034, 22071, 3251, 9099, 4791, 10828, 14049, 12651, 24801, 7019, 21730, 30599, 29181, 22921, 12444, 831, 7507, 14563, 18665, 1519, 2960, 14893, 11571, 6442, 1361, 20464, 3809, 30168, 28090, 3207, 27282, 5943, 17765, 13628, 1932, 27237, 11803, 973, 3115, 23299, 14827, 23675, 19075, 26705, 16175, 26806, 10631, 28339, 23055, 7660, 28445, 22378, 24454, 20543, 10047, 8282, 27489, 17434, 15058, 2686, 23292, 3914, 15183, 23473, 28543, 14802, 24022, 21782, 21534, 6891, 24820, 1433, 9388, 11689, 30043, 28862, 24695, 21967, 9123, 15000, 7933, 14404, 29456, 27233, 3990, 20600, 1515] 1\n"
          ]
        }
      ],
      "source": [
        "for i, j in zip(X_train[:3], y_train[:3]):\n",
        "  print(i, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvoZBxauA2ln"
      },
      "source": [
        "# DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za-9ayS9A6Zw"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "X_val = np.array(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=len(X_test), drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdO-D4Z_dCC2"
      },
      "source": [
        "# Document embedding\n",
        "The document embedding will work in a different way from the word embedding. Instead of using an embedding layer into our RNN, we will directly transform the document into its embedded vector and provide that vector as an input to the RNN. This will thus require to use a RNN a bit modified from the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oG3wsiTdZv6",
        "outputId": "aace584c-2587-421f-a3df-0d1c97be206a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading doc2vec...\n"
          ]
        }
      ],
      "source": [
        "document_model = \"doc2vec\"\n",
        "#document_model = \"word2vec-tfidf\"\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "new_df_train = np.zeros((len(df_train), embedding_dim))\n",
        "new_df_test = np.zeros((len(df_test), embedding_dim))\n",
        "i = 0\n",
        "\n",
        "if document_model == \"doc2vec\":\n",
        "\n",
        "  print(\"Loading doc2vec...\")\n",
        "\n",
        "  d2v_model = Doc2Vec(vector_size=embedding_dim, min_count=2, epochs=30)\n",
        "\n",
        "  sentences = [TaggedDocument(sentence, [i]) for i,sentence in enumerate(df_train.text)]\n",
        "\n",
        "  d2v_model.build_vocab(sentences)\n",
        "  d2v_model.wv.vocab\n",
        "  d2v_model.train(sentences, total_examples=len(sentences), epochs=30)\n",
        "\n",
        "  for sentence in df_train.text:\n",
        "    new_df_train[i] = d2v_model.infer_vector(sentence)\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for sentence in df_test.text:\n",
        "    new_df_test[i] = d2v_model.infer_vector(sentence)\n",
        "    i += 1\n",
        "\n",
        "elif document_model == \"word2vec-tfidf\":\n",
        "\n",
        "  print(\"Loading word2vec averaged with tf-idf...\")\n",
        "\n",
        "  # Loading word2vec\n",
        "  embedding_path = root + '/Word2Vec/GoogleNews-vectors-negative300.bin'\n",
        "        \n",
        "  # Load the vocabulary and the corresponding vectors\n",
        "  model_w2v = KeyedVectors.load_word2vec_format(root + '/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "  dictionary = Dictionary()\n",
        "  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in df_train.text]\n",
        "\n",
        "  tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n",
        "\n",
        "  unknown = 0\n",
        "  num_words = 0\n",
        "  #i = 0\n",
        "\n",
        "  for doc in tfidf[bow_corpus]:\n",
        "    if i % 5000 == 0:\n",
        "      print(i, \"/\", len(df_train))\n",
        "\n",
        "    total_freq = 0 #used to normalize the weigths\n",
        "\n",
        "    for id, freq in doc:\n",
        "      total_freq += freq\n",
        "\n",
        "    #Bulding the vector for each document\n",
        "    for id, freq in doc: \n",
        "      num_words += 1\n",
        "      word = dictionary[id]\n",
        "      #print(word)\n",
        "      if word in model_w2v.vocab:\n",
        "        embedding_vector = model_w2v[word]\n",
        "        embedding_vector = np.array(embedding_vector)\n",
        "        new_df_train[i] += (freq/total_freq)*embedding_vector\n",
        "      else:\n",
        "        unknown += 1\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  print(unknown/num_words)\n",
        "\n",
        "  # Now we do the same but for df_test\n",
        "  unknown = 0\n",
        "  num_words = 0\n",
        "  i = 0\n",
        "  dictionary = Dictionary()\n",
        "  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in df_test.text]\n",
        "\n",
        "  tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n",
        "\n",
        "  for doc in tfidf[bow_corpus]:\n",
        "    if i % 500 == 0:\n",
        "      print(i, \"/\", len(df_test))\n",
        "\n",
        "    total_freq = 0 #used to normalize the weigths\n",
        "\n",
        "    for id, freq in doc:\n",
        "      total_freq += freq\n",
        "\n",
        "    #Bulding the vector for each document\n",
        "    for id, freq in doc: \n",
        "      num_words += 1\n",
        "      word = dictionary[id]\n",
        "      #print(word)\n",
        "      if word in model_w2v.vocab:\n",
        "        embedding_vector = model_w2v[word]\n",
        "        embedding_vector = np.array(embedding_vector)\n",
        "        new_df_test[i] += (freq/total_freq)*embedding_vector\n",
        "      else:\n",
        "        unknown += 1\n",
        "\n",
        "    i += 1\n",
        "  print(unknown/num_words)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUKRH6dXdeiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a917896-bc6c-45f0-d97c-0bc1224acd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 300)\n",
            "(45000, 300)\n"
          ]
        }
      ],
      "source": [
        "print(new_df_test.shape)\n",
        "print(new_df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GVkrMrHdf1E"
      },
      "outputs": [],
      "source": [
        "y_train = df_train[\"sentiment\"].tolist()\n",
        "y_test = df_test[\"sentiment\"].tolist()\n",
        "y_train = [0 if y == 'negative' else 1 for y in y_train]\n",
        "y_test = [0 if y == 'negative' else 1 for y in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80cW_V7Cdimz"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(new_df_train, y_train, \n",
        "                                                  random_state = 42, \n",
        "                                                  shuffle = True, \n",
        "                                                  test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUmmB3jrdkQk",
        "outputId": "1dc11ecf-cace-460c-b110-33755df5d591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtNO-s_idk0X",
        "outputId": "07bafd4f-3a34-480e-c5c0-d3780ae04e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(36000, 1, 300)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 10\n",
        "\n",
        "X_train = [[doc] for doc in X_train]\n",
        "X_train = np.array(X_train)\n",
        "print(X_train.shape)\n",
        "X_test = [[doc] for doc in new_df_test]\n",
        "X_test = np.array(X_test)\n",
        "X_val = [[doc] for doc in X_val]\n",
        "X_val = np.array(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbTtyocUWuoC"
      },
      "source": [
        "# Model architecture\n",
        "To train the model, we will use the following architectures :\n",
        "- RNN : \n",
        "  - Embedding layer (pre-trained GloVe, Word2Vec, FastText)\n",
        "    https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
        "    The weights of the embedding_matrix will be loaded into that layer.\n",
        "  - LSTM layer or GRU layer\n",
        "  - Linear layer\n",
        "\n",
        "Or we can use the following architecture :\n",
        "- Transformer architecture (self attention)\n",
        "\n",
        "Or the following architecture : \n",
        "- RNN + Attention\n",
        "  - Embedding layer (pre-trained GloVe, Word2Vec, FastText)\n",
        "  - LSTM layer or GRU layer\n",
        "  - Attention layer\n",
        "  - Linear layer\n",
        "\n",
        "Or the following architecture:\n",
        "- RNN + Attention for document embedding\n",
        "  - LSTM layer or GRU layer\n",
        "  - Attention layer\n",
        "  - Linear layer\n",
        "\n",
        "Finally, we can use the following architecture:\n",
        "- RNN for document embedding : \n",
        "  - LSTM layer or GRU layer\n",
        "  - Linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Reyt1GwSDRRr"
      },
      "outputs": [],
      "source": [
        "def CreateEmbeddingLayer(embedding_matrix, padding_id, non_trainable=False):\n",
        "  # Retrieve dimensions of embedding\n",
        "  num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "  # Initialize nn.Embedding with the pre-trained weights\n",
        "  embedding_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_id)\n",
        "  embedding_layer.load_state_dict({'weight': torch.from_numpy(embedding_matrix)})\n",
        "\n",
        "  if non_trainable:\n",
        "    embedding_layer.weight.requires_grad = False\n",
        "\n",
        "  return embedding_layer, num_embeddings, embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdq7L4R6sJ11"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_length, embedding_dim):\n",
        "  pos_enc = np.array([\n",
        "      [pos / np.power(10000, 2 * (j // 2) / embedding_dim) for j in range(embedding_dim)]\n",
        "      if pos != 0 else np.zeros(embedding_dim) for pos in range(seq_length)\n",
        "  ])\n",
        "  pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
        "  pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
        "  \n",
        "  return torch.from_numpy(pos_enc).type(torch.FloatTensor).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMmiBdkAYJrI"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOrXOnN1AyJs"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, dropout, bidirectional, padding_id, num_layers, type):\n",
        "      super().__init__()\n",
        "\n",
        "      # Embedding layer\n",
        "      self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "\n",
        "      # LSTM or GRU\n",
        "      if type==\"LSTM\":\n",
        "        self.model = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            batch_first = True, \n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "      elif type==\"GRU\":\n",
        "        self.model = nn.GRU(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "\n",
        "      # Fully-connected layer\n",
        "      self.fc = nn.Linear(hidden_dim * 2, 2)\n",
        "      # The output of the fully connected layer is a vector of size 2 containing\n",
        "      # the probability of the tweet to belong to either positive or negative\n",
        "      # class (--> 2 classes)\n",
        "\n",
        "      # Dropout\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      # Embed the input\n",
        "      embs = self.dropout(self.embedding(input))\n",
        "\n",
        "      # pack sequence\n",
        "      # lengths need to be on CPU!\n",
        "      # This will cause the RNN to only process non-padded elements\n",
        "      #packed_embedded = nn.utils.rnn.pack_padded_sequence(embs, input_length.to('cpu'))\n",
        "\n",
        "      # Feed embeddings to LSTM\n",
        "      packed_output, (hidden, cell) = self.model(embs)\n",
        "\n",
        "      # Apply droupout\n",
        "      # Dropout is applied to the concatenation of the final forward and backward hidden layers\n",
        "      # (Because of bidirectionality)\n",
        "      out = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "      # Through fully connected layer\n",
        "      out = self.fc(out)\n",
        "\n",
        "      return out\n",
        "    \n",
        "    def init_hidden(self):\n",
        "      \"\"\"At the beginning of the sequence, there are no hidden state.\n",
        "          Thus, we need to initialise the hidden state vector.\"\"\"\n",
        "\n",
        "      return (torch.zeros(2, batch_size, 32), torch.zeros(2, batch_size, 32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqu-0LAYJrI"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1FDyXSsJ11"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "    # Q, K, V = [batch size, seq len, embedding dim]\n",
        "    energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale\n",
        "    # energy = [batch size, seq len, seq len]\n",
        "    attention = self.softmax(energy)\n",
        "    # attention = [batch size, seq len, seq len]\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    x = self.out(x)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3V5rl-WsJ11"
      },
      "outputs": [],
      "source": [
        "# Attention layer\n",
        "class TransformerAttention(nn.Module):\n",
        "  def __init__(self, embedding_matrix, heads, padding_id, max_sequence_length, dropout=0.1):\n",
        "    super(TransformerAttention, self).__init__()\n",
        "    \n",
        "    # Embedding layer\n",
        "    self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.heads = heads\n",
        "    self.head_dim = embedding_dim // heads\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    \n",
        "    assert (self.head_dim * heads == embedding_dim), \"Embedding dimension should be divisible by heads\"\n",
        "\n",
        "    self.attention = SelfAttention(self.embedding_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Linear layer of size d_model * d_model\n",
        "    self.fc = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
        "    # Linear project layer of size d_model * d_classes\n",
        "    self.fc_out = nn.Linear(embedding_dim, 2)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Fetch embedding from input\n",
        "    embs = self.embedding(inputs)\n",
        "    \n",
        "    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n",
        "    # Add position encoding to the embeddings\n",
        "    embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n",
        "    \n",
        "    # Apply dropout\n",
        "    embs = self.dropout(embs) # (batch_size, seq_length, embedding_dim)\n",
        "    \n",
        "    # Apply self attention once \n",
        "    new_embs, attention = self.attention(embs)\n",
        "      \n",
        "    # Average the attention embeddings\n",
        "    avg = torch.mean(new_embs, dim=1)\n",
        "    \n",
        "    # Feed the average attention heads through a fully connected layer\n",
        "    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n",
        "    \n",
        "    # Linear projection of size d_model * nb_classes\n",
        "    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n",
        "    \n",
        "    # Softmax \n",
        "    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n",
        "  \n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD7uTJeNlRJb"
      },
      "source": [
        "## RNN + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhZoTC1OlRJb"
      },
      "outputs": [],
      "source": [
        "#performer_pytorch FastAttention\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "    # Q, K, V = [batch size, seq len, embedding dim]\n",
        "    energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale\n",
        "    # energy = [batch size, seq len, seq len]\n",
        "    attention = self.softmax(energy)\n",
        "    # attention = [batch size, seq len, seq len]\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    x = self.out(x)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49QVzDe3lRJb"
      },
      "outputs": [],
      "source": [
        "class AttentiveRNN(nn.Module):\n",
        "  def __init__(self, embedding_matrix, hidden_size, bidirectional, padding_id, max_sequence_length, dropout=0.1, type='LSTM'):\n",
        "    super(AttentiveRNN, self).__init__()\n",
        "    \n",
        "    # Embedding layer\n",
        "    self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    # LSTM layer or GRU layer\n",
        "    if type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    else:\n",
        "      self.rnn = nn.GRU(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    \n",
        "    # Attention layer\n",
        "    self.attention = SelfAttention(hidden_size * 2)\n",
        "    \n",
        "    # Linear layer of size d_model * d_model\n",
        "    self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "    # Linear project layer of size d_model * d_classes\n",
        "    self.fc_out = nn.Linear(hidden_size * 2, 2)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Fetch embedding from input\n",
        "    embs = self.embedding(inputs)\n",
        "    \n",
        "    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n",
        "    # Add position encoding to the embeddings\n",
        "    # embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n",
        "    \n",
        "    # Apply dropout\n",
        "    embs = self.dropout(embs) # (batch_size, seq_length, embedding_dim)\n",
        "    \n",
        "    # Apply LSTM\n",
        "    lstm_out, (hidden, cell) = self.rnn(embs)\n",
        "    \n",
        "    # Apply self attention once \n",
        "    new_embs, attention = self.attention(lstm_out)\n",
        "      \n",
        "    # Average the attention embeddings\n",
        "    avg = torch.mean(new_embs, dim=1)\n",
        "    \n",
        "    # Feed the average attention heads through a fully connected layer\n",
        "    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n",
        "    \n",
        "    # Linear projection of size d_model * nb_classes\n",
        "    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n",
        "    \n",
        "    # Softmax \n",
        "    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n",
        "  \n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TLQZrxAenlu"
      },
      "source": [
        "# RNN for document embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSKWsBuuioBK"
      },
      "outputs": [],
      "source": [
        "class RNN_Doc_Emb(nn.Module):\n",
        "    def __init__(self, hidden_dim, dropout, bidirectional, num_layers, type):\n",
        "      super().__init__()\n",
        "\n",
        "      # Embedding layer\n",
        "      #self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "\n",
        "      # LSTM or GRU\n",
        "      if type==\"LSTM\":\n",
        "        self.model = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            batch_first = True, \n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "      elif type==\"GRU\":\n",
        "        self.model = nn.GRU(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "\n",
        "      # Fully-connected layer\n",
        "      self.fc = nn.Linear(hidden_dim * 2, 2)\n",
        "      # The output of the fully connected layer is a vector of size 2 containing\n",
        "      # the probability of the tweet to belong to either positive or negative\n",
        "      # class (--> 2 classes)\n",
        "\n",
        "      # Dropout\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      # Embed the input\n",
        "      embs = self.dropout(input)\n",
        "\n",
        "      # pack sequence\n",
        "      # lengths need to be on CPU!\n",
        "      # This will cause the RNN to only process non-padded elements\n",
        "      #packed_embedded = nn.utils.rnn.pack_padded_sequence(embs, input_length.to('cpu'))\n",
        "\n",
        "      # Feed embeddings to LSTM\n",
        "      packed_output, (hidden, cell) = self.model(input)\n",
        "\n",
        "      # Apply droupout\n",
        "      # Dropout is applied to the concatenation of the final forward and backward hidden layers\n",
        "      # (Because of bidirectionality)\n",
        "      out = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "      # Through fully connected layer\n",
        "      out = self.fc(out)\n",
        "\n",
        "      return out\n",
        "    \n",
        "    def init_hidden(self):\n",
        "      \"\"\"At the beginning of the sequence, there are no hidden state.\n",
        "          Thus, we need to initialise the hidden state vector.\"\"\"\n",
        "\n",
        "      return (torch.zeros(2, batch_size, 32), torch.zeros(2, batch_size, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHNDFvxDeqGk"
      },
      "outputs": [],
      "source": [
        "class AttentiveRNN_Doc_emb(nn.Module):\n",
        "  def __init__(self, hidden_size, bidirectional, dropout=0.1, type='LSTM'):\n",
        "    super(AttentiveRNN_Doc_emb, self).__init__()\n",
        "    \n",
        "    # Embedding layer\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    # LSTM layer or GRU layer\n",
        "    if type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    else:\n",
        "      self.rnn = nn.GRU(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    \n",
        "    # Attention layer\n",
        "    self.attention = SelfAttention(hidden_size * 2)\n",
        "    \n",
        "    # Linear layer of size d_model * d_model\n",
        "    self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "    # Linear project layer of size d_model * d_classes\n",
        "    self.fc_out = nn.Linear(hidden_size * 2, 2)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    # Apply dropout\n",
        "    embs = self.dropout(inputs) # (batch_size, seq_length, embedding_dim)\n",
        "    \n",
        "    # Apply LSTM\n",
        "    lstm_out, (hidden, cell) = self.rnn(embs)\n",
        "    \n",
        "    # Apply self attention once \n",
        "    new_embs, attention = self.attention(lstm_out)\n",
        "      \n",
        "    # Average the attention embeddings\n",
        "    avg = torch.mean(new_embs, dim=1)\n",
        "    \n",
        "    # Feed the average attention heads through a fully connected layer\n",
        "    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n",
        "    \n",
        "    # Linear projection of size d_model * nb_classes\n",
        "    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n",
        "    \n",
        "    # Softmax \n",
        "    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n",
        "  \n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jphW0ft_GJVG"
      },
      "source": [
        "# Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-pCg_J-0QKK"
      },
      "outputs": [],
      "source": [
        "\"\"\"The learnable parametrs of a model are contained in the model's parameters.\n",
        "A state_dict is dictionary that maps each layer to its parameter tensor.\n",
        "The state_dict thus contains all the useful information about the model and must be saved after training. (To maybe be re-loaded later on)\"\"\"\n",
        "def saveModel(model, optimizer):\n",
        "  # Save model\n",
        "  now = time.localtime(time.time())\n",
        "  now_str = time.strftime(\"%a %b %d\", now)\n",
        "\n",
        "  checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict()\n",
        "  }\n",
        "\n",
        "  save_path = root + \"/Models/\" + now_str + \".pth\"\n",
        "  torch.save(checkpoint, save_path)\n",
        "  \n",
        "  \n",
        "def progressBar(loss_training, loss_validation, estimated_time, percent, width = 40):\n",
        "\n",
        "    # Setting up the useful information\n",
        "    left  = width * percent // 100\n",
        "    right = width - left\n",
        "    tags = \"#\" * int(left)\n",
        "    spaces = \" \" * int(right)\n",
        "    percents = f\"{percent:.2f} %\"\n",
        "    loss_training = f\"{loss_training * 1:.6f}\"\n",
        "    loss_validation = f\"{loss_validation * 1:.6f}\"\n",
        "    estimated_time = f\"{estimated_time:.2f}\"\n",
        "\n",
        "    # Displaying a really cool progress bar !\n",
        "    print(\"\\r[\", tags, spaces, \"] - \", percents, \" | Loss (Training) = \", loss_training, \" | Loss (Validation) = \", loss_validation,  \" | Time left : \", estimated_time ,sep=\"\", end=\"\", flush = True)\n",
        "    \n",
        "def binary_accuracy(preds, y):\n",
        "  \"\"\"\n",
        "  Returns the accuracy per batch\n",
        "  \"\"\"\n",
        "\n",
        "  prob_preds = torch.nn.Softmax(dim=-1)(preds)\n",
        "  #print(preds)\n",
        "  #rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "  final_preds = torch.argmax(prob_preds, dim=1)\n",
        "  correct = (final_preds == y).float()\n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ITzWlD0YJrJ"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wja-QRV0HIQj"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRiSJulQjjum"
      },
      "outputs": [],
      "source": [
        "# Global parameters\n",
        "MODEL_NAME = \"AttentiveRNN\" # Can be \"RNN\", or \"Transformer\" or \"AttentiveRNN\" or \"DocumentEmbs\"\n",
        "RNN_TYPE = \"LSTM\" # Can be \"LSTM\" or \"GRU\"\n",
        "DROPOUT = 0.2\n",
        "NB_EPOCHS = 20\n",
        "PADDING_ID = word2index[\"<PAD>\"]\n",
        "# ATTENTION :\n",
        "# Learning the embedding of <PAD> tokens is irrelevant to determine the sentiment of a tweet!\n",
        "# The embedding for this token must stay what it was initialized to. To do that, we need to tell the newtork that # this token must not be learnt. \n",
        "\n",
        "# Automatic save \n",
        "checkpoints = [2, 5, 10, 15, 20]\n",
        "\n",
        "#--------------------------------------#\n",
        "# RNN model parameters\n",
        "HIDDEN_DIM = 32 #tested 256 but barely changed the results\n",
        "BIDIRECTIONAL = True\n",
        "NUM_LAYERS = 2 # For GRUs\n",
        "\n",
        "#--------------------------------------#\n",
        "# Transformer model parameters\n",
        "HEADS = 1\n",
        "\n",
        "\n",
        "#--------------------------------------#\n",
        "#                  MODEL               #\n",
        "#--------------------------------------#\n",
        "if MODEL_NAME == \"RNN\":\n",
        "  model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, RNN_TYPE)\n",
        "elif MODEL_NAME == \"Transformer\":\n",
        "  model = TransformerAttention(embedding, HEADS, PADDING_ID, seq_length, DROPOUT)\n",
        "elif MODEL_NAME == \"AttentiveRNN\":\n",
        "  model = AttentiveRNN(embedding, HIDDEN_DIM, BIDIRECTIONAL, PADDING_ID, seq_length, dropout=DROPOUT, type=RNN_TYPE)\n",
        "elif MODEL_NAME == \"DocumentEmbs\":\n",
        "  model = RNN_Doc_Emb(HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, NUM_LAYERS, RNN_TYPE)\n",
        "  #model = AttentiveRNN_Doc_emb(HIDDEN_DIM, BIDIRECTIONAL, dropout=DROPOUT, type=RNN_TYPE)\n",
        "\n",
        "model = model.to(device) # Move to GPU\n",
        "\n",
        "# Criterion\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Early stopping"
      ],
      "metadata": {
        "id": "1Y_uS6AcpTen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVETTDl6ef0K"
      },
      "outputs": [],
      "source": [
        "patience = 6    # Number of epochs to wait before early stopping\n",
        "epochs_no_improve = 0  # Number of epochs with no improvement in validation loss\n",
        "early_stop = False    # Boolean to activate early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nMxfAEKHPpu",
        "outputId": "ec562237-6e9b-4b2d-a18e-f545426361cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0:\n",
            "[########################################] - 100.00 % | Loss (Training) = 0.489978 | Loss (Validation) = 0.459134 | Time left : 0.00\n",
            "\n",
            "EPOCH 1:\n",
            "[########################################] - 100.00 % | Loss (Training) = 0.457067 | Loss (Validation) = 0.467349 | Time left : 0.00\n",
            "\n",
            "EPOCH 2:\n",
            "[########################################] - 100.00 % | Loss (Training) = 0.449007 | Loss (Validation) = 0.444444 | Time left : 0.00\n",
            "\n",
            "EPOCH 3:\n",
            "[########################################] - 100.00 % | Loss (Training) = 0.442717 | Loss (Validation) = 0.443138 | Time left : 0.00\n",
            "\n",
            "EPOCH 4:\n",
            "[########################################] - 100.00 % | Loss (Training) = 0.438551 | Loss (Validation) = 0.446617 | Time left : 0.00\n",
            "\n",
            "EPOCH 5:\n",
            "[########################################] - 100.00 % | Loss (Training) = 0.433558 | Loss (Validation) = 0.413772 | Time left : 8.46Early stopping!\n",
            "---------------------------------------------------------------------\n",
            "               Finished training and model saved                     \n",
            "---------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Store training info\n",
        "losses_train = []\n",
        "accuracies_train = []\n",
        "losses_val = []\n",
        "accuracies_val = []\n",
        "\n",
        "estimated_time = 0 # To estimate time of training\n",
        "train_size = len(X_train)\n",
        "val_size = len(X_val)\n",
        "\n",
        "for epoch in range(NB_EPOCHS):\n",
        "\n",
        "  print('EPOCH {}:'.format(epoch))\n",
        "  start = time.time()\n",
        "  index = batch_size\n",
        "\n",
        "  #---------------------------------------------------------------------------\n",
        "  #                                   Training\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Training set\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Data to GPU\n",
        "    tweet = batch[0].to(device)\n",
        "    sentiment = batch[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad() # Resets gradients\n",
        "\n",
        "    # Combute predictions\n",
        "    predictions = model(tweet).squeeze(1)\n",
        "\n",
        "    # Compute loss and accuracy\n",
        "    loss = criterion(predictions, sentiment)\n",
        "    acc = binary_accuracy(predictions, sentiment)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Optimize parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store loss anc accuracy for training \n",
        "    train_loss.append(loss.detach().item())\n",
        "    train_acc.append(acc.detach().item())\n",
        "\n",
        "    # Remove data from GPU\n",
        "    tweet.to('cpu')\n",
        "    sentiment.to('cpu')\n",
        "\n",
        "    # Update progress bar\n",
        "    time_left = estimated_time -(time.time() - start)\n",
        "    progressBar(loss, 0, time_left, (index/train_size)*100)\n",
        "    index = index + batch_size\n",
        "  \n",
        "  # Compute mean loss and accuracy for training epoch\n",
        "  mean_loss = sum(train_loss)/len(train_loss)\n",
        "  losses_train.append(mean_loss)\n",
        "  mean_acc = sum(train_acc)/len(train_acc)\n",
        "  accuracies_train.append(mean_acc)\n",
        "\n",
        "  # Progress bar\n",
        "  estimated_time = time.time() - start\n",
        "  progressBar(mean_loss, 0, 0, 100)\n",
        "\n",
        "  #---------------------------------------------------------------------------\n",
        "  #                                 Validation\n",
        "  #---------------------------------------------------------------------------\n",
        "  index_validation = batch_size\n",
        "\n",
        "  # Validation set\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_dataloader):\n",
        "\n",
        "      # Data to GPU\n",
        "      tweet = batch[0].to(device)\n",
        "      sentiment = batch[1].to(device)\n",
        "\n",
        "      # Combute predictions\n",
        "      predictions = model(tweet).squeeze(1)\n",
        "\n",
        "      # Compute loss and accuracy\n",
        "      loss_val = criterion(predictions, sentiment)\n",
        "      acc_val = binary_accuracy(predictions, sentiment)\n",
        "\n",
        "      # Store loss and acccuracy for validation\n",
        "      val_loss.append(loss_val.detach().item())\n",
        "      val_acc.append(acc_val.detach().item())\n",
        "\n",
        "      # Remove data from GPU\n",
        "      tweet.to('cpu')\n",
        "      sentiment.to('cpu')\n",
        "\n",
        "      # Update progress bar\n",
        "      progressBar(mean_loss, loss_val, time_left, (index_validation/val_size)*100)\n",
        "      index_validation = index_validation + batch_size\n",
        "\n",
        "    # Compute mean loss and accuracy for validation epoch\n",
        "    mean_loss_val = sum(val_loss)/len(val_loss)\n",
        "    losses_val.append(mean_loss_val)\n",
        "    mean_acc_val = sum(val_acc)/len(val_acc)\n",
        "    accuracies_val.append(mean_acc_val)\n",
        "\n",
        "    # EARLY STOPPING\n",
        "    if mean_loss_val < min(losses_val):\n",
        "      print(\"Validation loss decreased\")\n",
        "      epochs_no_improve = 0\n",
        "    else:\n",
        "      epochs_no_improve += 1\n",
        "      if epochs_no_improve == patience:\n",
        "        print(\"Early stopping!\")\n",
        "        early_stop = True\n",
        "        # Save model and break\n",
        "        saveModel(model, optimizer)\n",
        "        break\n",
        "\n",
        "\n",
        "    # Display useful information\n",
        "    estimated_time = time.time() - start\n",
        "    progressBar(mean_loss, mean_loss_val, 0, 100)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Automatic save of the model\n",
        "    if (epoch+1) in checkpoints:\n",
        "      saveModel(model, optimizer)\n",
        "      \n",
        "# Information over terminal\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"               Finished training and model saved                     \")\n",
        "print(\"---------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "181-MlvGIvZN",
        "outputId": "361b1cfe-f468-48bd-ca73-bd2556543f17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f87b5699050>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(losses_train, label=\"Training loss\")\n",
        "plt.plot(losses_val, label=\"Validation loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rUSDmG9cHdHI",
        "outputId": "4983fe49-7b30-4287-c5d9-24e2504d72dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(accuracies_train, label=\"train\")\n",
        "plt.plot(accuracies_val, label=\"validation\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjuhVjdQoSRy"
      },
      "source": [
        "## Load Model\n",
        "If you want to reload the weights of a previous model for testing of starting training again : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "i_maD8sxoTTO",
        "outputId": "973fe23a-b488-4ff3-afab-c5b184c9c488"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3a1d992f1d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, \"LSTM\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model_str = \"Wed Nov 02\"\n",
        "model_path = root + \"/Models/\" + model_str + \".pth\"\n",
        "#model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, \"LSTM\")\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYqmxOeQYJrK",
        "outputId": "a75d11e5-222a-42a1-cfa0-133e875d2900"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(393023, 300, padding_idx=289283)\n",
              "  (model): LSTM(300, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qArMwrpwXTsl"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTFdt4uf5Wiz"
      },
      "source": [
        "### Evaluate the model with the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qClHWSRXXgIf"
      },
      "outputs": [],
      "source": [
        "def positive_negative_count(preds, y):\n",
        "  \"\"\"\n",
        "  Returns the accuracy per batch\n",
        "  \"\"\"\n",
        "  pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "\n",
        "  prob_preds = torch.nn.Softmax(dim=-1)(preds)\n",
        "  final_preds = torch.argmax(prob_preds, dim=1)\n",
        "\n",
        "  numpy_preds = final_preds.cpu().numpy()\n",
        "  numpy_y = y.cpu().numpy()\n",
        "\n",
        "  for i in range(len(y)):\n",
        "    if numpy_preds[i] == numpy_y[i]:\n",
        "      if numpy_y[i] == 0:\n",
        "        neg_correct += 1\n",
        "      else:\n",
        "        pos_correct += 1\n",
        "\n",
        "    if numpy_y[i] == 0:\n",
        "      neg_cnt += 1\n",
        "    else:\n",
        "      pos_cnt += 1\n",
        "\n",
        "  return neg_correct, neg_cnt , pos_correct, pos_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn3xHSv5XVvr",
        "outputId": "2bb7864e-aa0c-402f-d671-0adad9026f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "                   classified positive   classified negative \n",
            "------------------------------------------------------------\n",
            " Actual positive      2121                   294\n",
            " Actual negative      398                   2187\n",
            "------------------------------------------------------------\n",
            "Acuracy:  0.8616\n",
            "Precision:  0.8782608695652174\n",
            "Recall:  0.8460348162475823\n"
          ]
        }
      ],
      "source": [
        "neg_correct = 0\n",
        "neg_cnt = 0\n",
        "pos_correct = 0\n",
        "pos_cnt = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    # Data to GPU\n",
        "    tweet = batch[0].to(device)\n",
        "    sentiment = batch[1].to(device)\n",
        "\n",
        "    predictions = model(tweet).squeeze(1)\n",
        "\n",
        "    neg_correct_tmp, neg_cnt_tmp, pos_correct_tmp, pos_cnt_tmp = positive_negative_count(predictions, sentiment)\n",
        "\n",
        "    neg_correct += neg_correct_tmp\n",
        "    neg_cnt += neg_cnt_tmp\n",
        "    pos_correct += pos_correct_tmp\n",
        "    pos_cnt += pos_cnt_tmp\n",
        "  \n",
        "\n",
        "    # Remove data from GPU\n",
        "    tweet.to('cpu')\n",
        "    sentiment.to('cpu')\n",
        "\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(\"                   classified positive   classified negative \")\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(\" Actual positive     \", pos_correct, \"                 \", neg_cnt-neg_correct)\n",
        "print(\" Actual negative     \", pos_cnt-pos_correct, \"                 \", neg_correct)\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "print(\"Acuracy: \", (pos_correct+neg_correct)/(pos_cnt+neg_cnt))\n",
        "print(\"Recall: \", pos_correct/(pos_correct + neg_cnt-neg_correct))\n",
        "print(\"Precision: \", pos_correct/(pos_correct + pos_cnt-pos_correct))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4kIUQ6d5aex"
      },
      "source": [
        "### Predict on a given sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0jOCMDMmhq9"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "  splitted = sentence.split(\" \")\n",
        "  Sentence_tokens = padding_and_encode(splitted, seq_length)\n",
        "  seq = [Sentence_tokens]\n",
        "  Sentence_tokens = torch.tensor(seq).to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    prediction = model(Sentence_tokens).squeeze(1)\n",
        "\n",
        "  prob_pred = torch.nn.Softmax(dim=-1)(prediction)\n",
        "  final_pred = torch.argmax(prob_pred, dim=1)\n",
        "  f = final_pred.cpu().numpy()\n",
        "\n",
        "  if(f[0] == 1):\n",
        "    print(\"Positive\")\n",
        "  else:\n",
        "    print(\"Negative\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f4-FIf9nDmC",
        "outputId": "758e2e43-2f16-4e5d-c652-70112a9a4556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n",
            "Positive\n",
            "Positive\n",
            "Negative\n",
            "Negative\n"
          ]
        }
      ],
      "source": [
        "predict(\"I was very disappointed with this series. It had lots of cool graphics and that's about it. The level of detail it went into was minimal, and I always got the feeling the audience was being patronized -- there was a lot of what seemed to me as \")\n",
        "predict(\"Great just great! The West Coast got Dirty Harry Callahan, the East Coast got Sharky. Burt Reynolds plays Sharky in \")\n",
        "predict(\"I loved that movie\")\n",
        "predict(\"I hate you\")\n",
        "predict(\"you are ugly\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ef1b119d281e6c8960c7e46c636ce2a9ec354a3da52520939e5f0bfd0bbd7c37"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}