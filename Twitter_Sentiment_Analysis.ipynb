{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yW-OMw3wSYB"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwt90Bj1PB_"
      },
      "source": [
        "Tuto pour d√©marrer :\n",
        "https://bhadreshpsavani.medium.com/tutorial-on-sentimental-analysis-using-pytorch-b1431306a2d7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smH29zmcyd43",
        "outputId": "84db0cc5-5038-4f45-e7f2-cdee5e32e131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJlIuoU3Wun9"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_25-QAQzKQg"
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/MyDrive/Sentiment Analyis'\n",
        "#root = '/content/drive/MyDrive'\n",
        "#root = \"/Users/lucienavez/Library/CloudStorage/GoogleDrive-lucienavez@gmail.com/Mon Drive/Sentiment Analyis\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUDD5XUlwPw2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrokRsDpzV99",
        "outputId": "89b0ccbb-dc5a-436f-e90c-10179086c766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bcolz-zipline in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (1.2.5)\n",
            "Requirement already satisfied: numpy<1.23,>=1.16 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from bcolz-zipline) (1.22.4)\n",
            "Requirement already satisfied: gensim in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from gensim) (6.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from gensim) (1.9.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (1.13.0)\n",
            "Requirement already satisfied: torchvision in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (0.15.0.dev20221101)\n",
            "Requirement already satisfied: torchaudio in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (0.14.0.dev20221101)\n",
            "Requirement already satisfied: typing-extensions in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from torch) (4.3.0)\n",
            "Requirement already satisfied: numpy in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from torchvision) (2.28.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from torchvision) (9.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->torchvision) (1.26.12)\n",
            "Requirement already satisfied: contractions in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/lucienavez/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/lucienavez/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/lucienavez/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     /Users/lucienavez/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "# Missing libraries\n",
        "! pip install bcolz-zipline\n",
        "! pip install gensim\n",
        "! pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
        "! pip install contractions\n",
        "! pip install bs4\n",
        "\n",
        "# Basics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "import bcolz\n",
        "import matplotlib.pyplot as plt\n",
        "import contractions\n",
        "import string\n",
        "from collections import Counter, OrderedDict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('wordnet') # for lemmatization\n",
        "nltk.download('stopwords') # for stopwords\n",
        "nltk.download('omw-1.4') # Don't know why but code doesn't work (lemmization part)\n",
        "nltk.download('words')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mdMjOQ5TOku",
        "outputId": "3bc26abf-03ce-4986-ed1f-74ed8146b309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "# Hardware\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnPsBKPUpZuy"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th-M7v_OTY7i"
      },
      "source": [
        "# Dataset\n",
        "The dataset is downloaded from the following link : \n",
        "https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis\n",
        "\n",
        "The dataset follows the following format :\n",
        "\n",
        "\n",
        "6 fields:\n",
        "\n",
        "- target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "- ids: The id of the tweet ( 2087)\n",
        "- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "- user: the user that tweeted (robotickilldozr)\n",
        "- text: the text of the tweet (Lyx is cool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcxi4X5CWuoA"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "See following links : \n",
        "https://medium.com/analytics-vidhya/pre-processing-tweets-for-sentiment-analysis-a74deda9993e\n",
        "https://towardsdatascience.com/how-to-preprocess-social-media-data-and-text-messages-b011efacf74\n",
        "\n",
        "Data must be pre-processed in order to be used for training.\n",
        "For the time being, the only kind of pre-processing that has been applied is removing the emojis.\n",
        "Howerver, some parts of the tweets may be noisy for the model and would require extra attention : \n",
        "- usernames and @ (handles) : @LATimesautos, @kirstiealley, ...\n",
        "- hashtags : #lebron, #kindle2\n",
        "- abbreviations : LMAO, pls, DM\n",
        "- people names + brands : Obama, Kindle, Nike, Susan Boyle\n",
        "- html characters : &amp, &lt, &gt\n",
        "- urls and links : http://bit.ly/1e1xQ6, http://t.co/3wEwWZi, http://t.co/3wEwWZi, ...\n",
        "- weird punctuation : --, -, \n",
        "- unexpected lowercases, uppercases\n",
        "- emojis with punctuation : :), :-), =D\n",
        "- ...\n",
        "\n",
        "Analyse data to find out what kind of pre-processing is needed.\n",
        "\n",
        "1. Put everything to lowercase (in the dataset, it does not seem that lowercase and uppercase play a significant enough role in the sentiment of the tweet)\n",
        "\n",
        "2. URLs : remove them, they won't be useful for the model\n",
        "3. HTML characters : remove them, they won't be useful for the model\n",
        "4. Emojis : I don't know if we should remove them or not, they may be useful for the model because they can be used to express emotions. For instance, we could imagine to replace them by their meaning : :) could be \"smile\", :D could be \"laugh\", :/ could be \"sad\", etc. However, it is not clear that the model will be able to understand the meaning of the emojis. For the time being, I have removed them.\n",
        "5. Punctuation : remove it, it won't be useful for the model\n",
        "6. Numbers : remove them, they won't be useful for the model\n",
        "7. Abbreviations : replace them with their full form, they may be useful for the model. We can maybe use a dictionary to replace them (custom) --> Maybe this is overkill compared to the actual gain of doing that. Or we can use a library like TextBlob to replace them (not custom).\n",
        "8. Hashtags : remove them, they won't be useful for the model\n",
        "9. @ : remove them, they won't be useful for the model\n",
        "10. People names : remove them, they won't be useful for the model\n",
        "11. Removing stopwords : remove them, they won't be useful for the model\n",
        "\n",
        "Additionnaly we can use techniques as :\n",
        "- Lemmatization : converting a word to its base form. For instance : \"running\" -> \"run\", \"better\" -> \"good\", \"am\" -> \"be\", etc.\n",
        "- Stemming : removing the suffixes of a word. For instance : \"running\" -> \"run\", \"better\" -> \"better\", \"tarts\" -> \"tart\", etc. Only removes suffixes, not prefixes. Stemming is faster than lemmatization but less accurate.\n",
        "\n",
        "Example where lemmatization and stemming are applied :\n",
        "- Lemmatization : \"Caring\" -> \"Care\"\n",
        "- Stemming : \"Caring\" -> \"Car\"\n",
        "\n",
        "Obviously, \"Care\" is not the base form of \"Caring\" and \"Car\" is not the base form of \"Caring\" either.\n",
        "There are many ways to implement them but the most common one is to use the NLTK package.\n",
        "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqU0E4AypktY"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zkfaRdJWuoB"
      },
      "source": [
        "#### Lemmization, Stemming and Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ1lRVQ98ioU"
      },
      "outputs": [],
      "source": [
        "def lemmatize(df):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  df.text = df.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "  return df\n",
        "\n",
        "def stem(df):\n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  df.text = df.text.apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "  return df\n",
        "\n",
        "def removeStopwords(df):\n",
        "  english_stopwords = stopwords.words('english')\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item not in english_stopwords])\n",
        "\n",
        "  return df\n",
        "\n",
        "def expandContractions(df):\n",
        "  df.text = df.text.apply(lambda x: [contractions.fix(word) for word in x])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBhCnWUvrNBI"
      },
      "outputs": [],
      "source": [
        "def preProcess(df, lemmatization = True, stemming = False, remove_stopwords = True):\n",
        "  \n",
        "  # Lowercase the text\n",
        "  df.text = df.text.str.lower()\n",
        "  # Remove urls \n",
        "  df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "  df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "  # Remove html reference characters\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "  # Remove hashtags (must be done before removing punctuation)\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'#', '', x))\n",
        "  # Remove mentions (must be done before removing punctuation)\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'@\\w+', '', x))\n",
        "  # Remove numbers\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "  # Remove special characters\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'\\[[^]]*\\]', '', x))\n",
        "  df.text = df.text.apply(lambda x: re.sub(r'[^a-zA-z0-9\\s]', '', x))\n",
        "\n",
        "  print(\"Special characters removed\")\n",
        "\n",
        "  # Tokenize\n",
        "  tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
        "  df['text'] = df.apply(lambda row: tokenizer(row['text']), axis=1)\n",
        "\n",
        "  print(\"Text tokenized\")\n",
        "\n",
        "  # Expand contractions\n",
        "  df = expandContractions(df)\n",
        "\n",
        "  if remove_stopwords:\n",
        "    # Remove stopwords\n",
        "    df = removeStopwords(df)\n",
        "\n",
        "  print(\"Stop words removed\")\n",
        "    \n",
        "  if lemmatization:\n",
        "    # Lemmatization\n",
        "    df = lemmatize(df)\n",
        "\n",
        "  if stemming:\n",
        "    # Stemming\n",
        "    df = stem(df)\n",
        "\n",
        "  print(\"Lemmatized and stemmatized\")\n",
        "\n",
        "    \n",
        "  # Remove punctuation tokens in each row of the text column\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item not in string.punctuation])\n",
        "  # Remove any weird characters tokens\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item.isalpha()])\n",
        "  # Remove any tokens with length less than 2\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if len(item) > 2])\n",
        "  # Remove any repeated tokens\n",
        "  df.text = df.text.apply(lambda x: list(OrderedDict.fromkeys(x)))\n",
        "\n",
        "  print(\"Punctuation removed\")\n",
        "\n",
        "  # Remove any non-english words\n",
        "  english_words = set(nltk.corpus.words.words())\n",
        "  df.text = df.text.apply(lambda x: [item for item in x if item in english_words])\n",
        "\n",
        "  print(\"Non-english words removed\")\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTHoUz8Cpnun"
      },
      "outputs": [],
      "source": [
        "def loadData(data_path):\n",
        "  # Load CSV file\n",
        "  df = pd.read_csv(data_path, encoding='latin-1')\n",
        "\n",
        "  # Add sentiment and text column header\n",
        "  column_names = ['text', 'sentiment']\n",
        "  df = pd.DataFrame(zip(df.iloc[:, 1], df.iloc[:, 2]), columns=column_names)\n",
        "\n",
        "  # Pre-process\n",
        "  df = preProcess(df)\n",
        "\n",
        "  # Vocabulary size \n",
        "  # Build the vocabulary\n",
        "  voc = set()\n",
        "  for text in df.text:\n",
        "    for word in text:\n",
        "      voc.add(word)\n",
        "\n",
        "  return df, len(voc), voc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZmZfLA4TZ7f"
      },
      "outputs": [],
      "source": [
        "# Read the data at the path\n",
        "print(\"Loading train data...\")\n",
        "df_train, vocab_size_train, voc_train = loadData(root + '/data/train.csv')\n",
        "print(\"\\nLoading test data...\")\n",
        "df_test, vocab_size_test, voc_test = loadData(root + '/data/test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train size: \", df_train.text.size)\n",
        "print(df_train.sentiment.value_counts())\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "id": "fi4Myd54njGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test size: \", df_test.text.size)\n",
        "print(df_test.sentiment.value_counts())\n",
        "print(df_test.head())"
      ],
      "metadata": {
        "id": "Ed6RiE5GnlFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGO4Lgqwm5OE",
        "outputId": "08ea2618-9388-4f8a-9762-a93c9e7dd4e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape:  (1600000, 2)\n",
            "Test data shape:  (498, 2)\n",
            "Vocabulary size:  30978\n",
            "Vocabulary size:  30978\n",
            "Vocabulary train:  ['chucky', 'trough', 'hawk', 'gestation', 'vie', 'guiltless', 'morong', 'pascual', 'dispersal', 'currently']\n",
            "Vocabulary test:  ['chucky', 'trough', 'hawk', 'gestation', 'vie', 'guiltless', 'morong', 'pascual', 'dispersal', 'currently']\n",
            "Train data sample: \n",
            "    sentiment                                               text\n",
            "0          0                    [bummer, got, carr, third, day]\n",
            "1          0  [upset, update, might, cry, result, school, to...\n",
            "2          0              [many, time, ball, save, rest, bound]\n",
            "3          0             [whole, body, feel, itchy, like, fire]\n",
            "4          0                                         [mad, see]\n",
            "Test data sample: \n",
            "    sentiment                                               text\n",
            "0          4  [loooooooovvvvvveee, kindle, cool, fantastic, ...\n",
            "1          4    [reading, kindle, love, lee, child, good, read]\n",
            "2          4          [first, assesment, kindle, fucking, rock]\n",
            "3          4  [love, kindle, mine, month, never, looked, bac...\n",
            "4          4             [fair, enough, kindle, think, perfect]\n"
          ]
        }
      ],
      "source": [
        "print('Train data shape: ', df_train.shape)\n",
        "print('Test data shape: ', df_test.shape)\n",
        "print('Vocabulary size: ', vocab_size_train)\n",
        "print('Vocabulary size: ', vocab_size_test)\n",
        "print('Vocabulary train: ', list(voc_train)[:10])\n",
        "print('Vocabulary test: ', list(voc_test)[:10])\n",
        "print('Train data sample: \\n', df_train.head())\n",
        "print('Test data sample: \\n', df_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FsWOb4-sYF4"
      },
      "source": [
        "## Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATKXjICmsYF4"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary for the training data\n",
        "vocabulary = set()\n",
        "\n",
        "# Add special tokens to the vocabulary\n",
        "pad = \"<PAD>\" # Used to pad short sentences to the MAX_TOKENS length\n",
        "sos = \"<SOS>\" # Start-of-sentence\n",
        "eos = \"<EOS>\" # End-of-sentence\n",
        "ukn = \"<UKN>\" # Unknown word\n",
        "\n",
        "vocabulary.add(pad)\n",
        "vocabulary.add(sos)\n",
        "vocabulary.add(eos)\n",
        "vocabulary.add(ukn)\n",
        "\n",
        "\n",
        "MAX_TOKENS = 0 # Maximum number of tokens in a sentence\n",
        "\n",
        "# Iterate over the training data\n",
        "for row in range(len(df_train)):\n",
        "      # Each row[\"text\"] is a list of tokens\n",
        "      # Each token is a word that has to be added to the vocabulary\n",
        "      MAX_TOKENS = max(MAX_TOKENS, len(df_train.loc[row, \"text\"]))\n",
        "      # Save the row that has the maximum number of tokens\n",
        "      if MAX_TOKENS == len(df_train.loc[row, \"text\"]):\n",
        "        max_row = df_train.loc[row, \"text\"]\n",
        "        row_index = row\n",
        "      tokens = df_train.loc[row, \"text\"]\n",
        "      for word in tokens:\n",
        "        vocabulary.add(word)\n",
        "            \n",
        "vocab_size = len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvvErr8d2Qg7",
        "outputId": "ea59d0f4-7ed2-474a-e440-7c6a7ca9564d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n",
            "30982\n",
            "['bad', 'day', 'stayed', 'late', 'making', 'business', 'card', 'tired', 'woke', 'early', 'rainy', 'parade', 'traffic', 'wrong', 'turn', 'stopped', 'cop', 'tie', 'soccer', 'game']\n"
          ]
        }
      ],
      "source": [
        "print(MAX_TOKENS)\n",
        "print(vocab_size)\n",
        "print(max_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCi-HitJ1GeC"
      },
      "outputs": [],
      "source": [
        "# For each word in the vocabulary, assign a word id\n",
        "word2index = {}\n",
        "for index, word in enumerate(vocabulary):\n",
        "    word2index[word] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0yOhek36Lew"
      },
      "outputs": [],
      "source": [
        "# For each word id, assign a word\n",
        "index2word = {id: token for token, id in enumerate(word2index)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0zIy4zW9IXy",
        "outputId": "49e0eb47-3d80-414e-f92e-14aea58e89ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7486\n"
          ]
        }
      ],
      "source": [
        "print(word2index[\"<SOS>\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To have an idea, compute statistics on the number of tokens in each sentence\n",
        "mean = 0\n",
        "for row in range(len(df_train)):\n",
        "    mean += len(df_train.loc[row, \"text\"])\n",
        "mean = mean / len(df_train)\n",
        "print(\"Mean length of sentences : \", mean)"
      ],
      "metadata": {
        "id": "RAYZEshDnyi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdtW1GzNkTU_"
      },
      "source": [
        "## Build Embedding\n",
        "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "Embedding converts words to integers, and there is a vector corresponding to eachinteger.\n",
        "If there x words in a dictionary, each word will be assigned a value between 1 and x.\n",
        "\n",
        "Embedding types : \n",
        "- GloVe : Global Vectors for Word Representation (pre-trained)\n",
        "- Word2Vec : \n",
        "    - Pre-trained : https://github.com/AI-Trends/NLP-Tutorial/blob/master/Word2vec_pretrained_embeddings.ipynb\n",
        "This first version of word2vec embedding will load the vectors of an already trained model.\n",
        "    - Trained on the fly : https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis\n",
        "Unlike the previous version, we will build a vocabulary and train it with the train dataset from our data.\n",
        "- FastText : Facebook AI Research\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4NFBDMDuOjC"
      },
      "source": [
        "### Embedding matrix\n",
        "When we will have a dataset, its vocabulary, and a dictionary of embedder words and their corresponding vectors.\n",
        "There will still be no correlation between our vocabulary, and the embedder vocabulary.\n",
        "To connect them, we need to create an embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3C6w3I6uOT8"
      },
      "outputs": [],
      "source": [
        "def build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding, glove, model):\n",
        "    print(\"Building embedding matrix...\")\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    # This matrix needs to be populated : \n",
        "    #   For each word in the training vocabulary, the corresponding vector is \n",
        "    #   retrieved from the GloVe dictionary\n",
        "\n",
        "    ukn_cnt = 0     # unknown words counter\n",
        "    ukn_set = set() # set of unknown words\n",
        "    for index, word in enumerate(vocabulary):\n",
        "        if word in embedding:\n",
        "          if glove:\n",
        "            embedding_matrix[index] = embedding[word]\n",
        "          else:\n",
        "            embedding_vector = model[word]\n",
        "            embedding_vector = np.array(embedding_vector)\n",
        "            if embedding_vector is not None:\n",
        "              embedding_matrix[index] = embedding_vector\n",
        "        else:\n",
        "            ukn_cnt += 1\n",
        "            ukn_set.add(word)\n",
        "            embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "    \n",
        "    print(\"Unknown words: \", ukn_cnt)\n",
        "    print(\"Percentage of unknown words: \", ukn_cnt / vocab_size)\n",
        "    \n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQxylFDBm5OG",
        "outputId": "d67c2ead-f751-4c52-f349-e635b23295a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained FastText embeddings...\n",
            "Error: embedding dimension is not  300\n",
            "Building embedding matrix...\n",
            "Unknown words:  2269\n",
            "Percentage of unknown words:  0.07323607255825963\n"
          ]
        }
      ],
      "source": [
        "# Choose the embedding parameters\n",
        "EMBEDDING_TYPE = \"glove\" # \"glove\" (pre-trained), \"word2vec\" or \"fasttext\"\n",
        "PRE_TRAINED = True # True if using pre-trained embeddings, False otherwise\n",
        "RETRAIN_EMBEDDINGS = False # True if training the embeddings, False otherwise\n",
        "\n",
        "#-----------------------------------------\n",
        "#                  GloVe \n",
        "#-----------------------------------------\n",
        "if EMBEDDING_TYPE == \"glove\":\n",
        "    if PRE_TRAINED:\n",
        "        print(\"Loading pre-trained GloVe embeddings...\")\n",
        "        embedding_path = root + '/Glove/glove.42B.300d.txt'\n",
        "        embedding_dim = 300 # The dimension of the embedding\n",
        "        \n",
        "        embedding_dict = {}\n",
        "        with open(embedding_path, 'r') as f:\n",
        "            for line in f:\n",
        "                tokens = line.split()\n",
        "                word = tokens[0]\n",
        "                vector = np.array(tokens[1:], dtype=np.float32)\n",
        "                if vector.shape[0] == embedding_dim:\n",
        "                    embedding_dict[word] = vector\n",
        "                else:\n",
        "                    print(\"Error: embedding dimension is not \", embedding_dim)\n",
        "        \n",
        "        # Create the embedding matrix   \n",
        "        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, True, embedding_dict)\n",
        "                \n",
        "#-----------------------------------------\n",
        "#                  Word2Vec \n",
        "#-----------------------------------------             \n",
        "elif EMBEDDING_TYPE == \"word2vec\":\n",
        "    if PRE_TRAINED:\n",
        "        print(\"Loading pre-trained Word2Vec embeddings...\")\n",
        "        \n",
        "        embedding_path = root + '/Word2Vec/GoogleNews-vectors-negative300.bin'\n",
        "        embedding_dim = 300 # The dimension of the embedding\n",
        "        \n",
        "        # Load the vocabulary and the corresponding vectors\n",
        "        model_w2v = KeyedVectors.load_word2vec_format(root + '/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "        embedding_dict = model_w2v.vocab\n",
        "        \n",
        "        # Create the embedding matrix\n",
        "        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n",
        "    \n",
        "    else:\n",
        "        if RETRAIN_EMBEDDINGS:\n",
        "            print(\"Training Word2Vec embeddings...\")\n",
        "            #-----------------------------------------\n",
        "            #                 Parameters\n",
        "            #-----------------------------------------\n",
        "            \n",
        "            embedding_dim = 300\n",
        "            w2v_window = 5\n",
        "            w2v_min_count = 10\n",
        "            w2v_workers = 8\n",
        "            train_embeddings_epochs = 30\n",
        "            \n",
        "            # Train the word2vec model\n",
        "            model_w2v = gensim.models.word2vec.Word2Vec(size=embedding_dim, window=w2v_window, min_count=w2v_min_count, workers=w2v_workers)\n",
        "            model_w2v.build_vocab(df_train.text)\n",
        "            words = model_w2v.wv.vocab.keys()\n",
        "            model_w2v.train(df_train.text, total_examples=len(df_train), epochs=train_embeddings_epochs)\n",
        "            model_w2v.save(root + \"/Word2Vec/w2v.model\")\n",
        "            embedding_dict = model_w2v.wv\n",
        "            \n",
        "            # Create the embedding matrix\n",
        "            embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n",
        "            \n",
        "        else:\n",
        "            print(\"Loading Word2Vec embeddings...\")\n",
        "            # Can avoid the training by loading the saved model\n",
        "            model_w2v = gensim.models.word2vec.Word2Vec.load(root + '/Word2Vec/w2v.model')\n",
        "            embedding_dim = 300\n",
        "            embedding_dict = model_w2v.wv\n",
        "        \n",
        "            # Create the embedding matrix\n",
        "            embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n",
        "            \n",
        "elif EMBEDDING_TYPE == \"fasttext\":\n",
        "    if PRE_TRAINED:\n",
        "        print(\"Loading pre-trained FastText embeddings...\")\n",
        "        \n",
        "        embedding_path = root + '/FastText/wiki-news-300d-1M.vec'\n",
        "        embedding_dim = 300\n",
        "        \n",
        "        # Load the vocabulary and the corresponding vectors\n",
        "        embedding_dict = {}\n",
        "        with open(embedding_path, 'r') as f:\n",
        "            for line in f:\n",
        "                tokens = line.split()\n",
        "                word = tokens[0]\n",
        "                vector = np.array(tokens[1:], dtype=np.float32)\n",
        "                if vector.shape[0] == embedding_dim:\n",
        "                    embedding_dict[word] = vector\n",
        "                else:\n",
        "                    print(\"Error: embedding dimension is not \", embedding_dim)\n",
        "                    \n",
        "        # Create the embedding matrix\n",
        "        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-BZqZTXm5OH",
        "outputId": "ba36ab7a-1095-4179-9ca1-b19f609b60b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary Size :  999994\n"
          ]
        }
      ],
      "source": [
        "# Check dictionary size : \n",
        "print(\"Dictionary Size : \", len(embedding_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Rp2m2F6-lX"
      },
      "source": [
        "## Padding sequences\n",
        "Some of the tweets are longer than others. \n",
        "In order to train the model on a fixed-length input, we must define an input length that will be the same for all tokens (MAX_TOKENS), and pad all tweets that are smaller than MAX_TOKENS with \\<PAD\\> tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFS-iTNa7phK",
        "outputId": "8b9e8f66-e624-4279-9995-86947c4be1b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum sequence length including EOS and SOS :  12\n"
          ]
        }
      ],
      "source": [
        "# Decide on the maximum length of the sequences\n",
        "seq_length = MAX_TOKENS + 2 - 10 # +2 stands for EOS and SOS (added next step)\n",
        "print(\"Maximum sequence length including EOS and SOS : \", seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEJFmLoRq6Xb"
      },
      "outputs": [],
      "source": [
        "#Returns the index if the word is in the vocabulary, otherwise, it returns the index of unkwon words\n",
        "def get_word(w):\n",
        "  if w in word2index:\n",
        "    return word2index[w]\n",
        "  else:\n",
        "    return word2index[\"<UKN>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmMbMzL57UNv"
      },
      "outputs": [],
      "source": [
        "def padding_and_encode(tweet, max_length):\n",
        "  sos_id = [word2index[\"<SOS>\"]]\n",
        "  eos_id = [word2index[\"<EOS>\"]]\n",
        "  pad_id = [word2index[\"<PAD>\"]]\n",
        "  \n",
        "  # Truncate the tweet if it is too long (more than seq_length)\n",
        "  if len(tweet) > max_length:\n",
        "    tweet = tweet[:max_length]\n",
        "    n_pads = 0\n",
        "  else:\n",
        "    n_pads = max_length - len(tweet)\n",
        "    \n",
        "  encoded = [get_word(w) for w in tweet] # encode without embedding here!!!\n",
        "  return sos_id + encoded + eos_id + pad_id*n_pads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxmG78TY9jqx"
      },
      "outputs": [],
      "source": [
        "# Transfrom dataframes into lists\n",
        "X_train = df_train[\"text\"].tolist()\n",
        "X_test = df_test[\"text\"].tolist()\n",
        "y_train = df_train[\"sentiment\"].tolist()\n",
        "y_test = df_test[\"sentiment\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVh-wWVhf_Ef",
        "outputId": "89b36b6c-2ca7-49cc-c3ea-c84b63371650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bummer', 'got', 'carr', 'third', 'day'] 0\n",
            "['upset', 'update', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'] 0\n",
            "['many', 'time', 'ball', 'save', 'rest', 'bound'] 0\n",
            "['whole', 'body', 'feel', 'itchy', 'like', 'fire'] 0\n",
            "['mad', 'see'] 0\n",
            "['whole', 'crew'] 0\n",
            "['need', 'hug'] 0\n",
            "['hey', 'long', 'time', 'see', 'yes', 'rain', 'bit', 'fine', 'thanks'] 0\n",
            "['nope'] 0\n",
            "[] 0\n"
          ]
        }
      ],
      "source": [
        "# Print the 10 first elements of the training set\n",
        "for i in range(10):\n",
        "  print(X_train[i], y_train[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aXbt9gr-D-1"
      },
      "outputs": [],
      "source": [
        "# Encode \n",
        "X_train = [padding_and_encode(tweet, seq_length-2) for tweet in X_train] # -2 because of SOS and EOS\n",
        "X_test = [padding_and_encode(tweet, seq_length-2) for tweet in X_test] # -2 because of SOS and EOS\n",
        "# Make sure that the values in y_train are either 0 (0) or 1 (instead of 4)\n",
        "y_train = [0 if y == 0 else 1 for y in y_train]\n",
        "y_test = [0 if y == 0 else 1 for y in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMaiJunpkjTA",
        "outputId": "173881fb-ec54-4522-d087-86f0a3e0dfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7486, 14681, 9923, 1141, 16388, 8497, 27142, 3996, 3996, 3996, 3996, 3996] 0\n",
            "[7486, 5671, 21732, 10936, 4855, 13906, 23584, 27194, 15803, 4837, 27142, 3996] 0\n",
            "[7486, 7246, 2473, 22594, 16662, 2787, 13672, 27142, 3996, 3996, 3996, 3996] 0\n"
          ]
        }
      ],
      "source": [
        "for i, j in zip(X_train[:3], y_train[:3]):\n",
        "  print(i, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2gwgoMKuYMb"
      },
      "source": [
        "Since we only have a training set and test set, we split the training set into training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkFvR1cYudfO",
        "outputId": "5697bd82-57c8-41ec-e495-842f1504a27b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples :  204800\n",
            "Number of validation examples :  51200\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
        "                                                  random_state = 42, \n",
        "                                                  shuffle = True, \n",
        "                                                  test_size = 0.2)\n",
        "\n",
        "# Now truncate the dataset because there is too much data\n",
        "keep_ratio = 0.2\n",
        "X_train, y_train = X_train[:int(len(X_train)*keep_ratio)], y_train[:int(len(y_train)*keep_ratio)]\n",
        "X_val, y_val = X_val[:int(len(X_val)*keep_ratio)], y_val[:int(len(y_val)*keep_ratio)]\n",
        "\n",
        "print(\"Number of training examples : \", len(X_train))\n",
        "print(\"Number of validation examples : \", len(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj-wn4z-xQLt",
        "outputId": "1612ce59-c28d-48c6-82b5-48bfa34d9e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7486, 26023, 27142, 3996, 3996, 3996, 3996, 3996, 3996, 3996, 3996, 3996] 0\n",
            "[7486, 10549, 23428, 28192, 27142, 3996, 3996, 3996, 3996, 3996, 3996, 3996] 1\n",
            "[7486, 27142, 3996, 3996, 3996, 3996, 3996, 3996, 3996, 3996, 3996, 3996] 0\n"
          ]
        }
      ],
      "source": [
        "for i, j in zip(X_train[:3], y_train[:3]):\n",
        "  print(i, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvoZBxauA2ln"
      },
      "source": [
        "# DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za-9ayS9A6Zw"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "X_val = np.array(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=len(X_test), drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document embedding\n",
        "The document embedding will work in a different way from the word embedding. Instead of using an embedding layer into our RNN, we will directly transform the document into its embedded vector and provide that vector as an input to the RNN. This will thus require to use a RNN a bit modified from the previous one."
      ],
      "metadata": {
        "id": "AEKPvlbvoYZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#document_model = \"doc2vec\"\n",
        "document_model = \"word2vec-tfidf\"\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "new_df_train = np.zeros((len(df_train), embedding_dim))\n",
        "new_df_test = np.zeros((len(df_test), embedding_dim))\n",
        "i = 0\n",
        "\n",
        "if document_model == \"doc2vec\":\n",
        "\n",
        "  print(\"Loading doc2vec...\")\n",
        "\n",
        "  d2v_model = Doc2Vec(vector_size=embedding_dim, min_count=2, epochs=30)\n",
        "\n",
        "  sentences = [TaggedDocument(sentence, [i]) for i,sentence in enumerate(df_train.text)]\n",
        "\n",
        "  d2v_model.build_vocab(sentences)\n",
        "  d2v_model.wv.vocab\n",
        "  d2v_model.train(sentences, total_examples=len(sentences), epochs=30)\n",
        "\n",
        "  for sentence in df_train.text:\n",
        "    new_df_train[i] = d2v_model.infer_vector(sentence)\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for sentence in df_test.text:\n",
        "    new_df_test[i] = d2v_model.infer_vector(sentence)\n",
        "    i += 1\n",
        "\n",
        "elif document_model == \"word2vec-tfidf\":\n",
        "\n",
        "  print(\"Loading word2vec averaged with tf-idf...\")\n",
        "\n",
        "  # Loading word2vec\n",
        "  embedding_path = root + '/Word2Vec/GoogleNews-vectors-negative300.bin'\n",
        "        \n",
        "  # Load the vocabulary and the corresponding vectors\n",
        "  model_w2v = KeyedVectors.load_word2vec_format(root + '/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "  dictionary = Dictionary()\n",
        "  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in df_train.text]\n",
        "\n",
        "  tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n",
        "\n",
        "  unknown = 0\n",
        "  num_words = 0\n",
        "  #i = 0\n",
        "\n",
        "  for doc in tfidf[bow_corpus]:\n",
        "    if i % 5000 == 0:\n",
        "      print(i, \"/\", len(df_train))\n",
        "\n",
        "    total_freq = 0 #used to normalize the weigths\n",
        "\n",
        "    for id, freq in doc:\n",
        "      total_freq += freq\n",
        "\n",
        "    #Bulding the vector for each document\n",
        "    for id, freq in doc: \n",
        "      num_words += 1\n",
        "      word = dictionary[id]\n",
        "      #print(word)\n",
        "      if word in model_w2v.vocab:\n",
        "        embedding_vector = model_w2v[word]\n",
        "        embedding_vector = np.array(embedding_vector)\n",
        "        new_df_train[i] += (freq/total_freq)*embedding_vector\n",
        "      else:\n",
        "        unknown += 1\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  print(unknown/num_words)\n",
        "\n",
        "  # Now we do the same but for df_test\n",
        "  unknown = 0\n",
        "  num_words = 0\n",
        "  i = 0\n",
        "  dictionary = Dictionary()\n",
        "  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in df_test.text]\n",
        "\n",
        "  tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n",
        "\n",
        "  for doc in tfidf[bow_corpus]:\n",
        "    if i % 500 == 0:\n",
        "      print(i, \"/\", len(df_test))\n",
        "\n",
        "    total_freq = 0 #used to normalize the weigths\n",
        "\n",
        "    for id, freq in doc:\n",
        "      total_freq += freq\n",
        "\n",
        "    #Bulding the vector for each document\n",
        "    for id, freq in doc: \n",
        "      num_words += 1\n",
        "      word = dictionary[id]\n",
        "      #print(word)\n",
        "      if word in model_w2v.vocab:\n",
        "        embedding_vector = model_w2v[word]\n",
        "        embedding_vector = np.array(embedding_vector)\n",
        "        new_df_test[i] += (freq/total_freq)*embedding_vector\n",
        "      else:\n",
        "        unknown += 1\n",
        "\n",
        "    i += 1\n",
        "  print(unknown/num_words)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "VSep-BMwobVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_df_test.shape)\n",
        "print(new_df_train.shape)"
      ],
      "metadata": {
        "id": "61fxHnG2ocIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train[\"sentiment\"].tolist()\n",
        "y_test = df_test[\"sentiment\"].tolist()\n",
        "y_train = [0 if y == 'negative' else 1 for y in y_train]\n",
        "y_test = [0 if y == 'negative' else 1 for y in y_test]"
      ],
      "metadata": {
        "id": "qAb0HVtroeGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(new_df_train, y_train, \n",
        "                                                  random_state = 42, \n",
        "                                                  shuffle = True, \n",
        "                                                  test_size = 0.2)"
      ],
      "metadata": {
        "id": "J4BY9Rf9oh03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0].shape)"
      ],
      "metadata": {
        "id": "AiWuSzhhomV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "\n",
        "#print(X_train.shape)\n",
        "X_train = [[doc] for doc in X_train]\n",
        "X_train = np.array(X_train)\n",
        "print(X_train.shape)\n",
        "X_test = [[doc] for doc in new_df_test]\n",
        "X_test = np.array(X_test)\n",
        "X_val = [[doc] for doc in X_val]\n",
        "X_val = np.array(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
        "#print(torch.from_numpy(X_train))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "metadata": {
        "id": "9aFq7Av9ooM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbTtyocUWuoC"
      },
      "source": [
        "# Model architecture\n",
        "To train the model, we will use the following architectures :\n",
        "- RNN : \n",
        "  - Embedding layer (pre-trained GloVe, Word2Vec, FastText)\n",
        "    https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
        "    The weights of the embedding_matrix will be loaded into that layer.\n",
        "  - LSTM layer or GRU layer\n",
        "  - Linear layer\n",
        "\n",
        "Or we can use the following architecture :\n",
        "- Transformer architecture (self attention)\n",
        "\n",
        "Or the following architecture : \n",
        "- RNN + Attention\n",
        "  - Embedding layer (pre-trained GloVe, Word2Vec, FastText)\n",
        "  - LSTM layer or GRU layer\n",
        "  - Attention layer\n",
        "  - Linear layer\n",
        "\n",
        "Or the following architecture:\n",
        "- RNN + Attention for document embedding\n",
        "  - LSTM layer or GRU layer\n",
        "  - Attention layer\n",
        "  - Linear layer\n",
        "\n",
        "Finally, we can use the following architecture:\n",
        "- RNN for document embedding : \n",
        "  - LSTM layer or GRU layer\n",
        "  - Linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Reyt1GwSDRRr"
      },
      "outputs": [],
      "source": [
        "def CreateEmbeddingLayer(embedding_matrix, padding_id, non_trainable=False):\n",
        "  # Retrieve dimensions of embedding\n",
        "  num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "  # Initialize nn.Embedding with the pre-trained weights\n",
        "  embedding_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_id)\n",
        "  embedding_layer.load_state_dict({'weight': torch.from_numpy(embedding_matrix)})\n",
        "\n",
        "  if non_trainable:\n",
        "    embedding_layer.weight.requires_grad = False\n",
        "\n",
        "  return embedding_layer, num_embeddings, embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(seq_length, embedding_dim):\n",
        "  pos_enc = np.array([\n",
        "      [pos / np.power(10000, 2 * (j // 2) / embedding_dim) for j in range(embedding_dim)]\n",
        "      if pos != 0 else np.zeros(embedding_dim) for pos in range(seq_length)\n",
        "  ])\n",
        "  pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
        "  pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
        "  \n",
        "  return torch.from_numpy(pos_enc).type(torch.FloatTensor).to(device)"
      ],
      "metadata": {
        "id": "7Scb5vcZovKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcs5tMQKm5OM"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOrXOnN1AyJs"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, dropout, bidirectional, padding_id, num_layers, type):\n",
        "      super().__init__()\n",
        "\n",
        "      # Embedding layer\n",
        "      self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "\n",
        "      # LSTM or GRU\n",
        "      if type==\"LSTM\":\n",
        "        self.model = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            batch_first = True, \n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "      elif type==\"GRU\":\n",
        "        self.model = nn.GRU(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "\n",
        "      # Fully-connected layer\n",
        "      self.fc = nn.Linear(hidden_dim * 2, 2)\n",
        "      # The output of the fully connected layer is a vector of size 2 containing\n",
        "      # the probability of the tweet to belong to either positive or negative\n",
        "      # class (--> 2 classes)\n",
        "\n",
        "      # Dropout\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      # Embed the input\n",
        "      embs = self.dropout(self.embedding(input))\n",
        "\n",
        "      # pack sequence\n",
        "      # lengths need to be on CPU!\n",
        "      # This will cause the RNN to only process non-padded elements\n",
        "      #packed_embedded = nn.utils.rnn.pack_padded_sequence(embs, input_length.to('cpu'))\n",
        "\n",
        "      # Feed embeddings to LSTM\n",
        "      packed_output, (hidden, cell) = self.model(embs)\n",
        "\n",
        "      # Apply droupout\n",
        "      # Dropout is applied to the concatenation of the final forward and backward hidden layers\n",
        "      # (Because of bidirectionality)\n",
        "      out = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "      # Through fully connected layer\n",
        "      out = self.fc(out)\n",
        "\n",
        "      return out\n",
        "    \n",
        "    def init_hidden(self):\n",
        "      \"\"\"At the beginning of the sequence, there are no hidden state.\n",
        "          Thus, we need to initialise the hidden state vector.\"\"\"\n",
        "\n",
        "      return (torch.zeros(2, batch_size, 32), torch.zeros(2, batch_size, 32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uyHOi7lm5ON"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1FDyXSsJ11"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "    # Q, K, V = [batch size, seq len, embedding dim]\n",
        "    energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale\n",
        "    # energy = [batch size, seq len, seq len]\n",
        "    attention = self.softmax(energy)\n",
        "    # attention = [batch size, seq len, seq len]\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    x = self.out(x)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3V5rl-WsJ11"
      },
      "outputs": [],
      "source": [
        "# Attention layer\n",
        "class TransformerAttention(nn.Module):\n",
        "  def __init__(self, embedding_matrix, heads, padding_id, max_sequence_length, dropout=0.1):\n",
        "    super(TransformerAttention, self).__init__()\n",
        "    \n",
        "    # Embedding layer\n",
        "    self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.heads = heads\n",
        "    self.head_dim = embedding_dim // heads\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    \n",
        "    assert (self.head_dim * heads == embedding_dim), \"Embedding dimension should be divisible by heads\"\n",
        "\n",
        "    self.attention = SelfAttention(self.embedding_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Linear layer of size d_model * d_model\n",
        "    self.fc = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
        "    # Linear project layer of size d_model * d_classes\n",
        "    self.fc_out = nn.Linear(embedding_dim, 2)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Fetch embedding from input\n",
        "    embs = self.embedding(inputs)\n",
        "    \n",
        "    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n",
        "    # Add position encoding to the embeddings\n",
        "    embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n",
        "    \n",
        "    # Apply dropout\n",
        "    embs = self.dropout(embs) # (batch_size, seq_length, embedding_dim)\n",
        "    \n",
        "    # Apply self attention once \n",
        "    new_embs, attention = self.attention(embs)\n",
        "      \n",
        "    # Average the attention embeddings\n",
        "    avg = torch.mean(new_embs, dim=1)\n",
        "    \n",
        "    # Feed the average attention heads through a fully connected layer\n",
        "    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n",
        "    \n",
        "    # Linear projection of size d_model * nb_classes\n",
        "    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n",
        "    \n",
        "    # Softmax \n",
        "    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n",
        "  \n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN + Attention"
      ],
      "metadata": {
        "id": "Ar-o6eY9o7o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#performer_pytorch FastAttention\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "    # Q, K, V = [batch size, seq len, embedding dim]\n",
        "    energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale\n",
        "    # energy = [batch size, seq len, seq len]\n",
        "    attention = self.softmax(energy)\n",
        "    # attention = [batch size, seq len, seq len]\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    x = self.out(x)\n",
        "    # x = [batch size, seq len, embedding dim]\n",
        "    return x, attention"
      ],
      "metadata": {
        "id": "6YxxNZzgo-s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentiveRNN(nn.Module):\n",
        "  def __init__(self, embedding_matrix, hidden_size, bidirectional, padding_id, max_sequence_length, dropout=0.1, type='LSTM'):\n",
        "    super(AttentiveRNN, self).__init__()\n",
        "    \n",
        "    # Embedding layer\n",
        "    self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    # LSTM layer or GRU layer\n",
        "    if type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    else:\n",
        "      self.rnn = nn.GRU(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    \n",
        "    # Attention layer\n",
        "    self.attention = SelfAttention(hidden_size * 2)\n",
        "    \n",
        "    # Linear layer of size d_model * d_model\n",
        "    self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "    # Linear project layer of size d_model * d_classes\n",
        "    self.fc_out = nn.Linear(hidden_size * 2, 2)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Fetch embedding from input\n",
        "    embs = self.embedding(inputs)\n",
        "    \n",
        "    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n",
        "    # Add position encoding to the embeddings\n",
        "    # embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n",
        "    \n",
        "    # Apply dropout\n",
        "    embs = self.dropout(embs) # (batch_size, seq_length, embedding_dim)\n",
        "    \n",
        "    # Apply LSTM\n",
        "    lstm_out, (hidden, cell) = self.rnn(embs)\n",
        "    \n",
        "    # Apply self attention once \n",
        "    new_embs, attention = self.attention(lstm_out)\n",
        "      \n",
        "    # Average the attention embeddings\n",
        "    avg = torch.mean(new_embs, dim=1)\n",
        "    \n",
        "    # Feed the average attention heads through a fully connected layer\n",
        "    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n",
        "    \n",
        "    # Linear projection of size d_model * nb_classes\n",
        "    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n",
        "    \n",
        "    # Softmax \n",
        "    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n",
        "  \n",
        "    return out"
      ],
      "metadata": {
        "id": "CpYSVO2ppBXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN for document embedding"
      ],
      "metadata": {
        "id": "nV0ouxuapEFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Doc_Emb(nn.Module):\n",
        "    def __init__(self, hidden_dim, dropout, bidirectional, num_layers, type):\n",
        "      super().__init__()\n",
        "\n",
        "      # Embedding layer\n",
        "      #self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n",
        "\n",
        "      # LSTM or GRU\n",
        "      if type==\"LSTM\":\n",
        "        self.model = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            batch_first = True, \n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "      elif type==\"GRU\":\n",
        "        self.model = nn.GRU(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = dropout,\n",
        "                            bidirectional = bidirectional)\n",
        "\n",
        "      # Fully-connected layer\n",
        "      self.fc = nn.Linear(hidden_dim * 2, 2)\n",
        "      # The output of the fully connected layer is a vector of size 2 containing\n",
        "      # the probability of the tweet to belong to either positive or negative\n",
        "      # class (--> 2 classes)\n",
        "\n",
        "      # Dropout\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      # Embed the input\n",
        "      #embs = self.dropout(self.embedding(input))\n",
        "\n",
        "      # pack sequence\n",
        "      # lengths need to be on CPU!\n",
        "      # This will cause the RNN to only process non-padded elements\n",
        "      #packed_embedded = nn.utils.rnn.pack_padded_sequence(embs, input_length.to('cpu'))\n",
        "\n",
        "      # Feed embeddings to LSTM\n",
        "      packed_output, (hidden, cell) = self.model(input)\n",
        "\n",
        "      # Apply droupout\n",
        "      # Dropout is applied to the concatenation of the final forward and backward hidden layers\n",
        "      # (Because of bidirectionality)\n",
        "      out = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "      # Through fully connected layer\n",
        "      out = self.fc(out)\n",
        "\n",
        "      return out\n",
        "    \n",
        "    def init_hidden(self):\n",
        "      \"\"\"At the beginning of the sequence, there are no hidden state.\n",
        "          Thus, we need to initialise the hidden state vector.\"\"\"\n",
        "\n",
        "      return (torch.zeros(2, batch_size, 32), torch.zeros(2, batch_size, 32))"
      ],
      "metadata": {
        "id": "KQu799ENpD0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentiveRNN_Doc_emb(nn.Module):\n",
        "  def __init__(self, hidden_size, bidirectional, dropout=0.1, type='LSTM'):\n",
        "    super(AttentiveRNN_Doc_emb, self).__init__()\n",
        "    \n",
        "    # Embedding layer\n",
        "    self.hidden_size = hidden_size\n",
        "    #self.max_sequence_length = max_sequence_length\n",
        "    \n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    embedding_dim = 300\n",
        "    \n",
        "    # LSTM layer or GRU layer\n",
        "    if type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    else:\n",
        "      self.rnn = nn.GRU(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n",
        "    \n",
        "    # Attention layer\n",
        "    self.attention = SelfAttention(hidden_size * 2)\n",
        "    \n",
        "    # Linear layer of size d_model * d_model\n",
        "    self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "    # Linear project layer of size d_model * d_classes\n",
        "    self.fc_out = nn.Linear(hidden_size * 2, 2)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Fetch embedding from input\n",
        "    #embs = self.embedding(inputs)\n",
        "    \n",
        "    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n",
        "    # Add position encoding to the embeddings\n",
        "    # embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n",
        "    \n",
        "    # Apply dropout\n",
        "    embs = self.dropout(inputs) # (batch_size, seq_length, embedding_dim)\n",
        "    \n",
        "    # Apply LSTM\n",
        "    lstm_out, (hidden, cell) = self.rnn(embs)\n",
        "    \n",
        "    # Apply self attention once \n",
        "    new_embs, attention = self.attention(lstm_out)\n",
        "      \n",
        "    # Average the attention embeddings\n",
        "    avg = torch.mean(new_embs, dim=1)\n",
        "    \n",
        "    # Feed the average attention heads through a fully connected layer\n",
        "    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n",
        "    \n",
        "    # Linear projection of size d_model * nb_classes\n",
        "    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n",
        "    \n",
        "    # Softmax \n",
        "    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n",
        "  \n",
        "    return out"
      ],
      "metadata": {
        "id": "D5HKPZ00pIPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jphW0ft_GJVG"
      },
      "source": [
        "# Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-pCg_J-0QKK"
      },
      "outputs": [],
      "source": [
        "\"\"\"The learnable parametrs of a model are contained in the model's parameters.\n",
        "A state_dict is dictionary that maps each layer to its parameter tensor.\n",
        "The state_dict thus contains all the useful information about the model and must be saved after training. (To maybe be re-loaded later on)\"\"\"\n",
        "def saveModel(model, optimizer):\n",
        "  # Save model\n",
        "  now = time.localtime(time.time())\n",
        "  now_str = time.strftime(\"%a %b %d\", now)\n",
        "\n",
        "  checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict()\n",
        "  }\n",
        "\n",
        "  save_path = root + \"/Models/\" + now_str + \".pth\"\n",
        "  torch.save(checkpoint, save_path)\n",
        "  \n",
        "  \n",
        "def progressBar(loss_training, loss_validation, estimated_time, percent, width = 40):\n",
        "\n",
        "    # Setting up the useful information\n",
        "    left  = width * percent // 100\n",
        "    right = width - left\n",
        "    tags = \"#\" * int(left)\n",
        "    spaces = \" \" * int(right)\n",
        "    percents = f\"{percent:.2f} %\"\n",
        "    loss_training = f\"{loss_training * 1:.6f}\"\n",
        "    loss_validation = f\"{loss_validation * 1:.6f}\"\n",
        "    estimated_time = f\"{estimated_time:.2f}\"\n",
        "\n",
        "    # Displaying a really cool progress bar !\n",
        "    print(\"\\r[\", tags, spaces, \"] - \", percents, \" | Loss (Training) = \", loss_training, \" | Loss (Validation) = \", loss_validation,  \" | Time left : \", estimated_time ,sep=\"\", end=\"\", flush = True)\n",
        "    \n",
        "def binary_accuracy(preds, y):\n",
        "  \"\"\"\n",
        "  Returns the accuracy per batch\n",
        "  \"\"\"\n",
        "\n",
        "  prob_preds = torch.nn.Softmax(dim=-1)(preds)\n",
        "  #print(preds)\n",
        "  #rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "  final_preds = torch.argmax(prob_preds, dim=1)\n",
        "  correct = (final_preds == y).float()\n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABcmlbBt2XIW"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wja-QRV0HIQj"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTONbeGRHHkU",
        "outputId": "6e0c1e33-ee9d-4c25-da20-860d53d41f9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lucienavez/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# Global parameters\n",
        "MODEL_NAME = \"AttentiveRNN\" # Can be \"RNN\", or \"Transformer\" or \"AttentiveRNN\" or \"DocumentEmbs\"\n",
        "RNN_TYPE = \"LSTM\" # Can be \"LSTM\" or \"GRU\"\n",
        "DROPOUT = 0.2\n",
        "NB_EPOCHS = 20\n",
        "PADDING_ID = word2index[\"<PAD>\"]\n",
        "# ATTENTION :\n",
        "# Learning the embedding of <PAD> tokens is irrelevant to determine the sentiment of a tweet!\n",
        "# The embedding for this token must stay what it was initialized to. To do that, we need to tell the newtork that # this token must not be learnt. \n",
        "\n",
        "# Automatic save \n",
        "checkpoints = [2, 5, 10, 15, 20]\n",
        "\n",
        "#--------------------------------------#\n",
        "# RNN model parameters\n",
        "HIDDEN_DIM = 32 #tested 256 but barely changed the results\n",
        "BIDIRECTIONAL = True\n",
        "NUM_LAYERS = 2 # For GRUs\n",
        "\n",
        "#--------------------------------------#\n",
        "# Transformer model parameters\n",
        "HEADS = 1\n",
        "\n",
        "\n",
        "#--------------------------------------#\n",
        "#                  MODEL               #\n",
        "#--------------------------------------#\n",
        "if MODEL_NAME == \"RNN\":\n",
        "  model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, RNN_TYPE)\n",
        "elif MODEL_NAME == \"Transformer\":\n",
        "  model = TransformerAttention(embedding, HEADS, PADDING_ID, seq_length, DROPOUT)\n",
        "elif MODEL_NAME == \"AttentiveRNN\":\n",
        "  model = AttentiveRNN(embedding, HIDDEN_DIM, BIDIRECTIONAL, PADDING_ID, seq_length, dropout=DROPOUT, type=RNN_TYPE)\n",
        "elif MODEL_NAME == \"DocumentEmbs\":\n",
        "  model = RNN_Doc_Emb(HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, NUM_LAYERS, RNN_TYPE)\n",
        "  #model = AttentiveRNN_Doc_emb(HIDDEN_DIM, BIDIRECTIONAL, dropout=DROPOUT, type=RNN_TYPE)\n",
        "\n",
        "model = model.to(device) # Move to GPU\n",
        "\n",
        "# Criterion\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Early stopping"
      ],
      "metadata": {
        "id": "TTDariXxpQXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 6    # Number of epochs to wait before early stopping\n",
        "epochs_no_improve = 0  # Number of epochs with no improvement in validation loss\n",
        "early_stop = False    # Boolean to activate early stopping"
      ],
      "metadata": {
        "id": "FyqHoTyLpP-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nMxfAEKHPpu",
        "outputId": "56269592-366d-4b8a-c911-9eb47d9a7f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0:\n",
            "[#############                           ] - 34.26 % | Loss (Training) = 0.728871 | Loss (Validation) = 0.000000 | Time left : -1444.24                                      ] - 2.79 % | Loss (Training) = 0.677296 | Loss (Validation) = 0.000000 | Time left : -116.84##                                      ] - 5.15 % | Loss (Training) = 0.699634 | Loss (Validation) = 0.000000 | Time left : -218.14###                                     ] - 8.96 % | Loss (Training) = 0.690041 | Loss (Validation) = 0.000000 | Time left : -376.25####                                    ] - 10.13 % | Loss (Training) = 0.716126 | Loss (Validation) = 0.000000 | Time left : -425.42####                                    ] - 11.55 % | Loss (Training) = 0.726862 | Loss (Validation) = 0.000000 | Time left : -485.04####                                    ] - 12.23 % | Loss (Training) = 0.704896 | Loss (Validation) = 0.000000 | Time left : -512.73#####                                   ] - 13.81 % | Loss (Training) = 0.668365 | Loss (Validation) = 0.000000 | Time left : -579.65######                                  ] - 16.54 % | Loss (Training) = 0.656430 | Loss (Validation) = 0.000000 | Time left : -693.03#######                                 ] - 18.30 % | Loss (Training) = 0.808551 | Loss (Validation) = 0.000000 | Time left : -765.10#######                                 ] - 19.81 % | Loss (Training) = 0.630125 | Loss (Validation) = 0.000000 | Time left : -824.87#######                                 ] - 19.95 % | Loss (Training) = 0.721683 | Loss (Validation) = 0.000000 | Time left : -830.35########                                ] - 20.18 % | Loss (Training) = 0.678712 | Loss (Validation) = 0.000000 | Time left : -839.40########                                ] - 21.25 % | Loss (Training) = 0.719785 | Loss (Validation) = 0.000000 | Time left : -884.68########                                ] - 22.28 % | Loss (Training) = 0.714903 | Loss (Validation) = 0.000000 | Time left : -926.04#########                               ] - 22.84 % | Loss (Training) = 0.749833 | Loss (Validation) = 0.000000 | Time left : -950.92##########                              ] - 25.48 % | Loss (Training) = 0.705067 | Loss (Validation) = 0.000000 | Time left : -1061.33##########                              ] - 26.19 % | Loss (Training) = 0.926642 | Loss (Validation) = 0.000000 | Time left : -1092.78###########                             ] - 28.29 % | Loss (Training) = 0.705181 | Loss (Validation) = 0.000000 | Time left : -1186.68###########                             ] - 28.73 % | Loss (Training) = 0.677852 | Loss (Validation) = 0.000000 | Time left : -1206.34############                            ] - 30.37 % | Loss (Training) = 0.772208 | Loss (Validation) = 0.000000 | Time left : -1277.57############                            ] - 30.74 % | Loss (Training) = 0.748584 | Loss (Validation) = 0.000000 | Time left : -1293.11#############                           ] - 33.65 % | Loss (Training) = 0.697141 | Loss (Validation) = 0.000000 | Time left : -1417.35"
          ]
        }
      ],
      "source": [
        "# Store training info\n",
        "losses_train = []\n",
        "accuracies_train = []\n",
        "losses_val = []\n",
        "accuracies_val = []\n",
        "\n",
        "estimated_time = 0 # To estimate time of training\n",
        "train_size = len(X_train)\n",
        "val_size = len(X_val)\n",
        "\n",
        "for epoch in range(NB_EPOCHS):\n",
        "\n",
        "  print('EPOCH {}:'.format(epoch))\n",
        "  start = time.time()\n",
        "  index = batch_size\n",
        "\n",
        "  #---------------------------------------------------------------------------\n",
        "  #                                   Training\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Training set\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Data to GPU\n",
        "    tweet = batch[0].to(device)\n",
        "    sentiment = batch[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad() # Resets gradients\n",
        "\n",
        "    # Combute predictions\n",
        "    predictions = model(tweet).squeeze(1)\n",
        "\n",
        "    # Compute loss and accuracy\n",
        "    loss = criterion(predictions, sentiment)\n",
        "    acc = binary_accuracy(predictions, sentiment)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Optimize parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store loss anc accuracy for training \n",
        "    train_loss.append(loss.detach().item())\n",
        "    train_acc.append(acc.detach().item())\n",
        "\n",
        "    # Remove data from GPU\n",
        "    tweet.to('cpu')\n",
        "    sentiment.to('cpu')\n",
        "\n",
        "    # Update progress bar\n",
        "    time_left = estimated_time -(time.time() - start)\n",
        "    progressBar(loss, 0, time_left, (index/train_size)*100)\n",
        "    index = index + batch_size\n",
        "  \n",
        "  # Compute mean loss and accuracy for training epoch\n",
        "  mean_loss = sum(train_loss)/len(train_loss)\n",
        "  losses_train.append(mean_loss)\n",
        "  mean_acc = sum(train_acc)/len(train_acc)\n",
        "  accuracies_train.append(mean_acc)\n",
        "\n",
        "  # Progress bar\n",
        "  estimated_time = time.time() - start\n",
        "  progressBar(mean_loss, 0, 0, 100)\n",
        "\n",
        "  #---------------------------------------------------------------------------\n",
        "  #                                 Validation\n",
        "  #---------------------------------------------------------------------------\n",
        "  index_validation = batch_size\n",
        "\n",
        "  # Validation set\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_dataloader):\n",
        "\n",
        "      # Data to GPU\n",
        "      tweet = batch[0].to(device)\n",
        "      sentiment = batch[1].to(device)\n",
        "\n",
        "      # Combute predictions\n",
        "      predictions = model(tweet).squeeze(1)\n",
        "\n",
        "      # Compute loss and accuracy\n",
        "      loss_val = criterion(predictions, sentiment)\n",
        "      acc_val = binary_accuracy(predictions, sentiment)\n",
        "\n",
        "      # Store loss and acccuracy for validation\n",
        "      val_loss.append(loss_val.detach().item())\n",
        "      val_acc.append(acc_val.detach().item())\n",
        "\n",
        "      # Remove data from GPU\n",
        "      tweet.to('cpu')\n",
        "      sentiment.to('cpu')\n",
        "\n",
        "      # Update progress bar\n",
        "      progressBar(mean_loss, loss_val, time_left, (index_validation/val_size)*100)\n",
        "      index_validation = index_validation + batch_size\n",
        "\n",
        "    # Compute mean loss and accuracy for validation epoch\n",
        "    mean_loss_val = sum(val_loss)/len(val_loss)\n",
        "    losses_val.append(mean_loss_val)\n",
        "    mean_acc_val = sum(val_acc)/len(val_acc)\n",
        "    accuracies_val.append(mean_acc_val)\n",
        "\n",
        "    # EARLY STOPPING\n",
        "    if mean_loss_val < min(losses_val):\n",
        "      print(\"Validation loss decreased\")\n",
        "      epochs_no_improve = 0\n",
        "    else:\n",
        "      epochs_no_improve += 1\n",
        "      if epochs_no_improve == patience:\n",
        "        print(\"Early stopping!\")\n",
        "        early_stop = True\n",
        "        # Save model and break\n",
        "        saveModel(model, optimizer)\n",
        "        break\n",
        "\n",
        "\n",
        "    # Display useful information\n",
        "    estimated_time = time.time() - start\n",
        "    progressBar(mean_loss, mean_loss_val, 0, 100)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Automatic save of the model\n",
        "    if (epoch+1) in checkpoints:\n",
        "      saveModel(model, optimizer)\n",
        "      \n",
        "# Information over terminal\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"               Finished training and model saved                     \")\n",
        "print(\"---------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "181-MlvGIvZN"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(losses_train, label=\"Training loss\")\n",
        "plt.plot(losses_val, label=\"Validation loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUSDmG9cHdHI"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(accuracies_train, label=\"train\")\n",
        "plt.plot(accuracies_val, label=\"validation\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjuhVjdQoSRy"
      },
      "source": [
        "## Load Model\n",
        "If you want to reload the weights of a previous model for testing of starting training again : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "i_maD8sxoTTO",
        "outputId": "50c4254a-e6f3-44c0-a46e-45bf5b1c31c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(393023, 300, padding_idx=289283)\n",
              "  (model): LSTM(300, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_str = \"Wed Nov 02\"\n",
        "model_path = root + \"/Models/\" + model_str + \".pth\"\n",
        "#model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, \"LSTM\")\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNKBj16xm5OP",
        "outputId": "c3c9b4c0-3e7a-4101-d0db-6311a3bf30c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(393023, 300, padding_idx=289283)\n",
              "  (model): LSTM(300, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qArMwrpwXTsl"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTFdt4uf5Wiz"
      },
      "source": [
        "### Evaluate the model with the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qClHWSRXXgIf"
      },
      "outputs": [],
      "source": [
        "def positive_negative_count(preds, y):\n",
        "  \"\"\"\n",
        "  Returns the accuracy per batch\n",
        "  \"\"\"\n",
        "  pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "\n",
        "  prob_preds = torch.nn.Softmax(dim=-1)(preds)\n",
        "  final_preds = torch.argmax(prob_preds, dim=1)\n",
        "\n",
        "  numpy_preds = final_preds.cpu().numpy()\n",
        "  numpy_y = y.cpu().numpy()\n",
        "\n",
        "  for i in range(len(y)):\n",
        "    if numpy_preds[i] == numpy_y[i]:\n",
        "      if numpy_y[i] == 0:\n",
        "        neg_correct += 1\n",
        "      else:\n",
        "        pos_correct += 1\n",
        "\n",
        "    if numpy_y[i] == 0:\n",
        "      neg_cnt += 1\n",
        "    else:\n",
        "      pos_cnt += 1\n",
        "\n",
        "  return neg_correct, neg_cnt , pos_correct, pos_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn3xHSv5XVvr",
        "outputId": "a56ba9a2-515c-4743-d228-9d2f3e2b2783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6445783376693726\n",
            "0.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "#accuracy = 0\n",
        "#pos_acc = 0\n",
        "#neg_acc = 0\n",
        "neg_correct = 0\n",
        "neg_cnt = 0\n",
        "pos_correct = 0\n",
        "pos_cnt = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    # Data to GPU\n",
        "    tweet = batch[0].to(device)\n",
        "    sentiment = batch[1].to(device)\n",
        "\n",
        "    predictions = model(tweet).squeeze(1)\n",
        "\n",
        "    #acc = binary_accuracy(predictions, sentiment)\n",
        "    neg_correct_tmp, neg_cnt_tmp, pos_correct_tmp, pos_cnt_tmp = positive_negative_count(predictions, sentiment)\n",
        "\n",
        "    #print(neg_correct_tmp, neg_cnt_tmp, pos_correct_tmp, pos_cnt_tmp)\n",
        "\n",
        "    neg_correct += neg_correct_tmp\n",
        "    neg_cnt += neg_cnt_tmp\n",
        "    pos_correct += pos_correct_tmp\n",
        "    pos_cnt += pos_cnt_tmp\n",
        "\n",
        "    #if batch_idx%500 == 0:\n",
        "    #  print(batch_idx, \"/\", len(test_dataloader))\n",
        "  \n",
        "\n",
        "    # Remove data from GPU\n",
        "    tweet.to('cpu')\n",
        "    sentiment.to('cpu')\n",
        "\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(\"                   classified positive   classified negative \")\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(\" Actual positive     \", pos_correct, \"                 \", neg_cnt-neg_correct)\n",
        "print(\" Actual negative     \", pos_cnt-pos_correct, \"                 \", neg_correct)\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "print(\"Acuracy: \", (pos_correct+neg_correct)/(pos_cnt+neg_cnt))\n",
        "print(\"Precision: \", pos_correct/(pos_correct + neg_cnt-neg_correct))\n",
        "print(\"Recall: \", neg_correct/(neg_correct + pos_cnt-pos_correct))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4kIUQ6d5aex"
      },
      "source": [
        "### Predict on a given sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0jOCMDMmhq9"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "  splitted = sentence.split(\" \")\n",
        "  #print(splitted)\n",
        "  Sentence_tokens = padding_and_encode(splitted, seq_length)\n",
        "  seq = [Sentence_tokens]\n",
        "  #print(seq)\n",
        "  Sentence_tokens = torch.tensor(seq).to(device)\n",
        "  #print(Sentence_tokens)\n",
        "  #prediction = []\n",
        "  with torch.no_grad():\n",
        "    prediction = model(Sentence_tokens).squeeze(1)\n",
        "  #print(prediction)\n",
        "  prob_pred = torch.nn.Softmax(dim=-1)(prediction)\n",
        "  final_pred = torch.argmax(prob_pred, dim=1)\n",
        "  #print(final_pred)\n",
        "  f = final_pred.cpu().numpy()\n",
        "  #print(f)\n",
        "  if(f[0] == 1):\n",
        "    print(\"Positive\")\n",
        "  else:\n",
        "    print(\"Negative\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f4-FIf9nDmC",
        "outputId": "3e4e9741-a681-447e-8996-587c459e817b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive\n",
            "Positive\n",
            "Positive\n",
            "Positive\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "predict(\"I was very disappointed with this series. It had lots of cool graphics and that's about it. The level of detail it went into was minimal, and I always got the feeling the audience was being patronized -- there was a lot of what seemed to me as \")\n",
        "predict(\"Great just great! The West Coast got Dirty Harry Callahan, the East Coast got Sharky. Burt Reynolds plays Sharky in \")\n",
        "predict(\"I loved that movie\")\n",
        "predict(\"I hate you\")\n",
        "predict(\"you are ugly\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('torch-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ef1b119d281e6c8960c7e46c636ce2a9ec354a3da52520939e5f0bfd0bbd7c37"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}