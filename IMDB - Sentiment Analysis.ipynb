{"cells":[{"cell_type":"markdown","metadata":{"id":"9yW-OMw3wSYB"},"source":["# Utils"]},{"cell_type":"markdown","metadata":{"id":"-mwt90Bj1PB_"},"source":["Tuto pour démarrer :\n","https://bhadreshpsavani.medium.com/tutorial-on-sentimental-analysis-using-pytorch-b1431306a2d7"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20837,"status":"ok","timestamp":1668069973210,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"smH29zmcyd43","outputId":"78e86d65-e621-41d0-feac-4fda282773ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eJlIuoU3Wun9"},"source":["## Paths"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Y_25-QAQzKQg","executionInfo":{"status":"ok","timestamp":1668069975967,"user_tz":-60,"elapsed":348,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["root = '/content/drive/MyDrive/Sentiment Analyis'\n","#root = \"/Users/lucienavez/Library/CloudStorage/GoogleDrive-lucienavez@gmail.com/Mon Drive/Sentiment Analyis\""]},{"cell_type":"markdown","metadata":{"id":"jUDD5XUlwPw2"},"source":["## Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22748,"status":"ok","timestamp":1668070000638,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"WrokRsDpzV99","outputId":"5a06172a-235d-456d-ad3f-401c747649bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bcolz-zipline\n","  Downloading bcolz_zipline-1.2.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 15.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.23,>=1.16 in /usr/local/lib/python3.7/dist-packages (from bcolz-zipline) (1.21.6)\n","Installing collected packages: bcolz-zipline\n","Successfully installed bcolz-zipline-1.2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/nightly/cpu\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting contractions\n","  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 16.5 MB/s \n","\u001b[?25hCollecting anyascii\n","  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n","\u001b[K     |████████████████████████████████| 287 kB 56.3 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]}],"source":["# Missing libraries\n","! pip install bcolz-zipline\n","! pip install gensim\n","! pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n","! pip install contractions\n","! pip install bs4\n","\n","# Basics\n","import numpy as np\n","import pandas as pd\n","import time\n","import re\n","import pickle\n","import bcolz\n","import matplotlib.pyplot as plt\n","import contractions\n","import string\n","from collections import Counter, OrderedDict\n","from bs4 import BeautifulSoup\n","\n","# NLTK\n","import nltk\n","nltk.download('wordnet') # for lemmatization\n","nltk.download('stopwords') # for stopwords\n","nltk.download('omw-1.4') # Don't know why but code doesn't work (lemmization part)\n","nltk.download('words')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchtext\n","from torchtext.vocab import vocab\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchtext.datasets import IMDB\n","\n","\n","# Scikit-Learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Gensim\n","import gensim\n","from gensim.models import KeyedVectors\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.models import TfidfModel\n","from gensim.corpora import Dictionary"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1668070003059,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"9mdMjOQ5TOku","outputId":"8a6adff0-81aa-44a4-ea3c-253496ea3fc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["# Hardware\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\") \n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"KnPsBKPUpZuy"},"source":["# Parameters"]},{"cell_type":"markdown","metadata":{"id":"th-M7v_OTY7i"},"source":["# Dataset\n","The dataset is downloaded from the following link : \n","https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/data\n","\n","This dataset is made of movie review available on amazon\n","\n","The dataset follows the following format :\n","\n","\n","2 fields:\n","\n","- review: text of the review\n","- sentiment: 2 possible values (positive or negative)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":488,"status":"error","timestamp":1667989569089,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"qHYgzvwIYcIn","outputId":"d9baf942-4317-4e08-881c-98c8066859cc"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-98e58b6670e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/IMDB Dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/lucienavez/Library/CloudStorage/GoogleDrive-lucienavez@gmail.com/Mon Drive/Sentiment Analyis/data/IMDB Dataset.csv'"]}],"source":["#Load the data\n","data_path = root + '/data/IMDB Dataset.csv'\n","\n","df = pd.read_csv(data_path, encoding='latin-1')\n","print(\"Size: \", df.shape)\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1667987436139,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"Q--LPdQqmFZ_","outputId":"e8417339-d9b7-479f-df46-43505b1056f2"},"outputs":[{"data":{"text/plain":["positive    25000\n","negative    25000\n","Name: sentiment, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df['sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1667987436139,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"-yUNhSdQGiI1","outputId":"0e364888-7bd4-4902-b337-f7e796378d92"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                  review sentiment\n","40877  I recently started watching this show, and I h...  positive\n","18057  \"Return of the Jedi\" is often remembered for w...  positive\n","19066  I remember I loved this movie when it came out...  negative\n","20525  I don't know what the last reviewer is talking...  positive\n","5847   From the very beginning I was so excited to se...  positive\n","                                                  review sentiment\n","33553  I really liked this Summerslam due to the look...  positive\n","9427   Not many television shows appeal to quite as m...  positive\n","199    The film quickly gets to a major chase scene w...  negative\n","12447  Jane Austen would definitely approve of this o...  positive\n","39489  Expectations were somewhat high for me when I ...  negative\n"]}],"source":["#Split the data between a training set and a test set\n","df_train, df_test = train_test_split(df, random_state = 42, shuffle = True, test_size = 0.1)\n","print(df_train.head())\n","print(df_test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDDigm6nGliP"},"outputs":[],"source":["#Create 2 separate csv file in order to load them later\n","df_train.to_csv(root + '/data/train_imdb.csv', index=True)\n","df_test.to_csv(root + '/data/test_imdb.csv', index=True)"]},{"cell_type":"markdown","metadata":{"id":"Rcxi4X5CWuoA"},"source":["## Pre-processing\n","\n","See following links : \n","https://medium.com/analytics-vidhya/pre-processing-tweets-for-sentiment-analysis-a74deda9993e\n","https://towardsdatascience.com/how-to-preprocess-social-media-data-and-text-messages-b011efacf74\n","\n","Data must be pre-processed in order to be used for training.\n","For the time being, the only kind of pre-processing that has been applied is removing the emojis.\n","Howerver, some parts of the tweets may be noisy for the model and would require extra attention : \n","- usernames and @ (handles) : @LATimesautos, @kirstiealley, ...\n","- hashtags : #lebron, #kindle2\n","- abbreviations : LMAO, pls, DM\n","- people names + brands : Obama, Kindle, Nike, Susan Boyle\n","- html characters : &amp, &lt, &gt\n","- urls and links : http://bit.ly/1e1xQ6, http://t.co/3wEwWZi, http://t.co/3wEwWZi, ...\n","- weird punctuation : --, -, \n","- unexpected lowercases, uppercases\n","- emojis with punctuation : :), :-), =D\n","- ...\n","\n","Analyse data to find out what kind of pre-processing is needed.\n","\n","1. Put everything to lowercase (in the dataset, it does not seem that lowercase and uppercase play a significant enough role in the sentiment of the tweet)\n","\n","2. URLs : remove them, they won't be useful for the model\n","3. HTML characters : remove them, they won't be useful for the model\n","4. Emojis : I don't know if we should remove them or not, they may be useful for the model because they can be used to express emotions. For instance, we could imagine to replace them by their meaning : :) could be \"smile\", :D could be \"laugh\", :/ could be \"sad\", etc. However, it is not clear that the model will be able to understand the meaning of the emojis. For the time being, I have removed them.\n","5. Punctuation : remove it, it won't be useful for the model\n","6. Numbers : remove them, they won't be useful for the model\n","7. Abbreviations : replace them with their full form, they may be useful for the model. We can maybe use a dictionary to replace them (custom) --> Maybe this is overkill compared to the actual gain of doing that. Or we can use a library like TextBlob to replace them (not custom).\n","8. Hashtags : remove them, they won't be useful for the model\n","9. @ : remove them, they won't be useful for the model\n","10. People names : remove them, they won't be useful for the model\n","11. Removing stopwords : remove them, they won't be useful for the model\n","\n","Additionnaly we can use techniques as :\n","- Lemmatization : converting a word to its base form. For instance : \"running\" -> \"run\", \"better\" -> \"good\", \"am\" -> \"be\", etc.\n","- Stemming : removing the suffixes of a word. For instance : \"running\" -> \"run\", \"better\" -> \"better\", \"tarts\" -> \"tart\", etc. Only removes suffixes, not prefixes. Stemming is faster than lemmatization but less accurate.\n","\n","Example where lemmatization and stemming are applied :\n","- Lemmatization : \"Caring\" -> \"Care\"\n","- Stemming : \"Caring\" -> \"Car\"\n","\n","Obviously, \"Care\" is not the base form of \"Caring\" and \"Car\" is not the base form of \"Caring\" either.\n","There are many ways to implement them but the most common one is to use the NLTK package.\n","https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"]},{"cell_type":"markdown","metadata":{"id":"XqU0E4AypktY"},"source":["## Functions"]},{"cell_type":"markdown","metadata":{"id":"5zkfaRdJWuoB"},"source":["#### Lemmization, Stemming and Stopwords"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wJ1lRVQ98ioU","executionInfo":{"status":"ok","timestamp":1668070011472,"user_tz":-60,"elapsed":287,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["def lemmatize(df):\n","  lemmatizer = WordNetLemmatizer()\n","  df.text = df.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","  return df\n","\n","def stem(df):\n","  stemmer = nltk.stem.PorterStemmer()\n","  df.text = df.text.apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","  return df\n","\n","def removeStopwords(df):\n","  english_stopwords = stopwords.words('english')\n","  df.text = df.text.apply(lambda x: [item for item in x if item not in english_stopwords])\n","\n","  return df\n","\n","def expandContractions(df):\n","  df.text = df.text.apply(lambda x: [contractions.fix(word) for word in x])\n","\n","  return df"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VBhCnWUvrNBI","executionInfo":{"status":"ok","timestamp":1668070013532,"user_tz":-60,"elapsed":302,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["def preProcess(df, lemmatization = True, stemming = False, remove_stopwords = True):\n","  \n","  # Lowercase the text\n","  df.text = df.text.str.lower()\n","  # Remove urls \n","  df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n","  df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n","  # Remove html reference characters\n","  df.text = df.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n","  # Remove hashtags (must be done before removing punctuation)\n","  df.text = df.text.apply(lambda x: re.sub(r'#', '', x))\n","  # Remove mentions (must be done before removing punctuation)\n","  df.text = df.text.apply(lambda x: re.sub(r'@\\w+', '', x))\n","  # Remove numbers\n","  df.text = df.text.apply(lambda x: re.sub(r'\\d+', '', x))\n","\n","  # Remove special characters\n","  df.text = df.text.apply(lambda x: re.sub(r'\\[[^]]*\\]', '', x))\n","  df.text = df.text.apply(lambda x: re.sub(r'[^a-zA-z0-9\\s]', '', x))\n","\n","  print(\"Special characters removed\")\n","\n","  # Tokenize\n","  tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n","  df['text'] = df.apply(lambda row: tokenizer(row['text']), axis=1)\n","\n","  print(\"Text tokenized\")\n","\n","  # Expand contractions\n","  df = expandContractions(df)\n","\n","  if remove_stopwords:\n","    # Remove stopwords\n","    df = removeStopwords(df)\n","\n","  print(\"Stop words removed\")\n","    \n","  if lemmatization:\n","    # Lemmatization\n","    df = lemmatize(df)\n","\n","  if stemming:\n","    # Stemming\n","    df = stem(df)\n","\n","  print(\"Lemmatized and stemmatized\")\n","\n","    \n","  # Remove punctuation tokens in each row of the text column\n","  df.text = df.text.apply(lambda x: [item for item in x if item not in string.punctuation])\n","  # Remove any weird characters tokens\n","  df.text = df.text.apply(lambda x: [item for item in x if item.isalpha()])\n","  # Remove any tokens with length less than 2\n","  df.text = df.text.apply(lambda x: [item for item in x if len(item) > 2])\n","  # Remove any repeated tokens\n","  df.text = df.text.apply(lambda x: list(OrderedDict.fromkeys(x)))\n","\n","  print(\"Punctuation removed\")\n","\n","  # Remove any non-english words\n","  english_words = set(nltk.corpus.words.words())\n","  df.text = df.text.apply(lambda x: [item for item in x if item in english_words])\n","\n","  print(\"Non-english words removed\")\n","\n","  return df"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uTHoUz8Cpnun","executionInfo":{"status":"ok","timestamp":1668070015737,"user_tz":-60,"elapsed":408,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["def loadData(data_path):\n","  # Load CSV file\n","  df = pd.read_csv(data_path, encoding='latin-1')\n","\n","  # Add sentiment and text column header\n","  column_names = ['text', 'sentiment']\n","  df = pd.DataFrame(zip(df.iloc[:, 1], df.iloc[:, 2]), columns=column_names)\n","\n","  # Pre-process\n","  df = preProcess(df)\n","\n","  # Vocabulary size \n","  # Build the vocabulary\n","  voc = set()\n","  for text in df.text:\n","    for word in text:\n","      voc.add(word)\n","\n","  return df, len(voc), voc"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98437,"status":"ok","timestamp":1668070116368,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"EZmZfLA4TZ7f","outputId":"93e1cba7-23fa-4610-eb85-ad999b7225cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading train data...\n","Special characters removed\n","Text tokenized\n","Stop words removed\n","Lemmatized and stemmatized\n","Punctuation removed\n","Non-english words removed\n","\n","Loading test data...\n","Special characters removed\n","Text tokenized\n","Stop words removed\n","Lemmatized and stemmatized\n","Punctuation removed\n","Non-english words removed\n"]}],"source":["# Read the data at the path\n","data_path = root + '/data/'\n","print(\"Loading train data...\")\n","df_train, vocab_size_train, voc_train = loadData(data_path + 'train_imdb.csv')\n","print(\"\\nLoading test data...\")\n","df_test, vocab_size_test, voc_test = loadData(data_path + 'test_imdb.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1668070172378,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"Y5MMAWJABX1Z","outputId":"e0f1c38a-9406-4369-eae2-fd3dbfd02402"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train size:  45000\n","negative    22519\n","positive    22481\n","Name: sentiment, dtype: int64\n","                                                text sentiment\n","0  [recently, watching, show, say, really, made, ...  positive\n","1  [return, often, wrong, rather, right, shame, l...  positive\n","2  [remember, movie, came, year, old, commodore, ...  negative\n","3  [know, last, reviewer, talking, show, pure, en...  positive\n","4  [beginning, excited, see, movie, poster, possi...  positive\n"]}],"source":["print(\"Train size: \", df_train.text.size)\n","print(df_train.sentiment.value_counts())\n","print(df_train.head())"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1668070174740,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"BGWDZr4uBpP6","outputId":"5bc35ccd-5611-4e55-bfd4-387e3f6ff94f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test size:  5000\n","positive    2519\n","negative    2481\n","Name: sentiment, dtype: int64\n","                                                text sentiment\n","0  [really, due, look, arena, curtain, overall, i...  positive\n","1  [many, television, show, appeal, quite, differ...  positive\n","2  [film, quickly, get, major, chase, scene, ever...  negative\n","3  [jane, would, definitely, approve, awesome, jo...  positive\n","4  [expectation, somewhat, high, went, see, movie...  negative\n"]}],"source":["print(\"Test size: \", df_test.text.size)\n","print(df_test.sentiment.value_counts())\n","print(df_test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1667989772869,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"LZgkwmLqYJrE","outputId":"1e51a6bb-59c2-4ba6-df7a-435279dd5f32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train data shape:  (45000, 2)\n","Test data shape:  (5000, 2)\n","Vocabulary size:  30615\n","Vocabulary size:  17650\n","Vocabulary train:  ['sickeningly', 'wary', 'sagittarius', 'outwit', 'progovernment', 'indignation', 'employed', 'unyieldingly', 'insufficiency', 'asbestos']\n","Vocabulary test:  ['restrictive', 'sickeningly', 'variously', 'wary', 'veer', 'snuggle', 'diversity', 'nodule', 'escapist', 'outwit']\n","Train data sample: \n","                                                 text sentiment\n","0  [recently, watching, show, say, really, made, ...  positive\n","1  [return, often, wrong, rather, right, shame, l...  positive\n","2  [remember, movie, came, year, old, commodore, ...  negative\n","3  [know, last, reviewer, talking, show, pure, en...  positive\n","4  [beginning, excited, see, movie, poster, possi...  positive\n","Test data sample: \n","                                                 text sentiment\n","0  [really, due, look, arena, curtain, overall, i...  positive\n","1  [many, television, show, appeal, quite, differ...  positive\n","2  [film, quickly, get, major, chase, scene, ever...  negative\n","3  [jane, would, definitely, approve, awesome, jo...  positive\n","4  [expectation, somewhat, high, went, see, movie...  negative\n"]}],"source":["print('Train data shape: ', df_train.shape)\n","print('Test data shape: ', df_test.shape)\n","print('Vocabulary size: ', vocab_size_train)\n","print('Vocabulary size: ', vocab_size_test)\n","print('Vocabulary train: ', list(voc_train)[:10])\n","print('Vocabulary test: ', list(voc_test)[:10])\n","print('Train data sample: \\n', df_train.head())\n","print('Test data sample: \\n', df_test.head())"]},{"cell_type":"markdown","metadata":{"id":"0FsWOb4-sYF4"},"source":["## Build vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATKXjICmsYF4"},"outputs":[],"source":["# Build the vocabulary for the training data\n","vocabulary = set()\n","\n","# Add special tokens to the vocabulary\n","pad = \"<PAD>\" # Used to pad short sentences to the MAX_TOKENS length\n","sos = \"<SOS>\" # Start-of-sentence\n","eos = \"<EOS>\" # End-of-sentence\n","ukn = \"<UKN>\" # Unknown word\n","\n","vocabulary.add(pad)\n","vocabulary.add(sos)\n","vocabulary.add(eos)\n","vocabulary.add(ukn)\n","\n","\n","MAX_TOKENS = 0 # Maximum number of tokens in a sentence\n","\n","# Iterate over the training data\n","for row in range(len(df_train)):\n","      # Each row[\"text\"] is a list of tokens\n","      # Each token is a word that has to be added to the vocabulary\n","      MAX_TOKENS = max(MAX_TOKENS, len(df_train.loc[row, \"text\"]))\n","      # Save the row that has the maximum number of tokens\n","      if MAX_TOKENS == len(df_train.loc[row, \"text\"]):\n","        max_row = df_train.loc[row, \"text\"]\n","        row_index = row\n","      tokens = df_train.loc[row, \"text\"]\n","      for word in tokens:\n","        vocabulary.add(word)\n","            \n","vocab_size = len(vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1667987593278,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"VvvErr8d2Qg7","outputId":"f040e211-46b1-411a-e70f-3206a8d8b696"},"outputs":[{"name":"stdout","output_type":"stream","text":["692\n","30619\n","['spoiler', 'although', 'many', 'commentator', 'film', 'term', 'fit', 'poorly', 'quote', 'encyclopedia', 'fantastic', 'incongruous', 'imagery', 'one', 'explain', 'unimaginative', 'way', 'plucky', 'boy', 'large', 'seeking', 'fortune', 'driver', 'seat', 'red', 'mustang', 'could', 'curious', 'might', 'read', 'said', 'lad', 'behind', 'wheel', 'sport', 'car', 'surely', 'protest', 'fantasy', 'incongruity', 'offer', 'mostly', 'appear', 'within', 'first', 'fifteen', 'minute', 'thereafter', 'get', 'iteration', 'squalid', 'progression', 'far', 'soon', 'prof', 'predictable', 'hand', 'literally', 'believable', 'unfair', 'tax', 'particular', 'flaw', 'plausible', 'suspension', 'disbelief', 'fallen', 'precipitously', 'typical', 'viewer', 'scale', 'value', 'ever', 'since', 'raider', 'lost', 'ark', 'hallucinatory', 'know', 'hallucination', 'part', 'knowing', 'rate', 'people', 'enjoy', 'drug', 'result', 'loss', 'anyone', 'would', 'take', 'course', 'occasional', 'bad', 'trip', 'movie', 'must', 'pun', 'juxtaposition', 'word', 'startling', 'time', 'startle', 'god', 'like', 'something', 'damn', 'two', 'interjection', 'par', 'script', 'sadly', 'sense', 'pass', 'dialogue', 'reveal', 'direct', 'proportion', 'naivete', 'regarding', 'speech', 'pattern', 'rising', 'world', 'completely', 'defined', 'minutely', 'make', 'rational', 'indeed', 'cartoon', 'elementary', 'school', 'newspaper', 'numerous', 'guest', 'star', 'cast', 'cameo', 'role', 'even', 'intelligent', 'little', 'hero', 'blink', 'eyelash', 'need', 'several', 'second', 'concoct', 'lie', 'character', 'despite', 'nearly', 'every', 'scene', 'significant', 'development', 'scant', 'reward', 'enough', 'wish', 'better', 'understand', 'coming', 'vaguely', 'story', 'protagonist', 'struggling', 'fully', 'recognizable', 'realistically', 'painted', 'image', 'removed', 'normal', 'context', 'ambiguous', 'paradoxical', 'shocking', 'framework', 'see', 'succession', 'stereotypical', 'dilapidated', 'billboard', 'filling', 'station', 'eatery', 'cheap', 'hotel', 'habitue', 'along', 'country', 'highway', 'exactly', 'largely', 'responsible', 'traditional', 'emphasis', 'content', 'sum', 'picture', 'millionaire', 'dressed', 'clown', 'pirate', 'posh', 'costume', 'party', 'sitting', 'serene', 'mute', 'cautious', 'chauffeur', 'inch', 'fragile', 'skiff', 'sea', 'desperate', 'humanity', 'implore', 'window', 'smear', 'glass', 'blood', 'imagine', 'stadium', 'full', 'abandoned', 'antique', 'limousine', 'white', 'piano', 'ghost', 'detritus', 'wander', 'exhausted', 'ailing', 'woman', 'cling', 'becoming', 'fall', 'asleep', 'side', 'grass', 'feast', 'transfiguration', 'day', 'brilliant', 'flash', 'horizon', 'sun', 'finding', 'consort', 'become', 'corpse', 'belief', 'soul', 'going', 'heaven', 'later', 'innocently', 'learned', 'new', 'today', 'taking', 'photograph', 'sample', 'cinematic', 'surrealism', 'whose', 'irony', 'ripple', 'invade', 'title', 'empire', 'seek', 'please', 'miss', 'ala', 'however', 'hard', 'tread', 'accelerator', 'race', 'chariot', 'beyond', 'desert', 'exquisitely', 'strange', 'rich', 'subtle', 'gorgeous', 'await', 'poor', 'none', 'necessarily', 'though', 'somewhat', 'disappointed', 'cannot', 'dismiss', 'view', 'respectability', 'another', 'genre', 'exemplify', 'sure', 'also', 'expressionism', 'existentialism', 'pessimism', 'amidst', 'omnipotent', 'power', 'structure', 'try', 'size', 'theater', 'turning', 'article', 'style', 'amazed', 'extent', 'absurd', 'valid', 'artistic', 'objection', 'vanish', 'puff', 'smoke', 'entire', 'text', 'support', 'attempt', 'show', 'human', 'situation', 'essentially', 'devoid', 'purpose', 'humankind', 'left', 'feeling', 'hopeless', 'bewildered', 'anxious', 'instantaneously', 'getting', 'away', 'depressing', 'home', 'life', 'among', 'parent', 'find', 'purposeless', 'drive', 'past', 'glittering', 'reading', 'win', 'lottery', 'promise', 'already', 'revealed', 'ambition', 'illusory', 'game', 'never', 'corporation', 'intention', 'trick', 'confuse', 'leave', 'crestfallen', 'aspirant', 'ultimately', 'playwright', 'therefore', 'logical', 'dramatic', 'action', 'conventionally', 'understood', 'frantically', 'perform', 'busyness', 'serf', 'underscore', 'fact', 'nothing', 'change', 'existence', 'timeless', 'circular', 'quality', 'language', 'play', 'repetition', 'obvious', 'sound', 'nonsense', 'underneath', 'sometimes', 'comic', 'surface', 'underlying', 'message', 'metaphysical', 'distress', 'obsession', 'silly', 'inane', 'plot', 'device', 'wherein', 'divine', 'bleak', 'future', 'return', 'moment', 'different', 'still', 'turn', 'much', 'admirer', 'anyway', 'really', 'instead', 'addition', 'work', 'quite', 'stop', 'disillusionment', 'love', 'attendant', 'met', 'person', 'decently', 'service', 'advertising', 'tutelage', 'waiting', 'note', 'famous', 'preoccupation', 'furthermore', 'indirect', 'previous', 'encounter', 'badly', 'maimed', 'arm', 'straight', 'horizontally', 'last', 'protege', 'say', 'want', 'hear', 'music', 'finger', 'end', 'beckoning', 'closer', 'finally', 'author', 'happen', 'currently', 'theologian', 'acknowledged', 'architect', 'undergirder', 'liberation', 'theology', 'catholic', 'movement', 'perhaps', 'police', 'brutality', 'corporate', 'greed', 'cliche', 'cinema', 'literature', 'sentiment', 'impressive', 'warrant', 'scripture', 'tradition', 'expose', 'earthly', 'activity', 'angel', 'principality', 'wrote', 'popular', 'three', 'institution', 'ideology', 'commend', 'worship', 'making', 'false', 'deeply', 'involved', 'becomes', 'slave', 'promising', 'control', 'immortality', 'inexorably', 'deliver', 'helplessness', 'chaos', 'death', 'yet', 'beguile', 'dominion', 'earth', 'book', 'genesis', 'bent', 'inevitably', 'hegemony', 'mistranslation', 'accurate', 'rendering', 'stewardship', 'quibble', 'beside', 'fundamental', 'problem', 'neglect', 'notice', 'reason', 'assume', 'descendent', 'exercise', 'contrary', 'demonic', 'force', 'stolen', 'add', 'observation', 'lewis', 'man', 'conquest', 'nature', 'mere', 'illusion', 'ruse', 'cover', 'talking', 'men', 'instrument', 'secondly', 'satan', 'kind', 'may', 'dangle', 'pleasure', 'niggardly', 'withdraw', 'firmly', 'thrall', 'leaving', 'prey', 'front', 'fire', 'miserably', 'sorry', 'seething', 'insight', 'seem', 'mirrored', 'remarkably', 'experience', 'nice', 'least', 'pretty', 'prior', 'falling', 'victim', 'sign', 'glisten', 'glamorously', 'longer', 'journey', 'towards', 'headquarters', 'shabby', 'lonely', 'meeting', 'else', 'giving', 'card', 'either', 'ruin', 'staffed', 'zombie', 'meet', 'ugly', 'deceitful', 'hostile', 'answer', 'common', 'dictator', 'mean', 'abide', 'totally', 'oblivious', 'partially', 'blinded', 'prematurely', 'aged', 'infantile', 'literal', 'linguistically', 'eventually', 'precious', 'taken', 'crash', 'continue', 'dead', 'wreck', 'long', 'done', 'everything', 'thought', 'present', 'tower', 'receive', 'prize', 'built', 'monument', 'vanity', 'agent', 'evade', 'disappoint', 'insult', 'throw', 'top', 'floor', 'landing', 'body', 'water', 'classic', 'symbolism', 'inevitable', 'put', 'faith', 'fate', 'warning', 'look', 'mutable', 'upon', 'seeing', 'generous', 'selfless', 'act', 'seen', 'almost', 'hour', 'half', 'handicapped', 'hardly', 'able', 'insert', 'hose', 'gas', 'tank', 'help', 'apply', 'job', 'explaining', 'motorist', 'hell', 'place', 'interpretation', 'conjectural', 'surprise', 'outrage', 'cult', 'point', 'cup', 'tea', 'convinced', 'worst', 'made']\n"]}],"source":["print(MAX_TOKENS)\n","print(vocab_size)\n","print(max_row)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCi-HitJ1GeC"},"outputs":[],"source":["# For each word in the vocabulary, assign a word id\n","word2index = {}\n","for index, word in enumerate(vocabulary):\n","    word2index[word] = index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0yOhek36Lew"},"outputs":[],"source":["# For each word id, assign a word\n","index2word = {id: token for token, id in enumerate(word2index)}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1667987593502,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"W0zIy4zW9IXy","outputId":"061b3f5a-40a3-43f1-bbc2-115e45088f95"},"outputs":[{"name":"stdout","output_type":"stream","text":["25277\n"]}],"source":["print(word2index[\"<SOS>\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":720,"status":"ok","timestamp":1667987594689,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"RMAGDKBDlRJW","outputId":"98aac424-dd21-440d-c6c2-da8e444108a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean length of sentences :  77.49162222222222\n"]}],"source":["# To have an idea, compute statistics on the number of tokens in each sentence\n","mean = 0\n","for row in range(len(df_train)):\n","    mean += len(df_train.loc[row, \"text\"])\n","mean = mean / len(df_train)\n","print(\"Mean length of sentences : \", mean)"]},{"cell_type":"markdown","metadata":{"id":"PdtW1GzNkTU_"},"source":["## Build Embedding\n","https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n","Embedding converts words to integers, and there is a vector corresponding to eachinteger.\n","If there x words in a dictionary, each word will be assigned a value between 1 and x.\n","\n","Embedding types : \n","- GloVe : Global Vectors for Word Representation (pre-trained)\n","- Word2Vec : \n","    - Pre-trained : https://github.com/AI-Trends/NLP-Tutorial/blob/master/Word2vec_pretrained_embeddings.ipynb\n","This first version of word2vec embedding will load the vectors of an already trained model.\n","    - Trained on the fly : https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis\n","Unlike the previous version, we will build a vocabulary and train it with the train dataset from our data.\n","- FastText : Facebook AI Research\n"]},{"cell_type":"markdown","metadata":{"id":"q4NFBDMDuOjC"},"source":["### Embedding matrix\n","When we will have a dataset, its vocabulary, and a dictionary of embedder words and their corresponding vectors.\n","There will still be no correlation between our vocabulary, and the embedder vocabulary.\n","To connect them, we need to create an embedding matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3C6w3I6uOT8"},"outputs":[],"source":["def build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding, glove, model):\n","    print(\"Building embedding matrix...\")\n","\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","    # This matrix needs to be populated : \n","    #   For each word in the training vocabulary, the corresponding vector is \n","    #   retrieved from the GloVe dictionary\n","\n","    ukn_cnt = 0     # unknown words counter\n","    ukn_set = set() # set of unknown words\n","    for index, word in enumerate(vocabulary):\n","        if word in embedding:\n","          if glove:\n","            embedding_matrix[index] = embedding[word]\n","          else:\n","            embedding_vector = model[word]\n","            embedding_vector = np.array(embedding_vector)\n","            if embedding_vector is not None:\n","              embedding_matrix[index] = embedding_vector\n","        else:\n","            ukn_cnt += 1\n","            ukn_set.add(word)\n","            embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n","    \n","    print(\"Unknown words: \", ukn_cnt)\n","    print(\"Percentage of unknown words: \", ukn_cnt / vocab_size)\n","    \n","    return embedding_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":144375,"status":"ok","timestamp":1667987748055,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"fi3Ub9VWYJrG","outputId":"27ef2f2d-f43c-4fee-9f02-40d53364623b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-trained GloVe embeddings...\n","Building embedding matrix...\n","Unknown words:  601\n","Percentage of unknown words:  0.019628335347333357\n"]}],"source":["# Choose the embedding parameters\n","EMBEDDING_TYPE = \"glove\" # \"glove\" (pre-trained), \"word2vec\" or \"fasttext\"\n","PRE_TRAINED = True # True if using pre-trained embeddings, False otherwise\n","RETRAIN_EMBEDDINGS = False # True if training the embeddings, False otherwise\n","\n","#-----------------------------------------\n","#                  GloVe \n","#-----------------------------------------\n","if EMBEDDING_TYPE == \"glove\":\n","    if PRE_TRAINED:\n","        print(\"Loading pre-trained GloVe embeddings...\")\n","        embedding_path = root + '/Glove/glove.42B.300d.txt'\n","        embedding_dim = 300 # The dimension of the embedding\n","        \n","        embedding_dict = {}\n","        with open(embedding_path, 'r') as f:\n","            for line in f:\n","                tokens = line.split()\n","                word = tokens[0]\n","                vector = np.array(tokens[1:], dtype=np.float32)\n","                if vector.shape[0] == embedding_dim:\n","                    embedding_dict[word] = vector\n","                else:\n","                    print(\"Error: embedding dimension is not \", embedding_dim)\n","        \n","        # Create the embedding matrix   \n","        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, True, embedding_dict)\n","                \n","#-----------------------------------------\n","#                  Word2Vec \n","#-----------------------------------------             \n","elif EMBEDDING_TYPE == \"word2vec\":\n","    if PRE_TRAINED:\n","        print(\"Loading pre-trained Word2Vec embeddings...\")\n","        \n","        embedding_path = root + '/Word2Vec/GoogleNews-vectors-negative300.bin'\n","        embedding_dim = 300 # The dimension of the embedding\n","        \n","        # Load the vocabulary and the corresponding vectors\n","        model_w2v = KeyedVectors.load_word2vec_format(root + '/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n","        embedding_dict = model_w2v.vocab\n","        \n","        # Create the embedding matrix\n","        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n","    \n","    else:\n","        if RETRAIN_EMBEDDINGS:\n","            print(\"Training Word2Vec embeddings...\")\n","            #-----------------------------------------\n","            #                 Parameters\n","            #-----------------------------------------\n","            \n","            embedding_dim = 300\n","            w2v_window = 5\n","            w2v_min_count = 10\n","            w2v_workers = 8\n","            train_embeddings_epochs = 30\n","            \n","            # Train the word2vec model\n","            model_w2v = gensim.models.word2vec.Word2Vec(size=embedding_dim, window=w2v_window, min_count=w2v_min_count, workers=w2v_workers)\n","            model_w2v.build_vocab(df_train.text)\n","            words = model_w2v.wv.vocab.keys()\n","            model_w2v.train(df_train.text, total_examples=len(df_train), epochs=train_embeddings_epochs)\n","            model_w2v.save(root + \"/Word2Vec/w2v.model\")\n","            embedding_dict = model_w2v.wv\n","            \n","            # Create the embedding matrix\n","            embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n","            \n","        else:\n","            print(\"Loading Word2Vec embeddings...\")\n","            # Can avoid the training by loading the saved model\n","            model_w2v = gensim.models.word2vec.Word2Vec.load(root + '/Word2Vec/w2v.model')\n","            embedding_dim = 300\n","            embedding_dict = model_w2v.wv\n","        \n","            # Create the embedding matrix\n","            embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict, False, model_w2v)\n","            \n","elif EMBEDDING_TYPE == \"fasttext\":\n","    if PRE_TRAINED:\n","        print(\"Loading pre-trained FastText embeddings...\")\n","        \n","        embedding_path = root + '/FastText/wiki-news-300d-1M.vec'\n","        embedding_dim = 300\n","        \n","        # Load the vocabulary and the corresponding vectors\n","        embedding_dict = {}\n","        with open(embedding_path, 'r') as f:\n","            for line in f:\n","                tokens = line.split()\n","                word = tokens[0]\n","                vector = np.array(tokens[1:], dtype=np.float32)\n","                if vector.shape[0] == embedding_dim:\n","                    embedding_dict[word] = vector\n","                else:\n","                    print(\"Error: embedding dimension is not \", embedding_dim)\n","                    \n","        # Create the embedding matrix\n","        embedding = build_embedding_matrix(vocab_size, embedding_dim, vocabulary, embedding_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1667987754289,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"ZGEzA_3LYJrG","outputId":"76c531b2-2e3d-42ee-94ee-93b5f52a7157"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dictionary Size :  1917494\n"]}],"source":["# Check dictionary size : \n","print(\"Dictionary Size : \", len(embedding_dict))"]},{"cell_type":"markdown","metadata":{"id":"_4Rp2m2F6-lX"},"source":["## Padding sequences\n","Some of the tweets are longer than others. \n","In order to train the model on a fixed-length input, we must define an input length that will be the same for all tokens (MAX_TOKENS), and pad all tweets that are smaller than MAX_TOKENS with \\<PAD\\> tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFS-iTNa7phK"},"outputs":[],"source":["# Decide on the maximum length of the sequences (based on the mean computed earlier)\n","seq_length = 100 # mean was around 77"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEJFmLoRq6Xb"},"outputs":[],"source":["#Returns the index if the word is in the vocabulary, otherwise, it returns the index of unkwon words\n","def get_word(w):\n","  if w in word2index:\n","    return word2index[w]\n","  else:\n","    return word2index[\"<UKN>\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmMbMzL57UNv"},"outputs":[],"source":["def padding_and_encode(tweet, max_length):\n","  sos_id = [word2index[\"<SOS>\"]]\n","  eos_id = [word2index[\"<EOS>\"]]\n","  pad_id = [word2index[\"<PAD>\"]]\n","  \n","  # Truncate the tweet if it is too long (more than seq_length)\n","  if len(tweet) > max_length:\n","    tweet = tweet[:max_length]\n","    n_pads = 0\n","  else:\n","    n_pads = max_length - len(tweet)\n","    \n","  encoded = [get_word(w) for w in tweet] # encode without embedding here!!!\n","  return sos_id + encoded + eos_id + pad_id*n_pads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxmG78TY9jqx"},"outputs":[],"source":["# Transfrom dataframes into lists\n","X_train = df_train[\"text\"].tolist()\n","X_test = df_test[\"text\"].tolist()\n","y_train = df_train[\"sentiment\"].tolist()\n","y_test = df_test[\"sentiment\"].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667987765325,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"QVh-wWVhf_Ef","outputId":"81169ed8-1f85-4a96-f284-17c986548e8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['recently', 'watching', 'show', 'say', 'really', 'made', 'laugh', 'appreciate', 'unrealistic', 'aspect', 'along', 'everything', 'else', 'people', 'said', 'realistic', 'reaction', 'dead', 'among', 'thing', 'going', 'accept', 'bring', 'back', 'life', 'completely', 'crazy', 'bit', 'help', 'smiling', 'every', 'episode', 'watched', 'think', 'great', 'take', 'strange', 'subject', 'make', 'watch', 'absolutely', 'love', 'narration', 'add', 'extra', 'wonder', 'whole', 'cannot', 'always', 'compare', 'old', 'writer', 'new', 'one', 'entity', 'definitely', 'give', 'chance', 'enjoy', 'ridiculous', 'part'] positive\n","['return', 'often', 'wrong', 'rather', 'right', 'shame', 'last', 'chronological', 'installment', 'star', 'war', 'saga', 'shining', 'example', 'epic', 'storytelling', 'wrap', 'story', 'line', 'previous', 'movie', 'one', 'grand', 'finale', 'yes', 'cute', 'cuddly', 'bear', 'broaden', 'demographic', 'middle', 'slow', 'bit', 'final', 'hour', 'best', 'piece', 'entire', 'luke', 'finally', 'come', 'face', 'recognizable', 'villain', 'many', 'thing', 'people', 'tend', 'overlook', 'incredible', 'conclusion', 'went', 'slightly', 'implausible', 'empire', 'strike', 'back', 'convincing', 'exciting', 'opening', 'palace', 'masterful', 'performance', 'emperor', 'coming', 'resolution', 'solo', 'romance', 'extremely', 'powerful', 'moment', 'slight', 'annoyance', 'generation', 'time', 'every', 'single', 'scene', 'still', 'magical', 'moving', 'cinema', 'also', 'serf', 'great', 'chapter', 'good', 'fantastic'] positive\n","['remember', 'movie', 'came', 'year', 'old', 'commodore', 'play', 'therefore', 'really', 'got', 'buy', 'cheap', 'put', 'man', 'bad', 'sylvester', 'say', 'like', 'word', 'entire', 'except', 'awful', 'sentimental', 'speech', 'end', 'expression', 'face', 'way', 'stupid', 'love', 'thing', 'middle', 'amazingly', 'predictable', 'ended', 'fast', 'forwarding', 'went', 'exchange', 'something', 'else'] negative\n","['know', 'last', 'reviewer', 'talking', 'show', 'pure', 'entertainment', 'basically', 'dude', 'put', 'competition', 'club', 'pick', 'girl', 'different', 'scenario', 'mix', 'every', 'time', 'panel', 'judge', 'afraid', 'call', 'people', 'admit', 'recognize', 'game', 'break', 'guy', 'wrong', 'right', 'contestant', 'weak', 'strong', 'always', 'entertaining', 'relate', 'seen', 'real', 'doubt'] positive\n","['beginning', 'excited', 'see', 'movie', 'poster', 'possibly', 'ever', 'seen', 'immediately', 'bought', 'one', 'dorm', 'every', 'element', 'came', 'together', 'beautifully', 'often', 'many', 'penis', 'gay', 'racial', 'joke', 'critic', 'rest', 'cast', 'deliver', 'sensationally', 'remains', 'sweet', 'throughout', 'entire', 'end', 'succeed', 'relationship', 'get', 'laid', 'supporting', 'brutal', 'problem', 'lady', 'thing', 'abundance', 'memorable', 'scene', 'given', 'make', 'easy', 'remember', 'fondly', 'brought', 'word', 'chest', 'waxing', 'watching', 'theater', 'older', 'people', 'watch', 'saw', 'group', 'four', 'mids', 'woman', 'come', 'despite', 'audience', 'still', 'filled', 'think', 'type', 'like', 'fan', 'office', 'lot', 'remind', 'family', 'guy', 'shallow', 'enough', 'adolescent', 'boy', 'clever', 'middle', 'aged', 'recommend', 'going', 'profanity', 'easily', 'offended', 'however', 'good', 'humor', 'funny'] positive\n","['film', 'moving', 'without', 'sentimental', 'meaningful', 'pretentious', 'tell', 'simple', 'story', 'family', 'danger', 'falling', 'apart', 'encroachment', 'technology', 'advancing', 'society', 'make', 'business', 'increasingly', 'acting', 'wonderful', 'though', 'none', 'west', 'likely', 'actor', 'long', 'ago', 'play', 'character', 'honesty', 'reverence', 'flawed', 'major', 'weakness', 'utter', 'humanity', 'kindness', 'impossible', 'become', 'engaged', 'need', 'like', 'western'] positive\n","['movie', 'think', 'like', 'amazing', 'intelligent', 'people', 'talented', 'even', 'fall', 'really', 'done', 'one', 'recent', 'year', 'come', 'back', 'wilderness', 'made', 'mark', 'truly', 'great', 'film', 'budget', 'cute', 'actress', 'intense', 'thought', 'would', 'excel', 'sentimental', 'emotional', 'role', 'ironically', 'care', 'sheer', 'incompetence', 'carelessness', 'awful', 'acting', 'banal', 'background', 'music', 'insensitive', 'direction', 'make', 'real', 'blunder', 'rule', 'german', 'police', 'issue', 'public', 'instruction', 'loud', 'speaker', 'communicate', 'among', 'unfortunately', 'become', 'standard', 'main', 'line', 'excellent', 'director', 'many', 'bad', 'worse', 'thing', 'majority', 'song', 'saving', 'please', 'repeat', 'find', 'good', 'low', 'villain', 'shaved', 'head', 'instead', 'wig', 'know'] negative\n","['really', 'say', 'felt', 'movie', 'right', 'essence', 'mind', 'game', 'dreamy', 'reality', 'enter', 'aspect', 'future', 'faced', 'cruise', 'much', 'acting', 'prowess', 'must', 'impress', 'every', 'point', 'simply', 'due', 'engaging', 'also', 'self', 'lead', 'merge', 'speak', 'beautiful', 'however', 'come', 'average', 'flick', 'weekend', 'pick', 'random', 'watch', 'gleefully', 'carry', 'strong', 'sentiment', 'character', 'wash', 'one', 'beer', 'popcorn', 'certainly', 'need'] positive\n","['one', 'word', 'young', 'demi', 'look', 'good', 'pregnant', 'point', 'movie', 'scary', 'first', 'scene', 'little', 'could', 'render', 'better', 'cloud', 'effect', 'get', 'old', 'well', 'worth', 'something', 'instead', 'like', 'exorcist', 'next', 'drama', 'part', 'beginning', 'simply'] negative\n","['well', 'done', 'spooky', 'horror', 'movie', 'film', 'company', 'usually', 'put', 'really', 'cheesy', 'like', 'devil', 'bat', 'flying', 'serpent', 'german', 'expatriate', 'director', 'wonder', 'small', 'budget', 'swamp', 'set', 'gaunt', 'ghoulish', 'effective', 'strangler'] positive\n"]}],"source":["# Print the 10 first elements of the training set\n","for i in range(10):\n","  print(X_train[i], y_train[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aXbt9gr-D-1"},"outputs":[],"source":["# Encode \n","X_train = [padding_and_encode(tweet, seq_length-2) for tweet in X_train] # -2 because of SOS and EOS\n","X_test = [padding_and_encode(tweet, seq_length-2) for tweet in X_test] # -2 because of SOS and EOS\n","# Make sure that the values in y_train are either 0 (0) or 1 (instead of 4)\n","y_train = [0 if y == 'negative' else 1 for y in y_train]\n","y_test = [0 if y == 'negative' else 1 for y in y_test]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1667987770095,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"OMaiJunpkjTA","outputId":"137f5f36-2bcc-449e-e3c4-20df3f4c4a75"},"outputs":[{"name":"stdout","output_type":"stream","text":["[25277, 11315, 25488, 4949, 9405, 25857, 23652, 24963, 22946, 16510, 21349, 16584, 3592, 18938, 22924, 24544, 28420, 10945, 18319, 16541, 25888, 8099, 20347, 17490, 15425, 23192, 24353, 25047, 26735, 1119, 11480, 2882, 13366, 15397, 24062, 16142, 7587, 1124, 15857, 18189, 3865, 17271, 6827, 6105, 2207, 580, 15105, 20325, 1435, 14988, 3242, 16144, 17508, 12839, 6466, 20883, 30230, 9523, 5686, 7636, 21688, 5990, 4656, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012] 1\n","[25277, 1154, 18748, 24844, 12269, 28771, 6024, 30292, 5416, 24143, 17534, 8041, 12417, 8825, 8155, 24126, 25403, 25556, 10908, 23111, 22173, 29443, 6466, 4105, 25401, 24460, 14132, 14861, 27941, 30336, 2155, 19289, 23573, 26735, 341, 3489, 28716, 21628, 14190, 29978, 5813, 2058, 29125, 12635, 11912, 6503, 25888, 22924, 25286, 18687, 9258, 7401, 28866, 7113, 10129, 11550, 14942, 15425, 9496, 28277, 6812, 14002, 16259, 27940, 6185, 6944, 1161, 28619, 4412, 29642, 29972, 4889, 16100, 9484, 26703, 23503, 2882, 11902, 3309, 27859, 5768, 29066, 26910, 23849, 6905, 16142, 10598, 10694, 22388, 4656, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012] 1\n","[25277, 24057, 29443, 9451, 2832, 16144, 27639, 26573, 5297, 25857, 26057, 27644, 24734, 15479, 1609, 26599, 30105, 9405, 28066, 12878, 14190, 977, 1783, 12235, 18899, 18680, 23664, 29125, 18023, 22281, 6827, 25888, 19289, 16870, 24198, 336, 4418, 1042, 28866, 26736, 26818, 18938, 4656, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012, 13012] 0\n"]}],"source":["for i, j in zip(X_train[:3], y_train[:3]):\n","  print(i, j)"]},{"cell_type":"markdown","metadata":{"id":"R2gwgoMKuYMb"},"source":["Since we only have a training set and test set, we split the training set into training and validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1667987772594,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"mkFvR1cYudfO","outputId":"5816b7d2-ecad-46c5-c841-8151ed198fb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training examples :  36000\n","Number of validation examples :  9000\n"]}],"source":["X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n","                                                  random_state = 42, \n","                                                  shuffle = True, \n","                                                  test_size = 0.2)\n","\n","# Now truncate the dataset because there is too much data\n","keep_ratio = 1\n","X_train, y_train = X_train[:int(len(X_train)*keep_ratio)], y_train[:int(len(y_train)*keep_ratio)]\n","X_val, y_val = X_val[:int(len(X_val)*keep_ratio)], y_val[:int(len(y_val)*keep_ratio)]\n","\n","print(\"Number of training examples : \", len(X_train))\n","print(\"Number of validation examples : \", len(X_val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1667987774317,"user":{"displayName":"Lucie Navez","userId":"17163118001494774550"},"user_tz":-60},"id":"gj-wn4z-xQLt","outputId":"7d86965a-0507-4ab1-9d6d-600ca68dd9ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["[25277, 29182, 15915, 1089, 19633, 5639, 1831, 24779, 2882, 14502, 15278, 436, 25103, 6812, 28475, 13378, 8358, 4748, 20185, 24284, 13957, 6150, 29443, 24601, 22639, 29271, 10419, 21055, 10940, 9466, 30310, 23072, 14434, 14915, 22025, 7852, 20568, 13693, 30438, 1079, 383, 3290, 16620, 9006, 9467, 9434, 18966, 8844, 7085, 20533, 12839, 29866, 11031, 8443, 14586, 7228, 27104, 10908, 3396, 14652, 11098, 15243, 14988, 5028, 12873, 9510, 21396, 28716, 23111, 11566, 27940, 4588, 3545, 20325, 22887, 11074, 10694, 1435, 16858, 7752, 678, 22438, 7253, 12824, 7570, 29423, 7444, 4613, 18592, 21760, 9523, 12539, 15028, 14838, 20588, 20646, 13292, 30041, 1034, 4656] 1\n","[25277, 6503, 19551, 321, 25837, 23225, 24796, 4949, 6466, 28716, 22, 25857, 22946, 546, 9414, 25941, 23503, 14989, 21935, 7492, 27590, 8525, 21, 5437, 17155, 16144, 11773, 7236, 9531, 8099, 2174, 22670, 3326, 29336, 16182, 20381, 28833, 23334, 127, 3405, 5250, 29125, 24040, 11254, 610, 12097, 23084, 10874, 29388, 18033, 16962, 23192, 14086, 9734, 6827, 10753, 9272, 24316, 30034, 14927, 29529, 3592, 20500, 23924, 18939, 8705, 7249, 21614, 18281, 8232, 3309, 7741, 21546, 24595, 12206, 29903, 7095, 30599, 14586, 26792, 12757, 16142, 4069, 8722, 679, 28211, 29230, 21688, 5766, 29066, 20345, 7021, 10265, 26852, 24057, 25328, 10861, 15295, 14865, 4656] 1\n","[25277, 8588, 18175, 546, 127, 25187, 28900, 9734, 19307, 10694, 20038, 321, 6466, 19295, 27456, 16851, 11460, 3314, 7899, 7158, 18398, 9805, 2524, 10455, 5305, 11979, 3981, 14269, 3519, 25597, 28066, 23966, 12597, 12873, 8956, 27825, 20580, 26599, 25107, 23967, 5468, 27827, 27734, 28383, 14313, 27525, 26664, 25298, 27859, 23579, 20723, 7587, 22058, 10986, 17967, 27050, 17167, 21514, 16142, 14988, 13995, 26936, 26057, 16255, 1800, 109, 4000, 11228, 18319, 2690, 1538, 5219, 20073, 23503, 8525, 4949, 18423, 29041, 6465, 10803, 8197, 29795, 19929, 15838, 29443, 19892, 17197, 3399, 11288, 21410, 19556, 18513, 20370, 17029, 1521, 21559, 18979, 12685, 1697, 4656] 1\n"]}],"source":["for i, j in zip(X_train[:3], y_train[:3]):\n","  print(i, j)"]},{"cell_type":"markdown","metadata":{"id":"PvoZBxauA2ln"},"source":["# DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Za-9ayS9A6Zw"},"outputs":[],"source":["batch_size = 10\n","\n","X_train = np.array(X_train)\n","X_test = np.array(X_test)\n","X_val = np.array(X_val)\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","y_val = np.array(y_val)\n","\n","train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","\n","\n","# DataLoaders\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n","test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=len(X_test), drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"xdO-D4Z_dCC2"},"source":["# Document embedding\n","The document embedding will work in a different way from the word embedding. Instead of uwing an embedding layer into our RNN, we will directly transform the document into its embedded vector and provide that vector as an input to the RNN. This will thus require to use a RNN a bit modified from the previous one."]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4oG3wsiTdZv6","outputId":"aace584c-2587-421f-a3df-0d1c97be206a","executionInfo":{"status":"ok","timestamp":1668073499379,"user_tz":-60,"elapsed":741548,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading doc2vec...\n"]}],"source":["document_model = \"doc2vec\"\n","#document_model = \"word2vec-tfidf\"\n","\n","embedding_dim = 300\n","\n","new_df_train = np.zeros((len(df_train), embedding_dim))\n","new_df_test = np.zeros((len(df_test), embedding_dim))\n","i = 0\n","\n","if document_model == \"doc2vec\":\n","\n","  print(\"Loading doc2vec...\")\n","\n","  d2v_model = Doc2Vec(vector_size=embedding_dim, min_count=2, epochs=30)\n","\n","  sentences = [TaggedDocument(sentence, [i]) for i,sentence in enumerate(df_train.text)]\n","\n","  d2v_model.build_vocab(sentences)\n","  d2v_model.wv.vocab\n","  d2v_model.train(sentences, total_examples=len(sentences), epochs=30)\n","\n","  for sentence in df_train.text:\n","    new_df_train[i] = d2v_model.infer_vector(sentence)\n","    i += 1\n","\n","  i = 0\n","  for sentence in df_test.text:\n","    new_df_test[i] = d2v_model.infer_vector(sentence)\n","    i += 1\n","\n","elif document_model == \"word2vec-tfidf\":\n","\n","  print(\"Loading word2vec averaged with tf-idf...\")\n","\n","  # Loading word2vec\n","  embedding_path = root + '/Word2Vec/GoogleNews-vectors-negative300.bin'\n","        \n","  # Load the vocabulary and the corresponding vectors\n","  model_w2v = KeyedVectors.load_word2vec_format(root + '/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)\n","\n","  dictionary = Dictionary()\n","  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in df_train.text]\n","\n","  tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n","\n","  unknown = 0\n","  num_words = 0\n","  #i = 0\n","\n","  for doc in tfidf[bow_corpus]:\n","    if i % 5000 == 0:\n","      print(i, \"/\", len(df_train))\n","\n","    total_freq = 0 #used to normalize the weigths\n","\n","    for id, freq in doc:\n","      total_freq += freq\n","\n","    #Bulding the vector for each document\n","    for id, freq in doc: \n","      num_words += 1\n","      word = dictionary[id]\n","      #print(word)\n","      if word in model_w2v.vocab:\n","        embedding_vector = model_w2v[word]\n","        embedding_vector = np.array(embedding_vector)\n","        new_df_train[i] += (freq/total_freq)*embedding_vector\n","      else:\n","        unknown += 1\n","\n","    i += 1\n","\n","  print(unknown/num_words)\n","\n","  # Now we do the same but for df_test\n","  unknown = 0\n","  num_words = 0\n","  i = 0\n","  dictionary = Dictionary()\n","  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in df_test.text]\n","\n","  tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n","\n","  for doc in tfidf[bow_corpus]:\n","    if i % 500 == 0:\n","      print(i, \"/\", len(df_test))\n","\n","    total_freq = 0 #used to normalize the weigths\n","\n","    for id, freq in doc:\n","      total_freq += freq\n","\n","    #Bulding the vector for each document\n","    for id, freq in doc: \n","      num_words += 1\n","      word = dictionary[id]\n","      #print(word)\n","      if word in model_w2v.vocab:\n","        embedding_vector = model_w2v[word]\n","        embedding_vector = np.array(embedding_vector)\n","        new_df_test[i] += (freq/total_freq)*embedding_vector\n","      else:\n","        unknown += 1\n","\n","    i += 1\n","  print(unknown/num_words)\n","\n","  "]},{"cell_type":"code","execution_count":47,"metadata":{"id":"WUKRH6dXdeiM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668073799440,"user_tz":-60,"elapsed":1058,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}},"outputId":"2a917896-bc6c-45f0-d97c-0bc1224acd56"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5000, 300)\n","(45000, 300)\n"]}],"source":["print(new_df_test.shape)\n","print(new_df_train.shape)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"_GVkrMrHdf1E","executionInfo":{"status":"ok","timestamp":1668073801542,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["y_train = df_train[\"sentiment\"].tolist()\n","y_test = df_test[\"sentiment\"].tolist()\n","y_train = [0 if y == 'negative' else 1 for y in y_train]\n","y_test = [0 if y == 'negative' else 1 for y in y_test]"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"80cW_V7Cdimz","executionInfo":{"status":"ok","timestamp":1668073803790,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["X_train, X_val, y_train, y_val = train_test_split(new_df_train, y_train, \n","                                                  random_state = 42, \n","                                                  shuffle = True, \n","                                                  test_size = 0.2)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1668073806036,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"IUmmB3jrdkQk","outputId":"1dc11ecf-cace-460c-b110-33755df5d591"},"outputs":[{"output_type":"stream","name":"stdout","text":["(300,)\n"]}],"source":["print(X_train[0].shape)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1668073807340,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"dtNO-s_idk0X","outputId":"07bafd4f-3a34-480e-c5c0-d3780ae04e2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(36000, 1, 300)\n"]}],"source":["batch_size = 10\n","\n","X_train = [[doc] for doc in X_train]\n","X_train = np.array(X_train)\n","print(X_train.shape)\n","X_test = [[doc] for doc in new_df_test]\n","X_test = np.array(X_test)\n","X_val = [[doc] for doc in X_val]\n","X_val = np.array(X_val)\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","y_val = np.array(y_val)\n","\n","train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n","val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val))\n","test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n","\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size, drop_last=True)\n","test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"CbTtyocUWuoC"},"source":["# Model architecture\n","To train the model, we will use the following architectures :\n","- RNN : \n","  - Embedding layer (pre-trained GloVe, Word2Vec, FastText)\n","    https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n","    The weights of the embedding_matrix will be loaded into that layer.\n","  - LSTM layer or GRU layer\n","  - Linear layer\n","\n","Or we can use the following architecture :\n","- Transformer architecture (self attention)\n","\n","Or the following architecture : \n","- RNN + Attention\n","  - Embedding layer (pre-trained GloVe, Word2Vec, FastText)\n","  - LSTM layer or GRU layer\n","  - Attention layer\n","  - Linear layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Reyt1GwSDRRr"},"outputs":[],"source":["def CreateEmbeddingLayer(embedding_matrix, padding_id, non_trainable=False):\n","  # Retrieve dimensions of embedding\n","  num_embeddings, embedding_dim = embedding_matrix.shape\n","\n","  # Initialize nn.Embedding with the pre-trained weights\n","  embedding_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_id)\n","  embedding_layer.load_state_dict({'weight': torch.from_numpy(embedding_matrix)})\n","\n","  if non_trainable:\n","    embedding_layer.weight.requires_grad = False\n","\n","  return embedding_layer, num_embeddings, embedding_dim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sdq7L4R6sJ11"},"outputs":[],"source":["def positional_encoding(seq_length, embedding_dim):\n","  pos_enc = np.array([\n","      [pos / np.power(10000, 2 * (j // 2) / embedding_dim) for j in range(embedding_dim)]\n","      if pos != 0 else np.zeros(embedding_dim) for pos in range(seq_length)\n","  ])\n","  pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n","  pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n","  \n","  return torch.from_numpy(pos_enc).type(torch.FloatTensor).to(device)"]},{"cell_type":"markdown","metadata":{"id":"SMmiBdkAYJrI"},"source":["## RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOrXOnN1AyJs"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_dim, dropout, bidirectional, padding_id, num_layers, type):\n","      super().__init__()\n","\n","      # Embedding layer\n","      self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n","\n","      # LSTM or GRU\n","      if type==\"LSTM\":\n","        self.model = nn.LSTM(embedding_dim, \n","                            hidden_dim, \n","                            batch_first = True, \n","                            dropout = dropout,\n","                            bidirectional = bidirectional)\n","      elif type==\"GRU\":\n","        self.model = nn.GRU(embedding_dim,\n","                            hidden_dim,\n","                            num_layers = num_layers,\n","                            dropout = dropout,\n","                            bidirectional = bidirectional)\n","\n","      # Fully-connected layer\n","      self.fc = nn.Linear(hidden_dim * 2, 2)\n","      # The output of the fully connected layer is a vector of size 2 containing\n","      # the probability of the tweet to belong to either positive or negative\n","      # class (--> 2 classes)\n","\n","      # Dropout\n","      self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input):\n","\n","      # Embed the input\n","      embs = self.dropout(self.embedding(input))\n","\n","      # pack sequence\n","      # lengths need to be on CPU!\n","      # This will cause the RNN to only process non-padded elements\n","      #packed_embedded = nn.utils.rnn.pack_padded_sequence(embs, input_length.to('cpu'))\n","\n","      # Feed embeddings to LSTM\n","      packed_output, (hidden, cell) = self.model(embs)\n","\n","      # Apply droupout\n","      # Dropout is applied to the concatenation of the final forward and backward hidden layers\n","      # (Because of bidirectionality)\n","      out = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","\n","      # Through fully connected layer\n","      out = self.fc(out)\n","\n","      return out\n","    \n","    def init_hidden(self):\n","      \"\"\"At the beginning of the sequence, there are no hidden state.\n","          Thus, we need to initialise the hidden state vector.\"\"\"\n","\n","      return (torch.zeros(2, batch_size, 32), torch.zeros(2, batch_size, 32))\n"]},{"cell_type":"markdown","metadata":{"id":"zlqu-0LAYJrI"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld1FDyXSsJ11"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","  def __init__(self, embedding_dim):\n","    super(SelfAttention, self).__init__()\n","    self.embedding_dim = embedding_dim\n","    self.query = nn.Linear(embedding_dim, embedding_dim)\n","    self.key = nn.Linear(embedding_dim, embedding_dim)\n","    self.value = nn.Linear(embedding_dim, embedding_dim)\n","    self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n","    self.softmax = nn.Softmax(dim=-1)\n","    self.dropout = nn.Dropout(0.1)\n","    self.out = nn.Linear(embedding_dim, embedding_dim)\n","  \n","  def forward(self, x):\n","    batch_size = x.shape[0]\n","    # x = [batch size, seq len, embedding dim]\n","    Q = self.query(x)\n","    K = self.key(x)\n","    V = self.value(x)\n","    # Q, K, V = [batch size, seq len, embedding dim]\n","    energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale\n","    # energy = [batch size, seq len, seq len]\n","    attention = self.softmax(energy)\n","    # attention = [batch size, seq len, seq len]\n","    x = torch.matmul(self.dropout(attention), V)\n","    # x = [batch size, seq len, embedding dim]\n","    x = self.out(x)\n","    # x = [batch size, seq len, embedding dim]\n","    return x, attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3V5rl-WsJ11"},"outputs":[],"source":["# Attention layer\n","class TransformerAttention(nn.Module):\n","  def __init__(self, embedding_matrix, heads, padding_id, max_sequence_length, dropout=0.1):\n","    super(TransformerAttention, self).__init__()\n","    \n","    # Embedding layer\n","    self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n","    self.embedding_dim = embedding_dim\n","    self.heads = heads\n","    self.head_dim = embedding_dim // heads\n","    self.max_sequence_length = max_sequence_length\n","    \n","    assert (self.head_dim * heads == embedding_dim), \"Embedding dimension should be divisible by heads\"\n","\n","    self.attention = SelfAttention(self.embedding_dim)\n","    self.dropout = nn.Dropout(dropout)\n","    # Linear layer of size d_model * d_model\n","    self.fc = nn.Linear(self.embedding_dim, self.embedding_dim)\n","    # Linear project layer of size d_model * d_classes\n","    self.fc_out = nn.Linear(embedding_dim, 2)\n","\n","  def forward(self, inputs):\n","    # Fetch embedding from input\n","    embs = self.embedding(inputs)\n","    \n","    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n","    # Add position encoding to the embeddings\n","    embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n","    \n","    # Apply dropout\n","    embs = self.dropout(embs) # (batch_size, seq_length, embedding_dim)\n","    \n","    # Apply self attention once \n","    new_embs, attention = self.attention(embs)\n","      \n","    # Average the attention embeddings\n","    avg = torch.mean(new_embs, dim=1)\n","    \n","    # Feed the average attention heads through a fully connected layer\n","    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n","    \n","    # Linear projection of size d_model * nb_classes\n","    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n","    \n","    # Softmax \n","    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n","  \n","    return out"]},{"cell_type":"markdown","metadata":{"id":"mD7uTJeNlRJb"},"source":["## RNN + Attention"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"nhZoTC1OlRJb","executionInfo":{"status":"ok","timestamp":1668072187519,"user_tz":-60,"elapsed":332,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["#performer_pytorch FastAttention\n","class SelfAttention(nn.Module):\n","  def __init__(self, embedding_dim):\n","    super(SelfAttention, self).__init__()\n","    self.embedding_dim = embedding_dim\n","    self.query = nn.Linear(embedding_dim, embedding_dim)\n","    self.key = nn.Linear(embedding_dim, embedding_dim)\n","    self.value = nn.Linear(embedding_dim, embedding_dim)\n","    self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n","    self.softmax = nn.Softmax(dim=-1)\n","    self.dropout = nn.Dropout(0.1)\n","    self.out = nn.Linear(embedding_dim, embedding_dim)\n","  \n","  def forward(self, x):\n","    batch_size = x.shape[0]\n","    # x = [batch size, seq len, embedding dim]\n","    Q = self.query(x)\n","    K = self.key(x)\n","    V = self.value(x)\n","    # Q, K, V = [batch size, seq len, embedding dim]\n","    energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale\n","    # energy = [batch size, seq len, seq len]\n","    attention = self.softmax(energy)\n","    # attention = [batch size, seq len, seq len]\n","    x = torch.matmul(self.dropout(attention), V)\n","    # x = [batch size, seq len, embedding dim]\n","    x = self.out(x)\n","    # x = [batch size, seq len, embedding dim]\n","    return x, attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49QVzDe3lRJb"},"outputs":[],"source":["class AttentiveRNN(nn.Module):\n","  def __init__(self, embedding_matrix, hidden_size, bidirectional, padding_id, max_sequence_length, dropout=0.1, type='LSTM'):\n","    super(AttentiveRNN, self).__init__()\n","    \n","    # Embedding layer\n","    self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n","    self.embedding_dim = embedding_dim\n","    self.hidden_size = hidden_size\n","    self.max_sequence_length = max_sequence_length\n","    \n","    # Dropout layer\n","    self.dropout = nn.Dropout(dropout)\n","    \n","    # LSTM layer or GRU layer\n","    if type == \"LSTM\":\n","      self.rnn = nn.LSTM(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n","    else:\n","      self.rnn = nn.GRU(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n","    \n","    # Attention layer\n","    self.attention = SelfAttention(hidden_size * 2)\n","    \n","    # Linear layer of size d_model * d_model\n","    self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n","    # Linear project layer of size d_model * d_classes\n","    self.fc_out = nn.Linear(hidden_size * 2, 2)\n","\n","  def forward(self, inputs):\n","    # Fetch embedding from input\n","    embs = self.embedding(inputs)\n","    \n","    # Embeddings are fed in as (batch_size, seq_length, embedding_dim)\n","    # Add position encoding to the embeddings\n","    # embs = embs + positional_encoding(self.max_sequence_length, self.embedding_dim)\n","    \n","    # Apply dropout\n","    embs = self.dropout(embs) # (batch_size, seq_length, embedding_dim)\n","    \n","    # Apply LSTM\n","    lstm_out, (hidden, cell) = self.rnn(embs)\n","    \n","    # Apply self attention once \n","    new_embs, attention = self.attention(lstm_out)\n","      \n","    # Average the attention embeddings\n","    avg = torch.mean(new_embs, dim=1)\n","    \n","    # Feed the average attention heads through a fully connected layer\n","    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n","    \n","    # Linear projection of size d_model * nb_classes\n","    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n","    \n","    # Softmax \n","    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n","  \n","    return out"]},{"cell_type":"markdown","metadata":{"id":"-TLQZrxAenlu"},"source":["# RNN for document embedding"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"QSKWsBuuioBK","executionInfo":{"status":"ok","timestamp":1668070396113,"user_tz":-60,"elapsed":322,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["class RNN_Doc_Emb(nn.Module):\n","    def __init__(self, hidden_dim, dropout, bidirectional, num_layers, type):\n","      super().__init__()\n","\n","      # Embedding layer\n","      #self.embedding, num_embeddings, embedding_dim = CreateEmbeddingLayer(embedding_matrix, padding_id, True)\n","\n","      # LSTM or GRU\n","      if type==\"LSTM\":\n","        self.model = nn.LSTM(embedding_dim, \n","                            hidden_dim, \n","                            batch_first = True, \n","                            dropout = dropout,\n","                            bidirectional = bidirectional)\n","      elif type==\"GRU\":\n","        self.model = nn.GRU(embedding_dim,\n","                            hidden_dim,\n","                            num_layers = num_layers,\n","                            dropout = dropout,\n","                            bidirectional = bidirectional)\n","\n","      # Fully-connected layer\n","      self.fc = nn.Linear(hidden_dim * 2, 2)\n","      # The output of the fully connected layer is a vector of size 2 containing\n","      # the probability of the tweet to belong to either positive or negative\n","      # class (--> 2 classes)\n","\n","      # Dropout\n","      self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input):\n","\n","      # Embed the input\n","      embs = self.dropout(input)\n","\n","      # pack sequence\n","      # lengths need to be on CPU!\n","      # This will cause the RNN to only process non-padded elements\n","      #packed_embedded = nn.utils.rnn.pack_padded_sequence(embs, input_length.to('cpu'))\n","\n","      # Feed embeddings to LSTM\n","      packed_output, (hidden, cell) = self.model(input)\n","\n","      # Apply droupout\n","      # Dropout is applied to the concatenation of the final forward and backward hidden layers\n","      # (Because of bidirectionality)\n","      out = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","\n","      # Through fully connected layer\n","      out = self.fc(out)\n","\n","      return out\n","    \n","    def init_hidden(self):\n","      \"\"\"At the beginning of the sequence, there are no hidden state.\n","          Thus, we need to initialise the hidden state vector.\"\"\"\n","\n","      return (torch.zeros(2, batch_size, 32), torch.zeros(2, batch_size, 32))"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"bHNDFvxDeqGk","executionInfo":{"status":"ok","timestamp":1668072193979,"user_tz":-60,"elapsed":319,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["class AttentiveRNN_Doc_emb(nn.Module):\n","  def __init__(self, hidden_size, bidirectional, dropout=0.1, type='LSTM'):\n","    super(AttentiveRNN_Doc_emb, self).__init__()\n","    \n","    # Embedding layer\n","    self.hidden_size = hidden_size\n","    \n","    # Dropout layer\n","    self.dropout = nn.Dropout(dropout)\n","    \n","    # LSTM layer or GRU layer\n","    if type == \"LSTM\":\n","      self.rnn = nn.LSTM(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n","    else:\n","      self.rnn = nn.GRU(embedding_dim, hidden_size, bidirectional=bidirectional, batch_first=True)\n","    \n","    # Attention layer\n","    self.attention = SelfAttention(hidden_size * 2)\n","    \n","    # Linear layer of size d_model * d_model\n","    self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n","    # Linear project layer of size d_model * d_classes\n","    self.fc_out = nn.Linear(hidden_size * 2, 2)\n","\n","  def forward(self, inputs):\n","    \n","    # Apply dropout\n","    embs = self.dropout(inputs) # (batch_size, seq_length, embedding_dim)\n","    \n","    # Apply LSTM\n","    lstm_out, (hidden, cell) = self.rnn(embs)\n","    \n","    # Apply self attention once \n","    new_embs, attention = self.attention(lstm_out)\n","      \n","    # Average the attention embeddings\n","    avg = torch.mean(new_embs, dim=1)\n","    \n","    # Feed the average attention heads through a fully connected layer\n","    sentence_rep = self.fc(avg) # (batch_size, seq_length, seq_length)\n","    \n","    # Linear projection of size d_model * nb_classes\n","    classes = self.fc_out(sentence_rep) # classes = [batch size, nb_classes]\n","    \n","    # Softmax \n","    out = F.softmax(classes, dim = 1) # out = [batch size, nb_classes]\n","  \n","    return out"]},{"cell_type":"markdown","metadata":{"id":"jphW0ft_GJVG"},"source":["# Training utils"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"p-pCg_J-0QKK","executionInfo":{"status":"ok","timestamp":1668070401304,"user_tz":-60,"elapsed":286,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["\"\"\"The learnable parametrs of a model are contained in the model's parameters.\n","A state_dict is dictionary that maps each layer to its parameter tensor.\n","The state_dict thus contains all the useful information about the model and must be saved after training. (To maybe be re-loaded later on)\"\"\"\n","def saveModel(model, optimizer):\n","  # Save model\n","  now = time.localtime(time.time())\n","  now_str = time.strftime(\"%a %b %d\", now)\n","\n","  checkpoint = {\n","      'state_dict': model.state_dict(),\n","      'optimizer': optimizer.state_dict()\n","  }\n","\n","  save_path = root + \"/Models/\" + now_str + \".pth\"\n","  torch.save(checkpoint, save_path)\n","  \n","  \n","def progressBar(loss_training, loss_validation, estimated_time, percent, width = 40):\n","\n","    # Setting up the useful information\n","    left  = width * percent // 100\n","    right = width - left\n","    tags = \"#\" * int(left)\n","    spaces = \" \" * int(right)\n","    percents = f\"{percent:.2f} %\"\n","    loss_training = f\"{loss_training * 1:.6f}\"\n","    loss_validation = f\"{loss_validation * 1:.6f}\"\n","    estimated_time = f\"{estimated_time:.2f}\"\n","\n","    # Displaying a really cool progress bar !\n","    print(\"\\r[\", tags, spaces, \"] - \", percents, \" | Loss (Training) = \", loss_training, \" | Loss (Validation) = \", loss_validation,  \" | Time left : \", estimated_time ,sep=\"\", end=\"\", flush = True)\n","    \n","def binary_accuracy(preds, y):\n","  \"\"\"\n","  Returns the accuracy per batch\n","  \"\"\"\n","\n","  prob_preds = torch.nn.Softmax(dim=-1)(preds)\n","  #print(preds)\n","  #rounded_preds = torch.round(torch.sigmoid(preds))\n","  final_preds = torch.argmax(prob_preds, dim=1)\n","  correct = (final_preds == y).float()\n","  acc = correct.sum() / len(correct)\n","  return acc\n"]},{"cell_type":"markdown","metadata":{"id":"2ITzWlD0YJrJ"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"wja-QRV0HIQj"},"source":["## Training Parameters"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":857,"status":"ok","timestamp":1668073827684,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"jRiSJulQjjum"},"outputs":[],"source":["# Global parameters\n","MODEL_NAME = \"DocumentEmbs\" # Can be \"RNN\", or \"Transformer\" or \"AttentiveRNN\" or \"DocumentEmbs\"\n","RNN_TYPE = \"LSTM\" # Can be \"LSTM\" or \"GRU\"\n","DROPOUT = 0.2\n","NB_EPOCHS = 20\n","#PADDING_ID = word2index[\"<PAD>\"]\n","# ATTENTION :\n","# Learning the embedding of <PAD> tokens is irrelevant to determine the sentiment of a tweet!\n","# The embedding for this token must stay what it was initialized to. To do that, we need to tell the newtork that # this token must not be learnt. \n","\n","# Automatic save \n","checkpoints = [2, 5, 10, 15, 20]\n","\n","#--------------------------------------#\n","# RNN model parameters\n","HIDDEN_DIM = 32 #tested 256 but barely changed the results\n","BIDIRECTIONAL = True\n","NUM_LAYERS = 2 # For GRUs\n","\n","#--------------------------------------#\n","# Transformer model parameters\n","HEADS = 1\n","\n","\n","#--------------------------------------#\n","#                  MODEL               #\n","#--------------------------------------#\n","if MODEL_NAME == \"RNN\":\n","  model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, RNN_TYPE)\n","elif MODEL_NAME == \"Transformer\":\n","  model = TransformerAttention(embedding, HEADS, PADDING_ID, seq_length, DROPOUT)\n","elif MODEL_NAME == \"AttentiveRNN\":\n","  model = AttentiveRNN(embedding, HIDDEN_DIM, BIDIRECTIONAL, PADDING_ID, seq_length, dropout=DROPOUT, type=RNN_TYPE)\n","elif MODEL_NAME == \"DocumentEmbs\":\n","  model = RNN_Doc_Emb(HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, NUM_LAYERS, RNN_TYPE)\n","  #model = AttentiveRNN_Doc_emb(HIDDEN_DIM, BIDIRECTIONAL, dropout=DROPOUT, type=RNN_TYPE)\n","\n","model = model.to(device) # Move to GPU\n","\n","# Criterion\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters())"]},{"cell_type":"markdown","source":["#### Early stopping"],"metadata":{"id":"1Y_uS6AcpTen"}},{"cell_type":"code","execution_count":53,"metadata":{"id":"XVETTDl6ef0K","executionInfo":{"status":"ok","timestamp":1668073829768,"user_tz":-60,"elapsed":668,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["patience = 6    # Number of epochs to wait before early stopping\n","epochs_no_improve = 0  # Number of epochs with no improvement in validation loss\n","early_stop = False    # Boolean to activate early stopping"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4nMxfAEKHPpu","outputId":"8c1672ab-5db4-4d61-a92c-d6294a52982c","executionInfo":{"status":"ok","timestamp":1668074097442,"user_tz":-60,"elapsed":266544,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH 0:\n","[########################################] - 100.00 % | Loss (Training) = 0.368873 | Loss (Validation) = 0.346009 | Time left : 0.00\n","\n","EPOCH 1:\n","[########################################] - 100.00 % | Loss (Training) = 0.315721 | Loss (Validation) = 0.350166 | Time left : 0.00\n","\n","EPOCH 2:\n","[########################################] - 100.00 % | Loss (Training) = 0.269572 | Loss (Validation) = 0.365701 | Time left : 0.00\n","\n","EPOCH 3:\n","[########################################] - 100.00 % | Loss (Training) = 0.220243 | Loss (Validation) = 0.400321 | Time left : 0.00\n","\n","EPOCH 4:\n","[########################################] - 100.00 % | Loss (Training) = 0.175492 | Loss (Validation) = 0.441883 | Time left : 0.00\n","\n","EPOCH 5:\n","[########################################] - 100.00 % | Loss (Training) = 0.141795 | Loss (Validation) = 0.175153 | Time left : 7.20Early stopping!\n","---------------------------------------------------------------------\n","               Finished training and model saved                     \n","---------------------------------------------------------------------\n"]}],"source":["# Store training info\n","losses_train = []\n","accuracies_train = []\n","losses_val = []\n","accuracies_val = []\n","\n","estimated_time = 0 # To estimate time of training\n","train_size = len(X_train)\n","val_size = len(X_val)\n","\n","for epoch in range(NB_EPOCHS):\n","\n","  print('EPOCH {}:'.format(epoch))\n","  start = time.time()\n","  index = batch_size\n","\n","  #---------------------------------------------------------------------------\n","  #                                   Training\n","  #---------------------------------------------------------------------------\n","  # Training set\n","  train_loss = []\n","  train_acc = []\n","\n","  for batch_idx, batch in enumerate(train_dataloader):\n","\n","    # Data to GPU\n","    tweet = batch[0].to(device)\n","    sentiment = batch[1].to(device)\n","\n","    optimizer.zero_grad() # Resets gradients\n","\n","    # Combute predictions\n","    predictions = model(tweet).squeeze(1)\n","\n","    # Compute loss and accuracy\n","    loss = criterion(predictions, sentiment)\n","    acc = binary_accuracy(predictions, sentiment)\n","\n","    # Backward pass\n","    loss.backward()\n","    # Optimize parameters\n","    optimizer.step()\n","\n","    # Store loss anc accuracy for training \n","    train_loss.append(loss.detach().item())\n","    train_acc.append(acc.detach().item())\n","\n","    # Remove data from GPU\n","    tweet.to('cpu')\n","    sentiment.to('cpu')\n","\n","    # Update progress bar\n","    time_left = estimated_time -(time.time() - start)\n","    progressBar(loss, 0, time_left, (index/train_size)*100)\n","    index = index + batch_size\n","  \n","  # Compute mean loss and accuracy for training epoch\n","  mean_loss = sum(train_loss)/len(train_loss)\n","  losses_train.append(mean_loss)\n","  mean_acc = sum(train_acc)/len(train_acc)\n","  accuracies_train.append(mean_acc)\n","\n","  # Progress bar\n","  estimated_time = time.time() - start\n","  progressBar(mean_loss, 0, 0, 100)\n","\n","  #---------------------------------------------------------------------------\n","  #                                 Validation\n","  #---------------------------------------------------------------------------\n","  index_validation = batch_size\n","\n","  # Validation set\n","  val_loss = []\n","  val_acc = []\n","\n","  with torch.no_grad():\n","    for batch_idx, batch in enumerate(val_dataloader):\n","\n","      # Data to GPU\n","      tweet = batch[0].to(device)\n","      sentiment = batch[1].to(device)\n","\n","      # Combute predictions\n","      predictions = model(tweet).squeeze(1)\n","\n","      # Compute loss and accuracy\n","      loss_val = criterion(predictions, sentiment)\n","      acc_val = binary_accuracy(predictions, sentiment)\n","\n","      # Store loss and acccuracy for validation\n","      val_loss.append(loss_val.detach().item())\n","      val_acc.append(acc_val.detach().item())\n","\n","      # Remove data from GPU\n","      tweet.to('cpu')\n","      sentiment.to('cpu')\n","\n","      # Update progress bar\n","      progressBar(mean_loss, loss_val, time_left, (index_validation/val_size)*100)\n","      index_validation = index_validation + batch_size\n","\n","    # Compute mean loss and accuracy for validation epoch\n","    mean_loss_val = sum(val_loss)/len(val_loss)\n","    losses_val.append(mean_loss_val)\n","    mean_acc_val = sum(val_acc)/len(val_acc)\n","    accuracies_val.append(mean_acc_val)\n","\n","    # EARLY STOPPING\n","    if mean_loss_val < min(losses_val):\n","      print(\"Validation loss decreased\")\n","      epochs_no_improve = 0\n","    else:\n","      epochs_no_improve += 1\n","      if epochs_no_improve == patience:\n","        print(\"Early stopping!\")\n","        early_stop = True\n","        # Save model and break\n","        saveModel(model, optimizer)\n","        break\n","\n","\n","    # Display useful information\n","    estimated_time = time.time() - start\n","    progressBar(mean_loss, mean_loss_val, 0, 100)\n","    print(\"\\n\")\n","\n","    # Automatic save of the model\n","    if (epoch+1) in checkpoints:\n","      saveModel(model, optimizer)\n","      \n","# Information over terminal\n","print(\"---------------------------------------------------------------------\")\n","print(\"               Finished training and model saved                     \")\n","print(\"---------------------------------------------------------------------\")\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1668074120156,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"181-MlvGIvZN","outputId":"aeddf764-a02a-4480-d613-6c2eaa9cf49e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff20b7d7310>"]},"metadata":{},"execution_count":55},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.title(\"Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(losses_train, label=\"Training loss\")\n","plt.plot(losses_val, label=\"Validation loss\")\n","plt.legend()"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1668074123077,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"rUSDmG9cHdHI","outputId":"e14e4eab-2130-4cf4-e483-c0fb2e7696ef"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.title(\"Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot(accuracies_train, label=\"train\")\n","plt.plot(accuracies_val, label=\"validation\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"QjuhVjdQoSRy"},"source":["## Load Model\n","If you want to reload the weights of a previous model for testing of starting training again : "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":6,"status":"error","timestamp":1667417164366,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"i_maD8sxoTTO","outputId":"fbd7bc6b-5fef-4179-84b8-68625f98802d"},"outputs":[{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-3a1d992f1d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, \"LSTM\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RNN:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"model.weight_ih_l0\", \"model.weight_hh_l0\", \"model.bias_ih_l0\", \"model.bias_hh_l0\", \"model.weight_ih_l0_reverse\", \"model.weight_hh_l0_reverse\", \"model.bias_ih_l0_reverse\", \"model.bias_hh_l0_reverse\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"state_dict\", \"optimizer\". "]}],"source":["model_str = \"Wed Nov 02\"\n","model_path = root + \"/Models/\" + model_str + \".pth\"\n","#model = RNN(embedding, HIDDEN_DIM, DROPOUT, BIDIRECTIONAL, PADDING_ID, NUM_LAYERS, \"LSTM\")\n","model.load_state_dict(torch.load(model_path, map_location=device))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYqmxOeQYJrK","outputId":"a75d11e5-222a-42a1-cfa0-133e875d2900"},"outputs":[{"data":{"text/plain":["RNN(\n","  (embedding): Embedding(393023, 300, padding_idx=289283)\n","  (model): LSTM(300, 128, batch_first=True, dropout=0.2, bidirectional=True)\n","  (fc): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"qArMwrpwXTsl"},"source":["# Test model"]},{"cell_type":"markdown","metadata":{"id":"OTFdt4uf5Wiz"},"source":["### Evaluate the model with the test set"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"qClHWSRXXgIf","executionInfo":{"status":"ok","timestamp":1668074152401,"user_tz":-60,"elapsed":349,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"}}},"outputs":[],"source":["def positive_negative_count(preds, y):\n","  \"\"\"\n","  Returns the accuracy per batch\n","  \"\"\"\n","  pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n","\n","  prob_preds = torch.nn.Softmax(dim=-1)(preds)\n","  final_preds = torch.argmax(prob_preds, dim=1)\n","\n","  numpy_preds = final_preds.cpu().numpy()\n","  numpy_y = y.cpu().numpy()\n","\n","  for i in range(len(y)):\n","    if numpy_preds[i] == numpy_y[i]:\n","      if numpy_y[i] == 0:\n","        neg_correct += 1\n","      else:\n","        pos_correct += 1\n","\n","    if numpy_y[i] == 0:\n","      neg_cnt += 1\n","    else:\n","      pos_cnt += 1\n","\n","  return neg_correct, neg_cnt , pos_correct, pos_cnt"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":437,"status":"ok","timestamp":1668074154763,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"cn3xHSv5XVvr","outputId":"6433363b-d831-461b-f3fe-2840117869c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------\n","                   classified positive   classified negative \n","------------------------------------------------------------\n"," Actual positive      2061                   435\n"," Actual negative      458                   2046\n","------------------------------------------------------------\n","Acuracy:  0.8214\n","Precision:  0.8257211538461539\n","Recall:  0.8170926517571885\n"]}],"source":["neg_correct = 0\n","neg_cnt = 0\n","pos_correct = 0\n","pos_cnt = 0\n","\n","with torch.no_grad():\n","  for batch_idx, batch in enumerate(test_dataloader):\n","    # Data to GPU\n","    tweet = batch[0].to(device)\n","    sentiment = batch[1].to(device)\n","\n","    predictions = model(tweet).squeeze(1)\n","\n","    neg_correct_tmp, neg_cnt_tmp, pos_correct_tmp, pos_cnt_tmp = positive_negative_count(predictions, sentiment)\n","\n","    neg_correct += neg_correct_tmp\n","    neg_cnt += neg_cnt_tmp\n","    pos_correct += pos_correct_tmp\n","    pos_cnt += pos_cnt_tmp\n","  \n","\n","    # Remove data from GPU\n","    tweet.to('cpu')\n","    sentiment.to('cpu')\n","\n","print(\"------------------------------------------------------------\")\n","print(\"                   classified positive   classified negative \")\n","print(\"------------------------------------------------------------\")\n","print(\" Actual positive     \", pos_correct, \"                 \", neg_cnt-neg_correct)\n","print(\" Actual negative     \", pos_cnt-pos_correct, \"                 \", neg_correct)\n","print(\"------------------------------------------------------------\")\n","\n","print(\"Acuracy: \", (pos_correct+neg_correct)/(pos_cnt+neg_cnt))\n","print(\"Precision: \", pos_correct/(pos_correct + neg_cnt-neg_correct))\n","print(\"Recall: \", neg_correct/(neg_correct + pos_cnt-pos_correct))"]},{"cell_type":"markdown","metadata":{"id":"H4kIUQ6d5aex"},"source":["### Predict on a given sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0jOCMDMmhq9"},"outputs":[],"source":["def predict(sentence):\n","  splitted = sentence.split(\" \")\n","  Sentence_tokens = padding_and_encode(splitted, seq_length)\n","  seq = [Sentence_tokens]\n","  Sentence_tokens = torch.tensor(seq).to(device)\n","  \n","  with torch.no_grad():\n","    prediction = model(Sentence_tokens).squeeze(1)\n","\n","  prob_pred = torch.nn.Softmax(dim=-1)(prediction)\n","  final_pred = torch.argmax(prob_pred, dim=1)\n","  f = final_pred.cpu().numpy()\n","\n","  if(f[0] == 1):\n","    print(\"Positive\")\n","  else:\n","    print(\"Negative\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1667896987553,"user":{"displayName":"Ophélie dlBB","userId":"12727404173145562274"},"user_tz":-60},"id":"4f4-FIf9nDmC","outputId":"50f919c8-7321-4b17-e917-43610ff40eae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive\n","Positive\n","Positive\n","Negative\n","Positive\n"]}],"source":["predict(\"I was very disappointed with this series. It had lots of cool graphics and that's about it. The level of detail it went into was minimal, and I always got the feeling the audience was being patronized -- there was a lot of what seemed to me as \")\n","predict(\"Great just great! The West Coast got Dirty Harry Callahan, the East Coast got Sharky. Burt Reynolds plays Sharky in \")\n","predict(\"I loved that movie\")\n","predict(\"I hate you\")\n","predict(\"you are ugly\")"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1Sfu9tYFI_ajKKFwilSklag1UysjsrVsZ","timestamp":1666637792462}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"ef1b119d281e6c8960c7e46c636ce2a9ec354a3da52520939e5f0bfd0bbd7c37"}}},"nbformat":4,"nbformat_minor":0}